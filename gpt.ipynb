{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9053ed3e",
   "metadata": {},
   "source": [
    "# LLM From Scratch\n",
    "\n",
    "This is a notebook I'm using to re-create the GPT-2 style architecture from the book \"Build a Large Language Model (From Scratch).\"\n",
    "I'm trying to do as much as possible from memory, other than having some notes on what classes and methods to implement.\n",
    "\n",
    "**Required classes:**\n",
    "1. `LayerNorm`\n",
    "2. `GELU`\n",
    "3. `GPT_CONFIG_124M`\n",
    "4. `FeedForward`\n",
    "5. `MultiHeadAttention`\n",
    "6. `TransformerBlock`\n",
    "7. `GPTModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63992f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "# Import torch and nn.Module for class definitions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de899cae",
   "metadata": {},
   "source": [
    "## 1. LayerNorm\n",
    "\n",
    "This class is responsible for layer normalization, which takes place _multiple times_ in the GPT architecture.\n",
    "Its purpose is to keep gradient magnitudes within a certain range, to avoid the problems of vanishing gradients and exploding gradients.\n",
    "The concrete goal is to adjust the outputs to have a mean of zero and a variance of one.\n",
    "\n",
    "To accomplish this, we need two values:\n",
    "- the mean: $\\mu = \\frac{(x_1 + x_2 + ... + x_n)}{n}$\n",
    "- the variance: $v = \\frac{(x_1 + \\mu)^2 + (x_2 + \\mu)^2 + ... + (x_n + \\mu)^2}{n} + \\epsilon$\n",
    "\n",
    "The normalized vector is then: $[\\frac{(x_1 - µ)}{\\sqrt{v}}, \\frac{(x_2 - µ)}{\\sqrt{v}}, ..., \\frac{(x_n - µ)}{\\sqrt{v}}]$\n",
    "\n",
    "NOTE: we're dividing by both n and $\\sqrt{v}$ and we need to make sure we never divide by zero. We know that n (the embedding dimension) will never be zero, but the variance could be. For that reason, we add a miniscule value epsilon to the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46c343c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim: int):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.epsilon = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        variance = x.var(dim=-1, keepdim=True, unbiased=False) + self.epsilon\n",
    "        norm = (x - mean) / torch.sqrt(variance)\n",
    "        return self.scale * norm + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a8ea67",
   "metadata": {},
   "source": [
    "## 2. GELU\n",
    "\n",
    "GELU, or Gaussian Error Linear Unit, is the activation function we'll be using. It's similar to RELU, but it's differentiable everywhere (even at zero, where RELU has a sharp corner discontinuity). GELU is also slightly negative between -2 and 0, rather than flatly zero like RELU. This provides a richer range of values for the network to train on.\n",
    "\n",
    "Calculating the GELU for real would take us out of closed-form math, so we'll use a very close approximation here instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f0b6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * 0.5 * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d1c13b",
   "metadata": {},
   "source": [
    "## 3. GPT_CONFIG_124M\n",
    "The configuration paramters for our GPT-2 implementation. These come directly from the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3da9131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class GPTConfigDict(TypedDict):\n",
    "    vocab_size: int        # the number of tokens in the vocabulary\n",
    "    context_length: int    # the maximum number of token vectors to consider at once\n",
    "    emb_dim: int           # the width of the token vectors\n",
    "    n_heads: int           # the number of heads to use for multi-head attention\n",
    "    n_layers: int          # the number of transformer layers to use\n",
    "    drop_rate: float       # the dropout percentage rate\n",
    "    qkv_bias: bool         # whether to use the bias setting for the KQV matrices.\n",
    "\n",
    "GPT_CONFIG_124M: GPTConfigDict = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d86c8",
   "metadata": {},
   "source": [
    "## 4. FeedForward\n",
    "\n",
    "The feed-forward network (or multi-layer perceptron) is the fundamental neural network used in the GPT model.\n",
    "It expands the number of outputs in a hidden layer before shrinking back down to the original size for the output.\n",
    "This allows the network to explore a richer space, while preserving the input and output dimensions to keep the overall architecture simple.\n",
    "\n",
    "In this example, we'll expand the dimensions by a factor of 4 for the internal layer. I would normally say that should be configurable, but the book just has it fixed at 4. Anyway, that means that our 768 parameters will expand to 3,072, then shrink back down to 768 for output.\n",
    "\n",
    "### How many layers?\n",
    "\n",
    "If you look at a diagram of a feed-forward network, you'll see three layers:\n",
    "1. a left-most layer with n weights\n",
    "2. a middle layer with n*4 weights (or some other factor)\n",
    "3. a right-most layer with n weights again.\n",
    "\n",
    "However, if you look at the implementation below, it kind of seems like there are two linear layers.\n",
    "Well, as you might guess, the middle layer is really the connection between the first and the second layers.\n",
    "The first layer has `dim_internal` outputs, and the second layer has `dim_internal` inputs. These represent overlapping,\n",
    "connected points—just as you might see in the diagram.\n",
    "\n",
    "You could think about like this: each `nn.Linear` has two sides, and of the four total sides there are two that overlap in the center. Thus you get three layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ee38523",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg: GPTConfigDict): \n",
    "        super().__init__()\n",
    "        expansion_factor = 4\n",
    "        dim_external = cfg[\"emb_dim\"]\n",
    "        dim_internal = expansion_factor * dim_external\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(dim_external, dim_internal),\n",
    "            GELU(),\n",
    "            nn.Linear(dim_internal, dim_external),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b36cd1c",
   "metadata": {},
   "source": [
    "## 5. MultiHeadAttention\n",
    "\n",
    "This is the heart of what makes GPT different to earlier language models. The attention mechanism tweaks context vectors in response to earlier tokens in the sequence, shifting their \"meaning\" to become much richer and more specific than a single word could be.\n",
    "\n",
    "### Motivating Examples\n",
    "\n",
    "For example, take the sentence \"the cat sat on the mat because it was warm.\" The word \"it\" has one particular vector embedding in the vocabulary, which might relate loosely to concepts like \"noun\" and \"non-human.\" That's not enough to capture the meaning of \"it\" in this sentence, where it most likely refers to \"mat.\" Attention allows the system to change the vector for the \"it\" token to resemble the vector for \"mat,\" clarifying its meaning in the context of the sentence.\n",
    "\n",
    "That's about the simplest possible example, but in reality each token is pushed and pulled in much more subtle ways by many more tokens in the sequence, so that by the end it somehow represents the meaning of the entire sequence of text. Ultimately, the attention-modulated vector of the final token in the sequence is _the only input needed_ to predict the next token. That's pretty wild.\n",
    "\n",
    "For a more contrived example of what this means, take another example sequence: \"This gritty, romantic, windswept, ornate, melancholic city is none other than\". The word \"than\" has nothing to do with any particular city or place, but by the time its vector is modulated by this long series of words preceding it, it will be something that appears close (in embedding space) to cities like Lisbon and Istanbul. Indeed, those are the two most likely predictions for the final word in the sequence from GPT-3.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Multi-head attention was first described in \"Attention is All You Need\" (2017), in sections 3.2.1 (scaled dot-product attention) and 3.2.2 (extending to multiple heads). I'll be using that paper as a reference for the following two sections.\n",
    "\n",
    "#### Scaled Dot-Product Attention\n",
    "\n",
    "Each attention head is an instance of something called \"scaled dot-product attention,\" which is given by:\n",
    "\n",
    "$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "\n",
    "That is, the attention weights given matrices K, Q, and V are the result of applying softmax to the product of Q times K-transpose over the square root of the embedding size of K, all multiplied by V.\n",
    "\n",
    "I'll try to break that down a bit more:\n",
    "- Q, K, and V are trainable matrix parameters with the same dimensions as the token embedding vectors. They are short for Query, Key, and Value.\n",
    "  - I think of the Query parameter as representing what a token is \"looking for\" to know if another token is worth attending to.\n",
    "  - To continue that metaphor, the Key parameter is what other tokens \"look like\" to the Query.\n",
    "  - The Value is the real identity of the tokens that are found, their deeper reality beneath the appearance presented by the Key.\n",
    "  - To sum up, a token's Query is used to examine every other token's Key to see if it's a good match. If it is, we use that token's Value in attention weight.\n",
    "- Multiplying Q by the transpose of K gives us the dot product of every Query row against every Key row. In other words, it tells us how aligned every Query is with every Key.\n",
    "- We scale that by the inverse square root of the Key dimensions to counteract a known issue with dot-product attention: \"for large values of d_k, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.\" (\"Attention is All You Need,\" p. 4). In other words, the dot product of two rows is going to tend to get larger the more columns you have, and these large values make it hard for training to adjust weights effectively. Scaling by the square root of the number of columns helps to solve this.\n",
    "- Applying softmax turns these scaled dot products into weights.\n",
    "- Multiplying by V translates the weights by Key into weights by Value.\n",
    "\n",
    "Note: it's not described in detail in the paper, but there's an important step carried out here called masking. Essentially, we only want Queries to find Keys that _precede_ them in the sequence. We accomplish this by zeroing out values above the main diagonal. To make sure that these values are zero _after_ softmax, we first set them to minus-infinity.\n",
    "\n",
    "#### Multi-Head Attention\n",
    "\n",
    "In single-headed dot-product attention, Q, K, and V all have the same dimensions as the input and output embeddings. To use multiple heads, we divide the width of each parameter by the number of heads and concatenate them together. This results in the same overall dimensions, but with different sets of columns relating to different Value vectors:\n",
    "\n",
    "$\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, ..., head_h)W^O$\n",
    "\n",
    "$\\text{ where } head_i = \\text{Attention}(Q_iW_i^Q, K_iW_i^K, V_iW_i^V)$\n",
    "\n",
    "$\\text{ where } W_i^Q \\in \\mathbb{R}^{d_{model} \\times d_k}$, $W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}$, $ W_i^V \\in \\mathbb{R}^{d_{model} \\times d_v}$, $W_i^O \\in \\mathbb{R}^{hd_{model} \\times d_{model}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf643cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in: int, d_out: int, context_length: int, dropout: float, num_heads: int, qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "        if d_out % num_heads != 0:\n",
    "            raise ValueError(\"The number of heads must evenly divide d_out.\")\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_width = d_out // num_heads\n",
    "        self.qkv_bias = qkv_bias\n",
    "\n",
    "        # construct the weights for Q, K, and V.\n",
    "        # these will be registered as trainable parameters automatically.\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # and the output projection, also trainable.\n",
    "        self.w_out = nn.Linear(d_out, d_out)\n",
    "        \n",
    "        # and the dropout layer. not trainable, just drops random values\n",
    "        # to zero with a probability determined by the dropout parameter\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # and the mask, which prevents each token from \"seeing\" later ones\n",
    "        mask = torch.triu( # an upper triangular matrix\n",
    "            torch.ones(context_length, context_length), # consisting of ones\n",
    "            diagonal=1, # starting one row above the diagonal, leaving the diagonal itself as zeroes.\n",
    "        )\n",
    "        self.register_buffer(\"mask\", mask) # register this tensor as non-trainable, but keep it on the same device\n",
    "        self.mask: torch.Tensor # to make the type-checker happy\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch, num_tokens, d_in = x.shape\n",
    "        queries = self.w_query(x)\n",
    "        keys = self.w_key(x)\n",
    "        values = self.w_value(x)\n",
    "\n",
    "        # Split the last dimension of the tensors into multiple heads\n",
    "        q_heads = queries.view(batch, num_tokens, self.num_heads, self.head_width)\n",
    "        k_heads = keys.view(batch, num_tokens, self.num_heads, self.head_width)\n",
    "        v_heads = values.view(batch, num_tokens, self.num_heads, self.head_width)\n",
    "\n",
    "        #                                  [  0  ,     1     ,    2     ,      3    ]\n",
    "        # {q,k,v}_heads now have the shape [batch, num_tokens, num_heads, head_width],\n",
    "        # but we want them to be:          [batch, num_heads, num_tokens, head_width]\n",
    "        q_heads = q_heads.transpose(1, 2)\n",
    "        k_heads = k_heads.transpose(1, 2)\n",
    "        v_heads = v_heads.transpose(1, 2)\n",
    "\n",
    "        # now we need to calculate the raw dot-product attention scores between Q and K^T,\n",
    "        # where K^T has the shape [batch, num_heads, head_width, num_tokens].\n",
    "        # that gives attention_scores the shape [batch, num_heads, num_tokens, num_tokens]\n",
    "        attention_scores = q_heads @ k_heads.transpose(2, 3)\n",
    "        # and apply the causal mask\n",
    "        mask = self.mask[:num_tokens, :num_tokens]\n",
    "        attention_scores = attention_scores.masked_fill(mask == 1, float('-inf'))\n",
    "\n",
    "        # and we construct the weights using softmax on the scaled final dimension\n",
    "        attention_weights = torch.softmax(attention_scores / self.head_width**0.5, dim=-1)\n",
    "        # and apply dropout\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        #                                 [  0  ,     1    ,     2     ,     3     ]\n",
    "        # attention_weights has the shape [batch, num_heads, num_tokens, num_tokens]\n",
    "        # v_heads has the shape:          [batch, num_heads, num_tokens, head_width]\n",
    "        # if we multiply them, we get:    [batch, num_heads, num_tokens, head_width]\n",
    "        # but in the end, we want:        [batch, num_tokens, d_out]\n",
    "        context = attention_weights @ v_heads # [batch, num_heads, num_tokens, head_width]\n",
    "\n",
    "        # so we need to first transpose and get [batch, num_tokens, num_heads, head_width]\n",
    "        context = context.transpose(1, 2)\n",
    "        # and then concatenate the last two dimensions together to get d_out\n",
    "        context = context.contiguous().view(batch, num_tokens, self.d_out)\n",
    "        # and multiply by the output projection\n",
    "        return self.w_out(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2047eee4",
   "metadata": {},
   "source": [
    "## 6. TransformerBlock\n",
    "\n",
    "This version of the transformer block is loosely based on \"Attention is All You Need\" section 3, but includes _only_ the decoder stack. The encoder stack is omitted from the GPT architecture, and thus from the Build a Large Language Model (From Scratch) book.\n",
    "\n",
    "The transformer block goes a little something like this:\n",
    "```\n",
    "Tokenized Text -> LayerNorm 1 -> MultiHeadAttention -> Dropout -> (+) -> LayerNorm 2 -> FeedForward -> Dropout -> (+) -> Output\n",
    "```\n",
    "\n",
    "Where `(+)` represents a shortcut connection, where a previous state is added back in to reinforce weights that are getting very small.\n",
    "\n",
    "As far as requirements:\n",
    "- I've already implemented the the LayerNorm, MultiHeadAttention, and FeedForward classes.\n",
    "- `nn.Dropout` is provided by PyTorch.\n",
    "- Shortcut connections just use ordinary variables and addition.\n",
    "\n",
    "So we're all set to put these elements together below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3fa3ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single GPT-2 transformer block.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: GPTConfigDict):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.attention = MultiHeadAttention(\n",
    "            cfg[\"emb_dim\"],\n",
    "            cfg[\"emb_dim\"],\n",
    "            cfg[\"context_length\"],\n",
    "            cfg[\"drop_rate\"],\n",
    "            cfg[\"n_heads\"],\n",
    "            cfg[\"qkv_bias\"],\n",
    "        )\n",
    "        self.drop_rate = cfg[\"drop_rate\"]\n",
    "        self.layer_norm_2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.feedforward = FeedForward(cfg)\n",
    "        self.dropout = nn.Dropout(self.drop_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        shortcut = x\n",
    "        x = self.layer_norm_1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.layer_norm_2(x)\n",
    "        x = self.feedforward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec0224e",
   "metadata": {},
   "source": [
    "## 7. GPTModel\n",
    "\n",
    "This is the big one, where everything comes together.\n",
    "The hard parts are all pretty much done, this is going to be just a bit more glue.\n",
    "\n",
    "The flow here goes like:\n",
    "```\n",
    "Tokenized Text -> Token Embedding Layer -> Positional Embedding Layer -> Dropout -> TransformerBlocks -> LayerNorm -> Output\n",
    "```\n",
    "\n",
    "Or, in detail:\n",
    "1. Tokenized Text: the tokenizer is outside of this module; we'll get to that later.\n",
    "2. Token Embedding Layer: this is a trainable `nn.Embedding` layer that starts out with random weights. It maps tokens to the embedding space.\n",
    "3. Positional Embedding Layer: very similar to the Token Embedding Layer, but encodes positional information rather than \"semantic\" content.\n",
    "4. Dropout: provided by `nn.Dropout` with a configurable drop rate.\n",
    "5. TransformerBlocks: implemented above. We'll have a number of these set by config, and they run in serial.\n",
    "6. LayerNorm: also implemented above. This keeps all values in the tensors in a range of [-1, 1], with a mean of 0.\n",
    "7. Output: the outputs are called \"logits,\" and they represent the likelihood that the following token will be the one with a given ID. In order to project these from the previous LayerNorm, we'll need the size to be $\\text{emb\\_dim} \\times \\text{vocab\\_size}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7369c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Top-level GPT-2 model.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: GPTConfigDict):\n",
    "        \"\"\"Initialize model with config.\"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.positional_embedding = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.layer_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.output = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass: input indices to logits.\"\"\"\n",
    "        batch_size, sequence_length = in_idx.shape\n",
    "        token_embeddings = self.token_embedding(in_idx)\n",
    "        positional_embeddings = self.positional_embedding(\n",
    "            # get the first N positional embeddings, where N is the sequence length\n",
    "            torch.arange(sequence_length, device=in_idx.device)\n",
    "        )\n",
    "\n",
    "        x = token_embeddings + positional_embeddings\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.output(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311222f7",
   "metadata": {},
   "source": [
    "# Smoke Test\n",
    "\n",
    "If everything above has worked, then we should be able to exactly replicate the results from the book as long as we use the same seed (123).\n",
    "\n",
    "Use the `smoke_test` function below to get the predicted completion for a given prompt from the _untrained_ LLM.\n",
    "\n",
    "Note: because the LLM is still untrained, the result will be total garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db9e0bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    \"\"\"\n",
    "    A helper function used by smoke_test. It's easier to pass the prompt to smoke_test, rather than call this directly.\n",
    "    \"\"\"\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probabilities, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n",
    "\n",
    "def smoke_test(prompt):\n",
    "    \"\"\"\n",
    "    Pass the prompt to the (untrained) GPT model with a manual seed. Should correspond to the expected output.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(123)\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    encoded = tokenizer.encode(prompt)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    model.eval()\n",
    "    out = generate_text_simple(\n",
    "        model,\n",
    "        encoded_tensor,\n",
    "        6,\n",
    "        GPT_CONFIG_124M[\"context_length\"]\n",
    "    )\n",
    "    decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "    print(decoded_text)\n",
    "\n",
    "smoke_test(\"Hello, I am\") # should output \"Hello, I am Featureiman Byeswickattribute argue\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed1431c",
   "metadata": {},
   "source": [
    "# Training a smaller GPT-2\n",
    "\n",
    "What follows is the code to train a version of this architecture. Because training is computationally expensive, I'm going to reduce the context length to a more manageable 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2416bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_MINI: GPTConfigDict = {**GPT_CONFIG_124M, \"context_length\": 256} # 1024 is just too big to train locally\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_MINI)\n",
    "model.eval(); # disables dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bf31ff",
   "metadata": {},
   "source": [
    "## Convenience Functions and Example\n",
    "\n",
    "We're adding a few functions to make it easier to interact with the model. These might've been useful in the smoke test above, so maybe I'll refactor a bit later.\n",
    "\n",
    "Also, another quick example of how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c38a43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "def text_to_token_ids(text: str, tokenizer: tiktoken.Encoding) -> torch.Tensor:\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids: torch.Tensor, tokenizer: tiktoken.Encoding) -> str:\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "# example:\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_MINI[\"context_length\"],\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf453bb",
   "metadata": {},
   "source": [
    "# Calculating Loss\n",
    "\n",
    "This is a temporary helper to show the training and validation loss scores for a given corpus. It still only uses the untrained model, so the results are guaranteed to be garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fed6fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text: str, tokenizer: tiktoken.Encoding, max_length: int, stride: int):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            start = i\n",
    "            end = start + max_length\n",
    "            input_chunk = token_ids[start:end]\n",
    "            target_chunk = token_ids[start+1:end+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "class LossCalculator:\n",
    "    def __init__(self, cfg: GPTConfigDict, train_ratio:float=0.9):\n",
    "        self.cfg = cfg\n",
    "        self.model = GPTModel(self.cfg)\n",
    "        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.train_ratio = train_ratio\n",
    "        self.device = self.get_device()\n",
    "\n",
    "    def get_device(self) -> torch.device:\n",
    "        if torch.cuda.is_available(): # type: ignore[attr-defined]\n",
    "            return torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available(): # type: ignore[attr-defined]\n",
    "            return torch.device(\"mps\")\n",
    "        else:\n",
    "            return torch.device(\"cpu\")\n",
    "\n",
    "    def create_dataloader(self, text: str, batch_size:int=4, max_length:int=256, stride:int=128, shuffle:bool=True, drop_last:bool=True, num_workers:int=0) -> DataLoader:\n",
    "        dataset = GPTDatasetV1(text, self.tokenizer, max_length, stride)\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            drop_last=drop_last,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "\n",
    "    def loss_for_batch(self, input_batch: torch.Tensor, target_batch: torch.Tensor):\n",
    "        input_batch, target_batch = input_batch.to(self.device), target_batch.to(self.device)\n",
    "        logits = self.model(input_batch)\n",
    "        return nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "\n",
    "    def calc_loss_loader(self, data_loader, num_batches=None):\n",
    "        total_loss = 0\n",
    "        if len(data_loader) == 0:\n",
    "            return float(\"nan\")\n",
    "        elif num_batches is None:\n",
    "            num_batches = len(data_loader)\n",
    "        else:\n",
    "            num_batches = min(num_batches, len(data_loader))\n",
    "        \n",
    "        for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "            if i < num_batches:\n",
    "                loss = self.loss_for_batch(input_batch, target_batch)\n",
    "                total_loss += loss.item()\n",
    "            else:\n",
    "                break\n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def run(self, text: str, max_length:int=0, stride:int=0):\n",
    "        split_idx = int(self.train_ratio * len(text))\n",
    "        train_data = text[:split_idx]\n",
    "        validation_data = text[split_idx:]\n",
    "        torch.manual_seed(123)\n",
    "        if stride == 0:\n",
    "            stride = self.cfg[\"context_length\"]\n",
    "        if max_length == 0:\n",
    "            max_length = self.cfg[\"context_length\"]\n",
    "        train_loader = self.create_dataloader(\n",
    "            train_data,\n",
    "            batch_size=2,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "        )\n",
    "        validation_loader = self.create_dataloader(\n",
    "            validation_data,\n",
    "            batch_size=2,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            training_loss = self.calc_loss_loader(train_loader)\n",
    "            validation_loss = self.calc_loss_loader(validation_loader)\n",
    "        elapsed = time.time() - start\n",
    "        return {\n",
    "            \"training_loss\": training_loss,\n",
    "            \"validation_loss\": validation_loss,\n",
    "            \"device_type\": self.device.type,\n",
    "            \"time_seconds\": elapsed\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea9024",
   "metadata": {},
   "source": [
    "# Example of Loss\n",
    "\n",
    "The `verdict_loss()` function is basically another smoke test. It loads the public domain book _The Verdict_ and passes it to the untrained model to calculate the loss metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aaed8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "def verdict_loss():\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "    text_data = \"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode('utf-8')\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "    lc = LossCalculator(GPT_CONFIG_MINI)\n",
    "    return lc.run(text_data, max_length=256, stride=8)\n",
    "\n",
    "# verdict_loss() # uncomment to see the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f7ff51",
   "metadata": {},
   "source": [
    "# Training a Model\n",
    "\n",
    "The following class is nearly a copy of the LossCalculator class above, but it actually trains the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e7019ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import json\n",
    "\n",
    "class DecodingStrategy(Enum):\n",
    "    Greedy = 1\n",
    "    Multinomial = 2\n",
    "    TopK = 3\n",
    "\n",
    "class TrainGPT:\n",
    "    def __init__(self, cfg: GPTConfigDict, eval_frequency:int=5, train_ratio:float=0.9, force_cpu:bool=False, name=\"traingpt\"):\n",
    "        self.cfg = cfg\n",
    "        self.name = name\n",
    "        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.train_ratio = train_ratio\n",
    "        self.force_cpu = force_cpu\n",
    "        self.device = self.get_device()\n",
    "        self.model = GPTModel(self.cfg).to(self.device)\n",
    "        self.optimizer = torch.optim.AdamW( # type: ignore[attr-defined]\n",
    "            self.model.parameters(),\n",
    "            lr=0.0004,\n",
    "            weight_decay=0.1,\n",
    "        )\n",
    "        self.eval_frequency = eval_frequency\n",
    "        self.tokens_seen, self.global_step = 0, -1\n",
    "\n",
    "    def save(self, name: str):\n",
    "        torch.save({\n",
    "            \"model_state_dict\": self.model.state_dict(),\n",
    "            \"optimizer_state_dict\": self.optimizer.state_dict()\n",
    "        },\n",
    "        f\"{name}.pth\")\n",
    "\n",
    "    def load(self, name: str):\n",
    "        checkpoint = torch.load(f\"{name}.pth\")\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "    def get_device(self) -> torch.device:\n",
    "        if self.force_cpu:\n",
    "            return torch.device(\"cpu\")\n",
    "        if torch.cuda.is_available(): # type: ignore[attr-defined]\n",
    "            return torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available(): # type: ignore[attr-defined]\n",
    "            return torch.device(\"mps\")\n",
    "        else:\n",
    "            return torch.device(\"cpu\")\n",
    "\n",
    "    def create_dataloader(self, text: str, batch_size:int=4, max_length:int=256, stride:int=128, shuffle:bool=True, drop_last:bool=True, num_workers:int=0) -> DataLoader:\n",
    "        dataset = GPTDatasetV1(text, self.tokenizer, max_length, stride)\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            drop_last=drop_last,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "\n",
    "    def loss_for_batch(self, input_batch: torch.Tensor, target_batch: torch.Tensor) -> torch.Tensor:\n",
    "        assert input_batch.max().item() < self.model.token_embedding.num_embeddings\n",
    "        assert target_batch.max().item() < self.model.token_embedding.num_embeddings\n",
    "        input_batch, target_batch = input_batch.to(self.device), target_batch.to(self.device)\n",
    "        logits = self.model(input_batch)\n",
    "        return nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten()).to(self.device)\n",
    "\n",
    "    def calc_loss_loader(self, data_loader, num_batches=None) -> float:\n",
    "        total_loss = 0\n",
    "        if len(data_loader) == 0:\n",
    "            return float(\"nan\")\n",
    "        elif num_batches is None:\n",
    "            num_batches = len(data_loader)\n",
    "        else:\n",
    "            num_batches = min(num_batches, len(data_loader))\n",
    "        \n",
    "        for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "            if i < num_batches:\n",
    "                loss = self.loss_for_batch(input_batch, target_batch)\n",
    "                total_loss += loss.item()\n",
    "            else:\n",
    "                break\n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def loaders(self, text: str, max_length:int=0, stride:int=0) -> tuple[DataLoader, DataLoader]:\n",
    "        split_idx = int(self.train_ratio * len(text))\n",
    "        train_data = text[:split_idx]\n",
    "        validation_data = text[split_idx:]\n",
    "        torch.manual_seed(123)\n",
    "        if stride == 0:\n",
    "            stride = self.cfg[\"context_length\"]\n",
    "        if max_length == 0:\n",
    "            max_length = self.cfg[\"context_length\"]\n",
    "        train_loader = self.create_dataloader(\n",
    "            train_data,\n",
    "            batch_size=2,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "        )\n",
    "        validation_loader = self.create_dataloader(\n",
    "            validation_data,\n",
    "            batch_size=2,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        return (train_loader, validation_loader)\n",
    "    \n",
    "    def evaluate(self, text: str, max_length:int=256, stride:int=128, epoch:int=0, prompt:str=\"\"):\n",
    "        train_loader, validation_loader = self.loaders(text, max_length, stride)\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            training_loss = self.calc_loss_loader(train_loader)\n",
    "            validation_loss = self.calc_loss_loader(validation_loader)\n",
    "        summary = {\n",
    "            \"training_loss\": training_loss,\n",
    "            \"validation_loss\": validation_loss,\n",
    "            \"device_type\": self.device.type,\n",
    "            \"epoch\": epoch\n",
    "        }\n",
    "        if len(prompt) > 0:\n",
    "            example_output = self.prompt(prompt)\n",
    "            summary[\"example_output\"] = example_output\n",
    "        return summary\n",
    "\n",
    "    def choose(self, strategy: DecodingStrategy, logits: torch.Tensor, temperature:float, k:int=25):\n",
    "        match strategy:\n",
    "            case DecodingStrategy.Greedy:\n",
    "                probabilities = torch.softmax(logits, dim=-1)\n",
    "                result = torch.argmax(probabilities, dim=-1, keepdim=True)\n",
    "                return result\n",
    "            case DecodingStrategy.Multinomial:\n",
    "                scaled = logits / temperature\n",
    "                probabilities = torch.softmax(scaled, dim=-1)\n",
    "                result = torch.multinomial(probabilities, num_samples=1)\n",
    "                return result\n",
    "            case DecodingStrategy.TopK:\n",
    "                batch_size, vocab_size = logits.shape\n",
    "                top_logits, top_pos = torch.topk(logits, k)\n",
    "                filtered = torch.full_like(\n",
    "                    logits, -torch.inf\n",
    "                )\n",
    "                filtered.scatter_(dim=1, index=top_pos, src=top_logits) #huh?\n",
    "                scaled = filtered / temperature\n",
    "                probabilities = torch.softmax(scaled, dim=-1)\n",
    "                if torch.any(torch.isnan(probabilities)) or torch.any(probabilities < 0):\n",
    "                    print(\"Bad probabilities:\", probabilities)\n",
    "                    print(\"Logits:\", logits)\n",
    "                    raise ValueError(\"NaNs or invalid values in probabilities\")\n",
    "                return torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "    def generate_text_simple(self, token_ids: torch.Tensor, max_new_tokens, context_size, temperature:float):\n",
    "        self.model.to(self.device)\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = token_ids[:, -context_size:]\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            idx_next = self.choose(DecodingStrategy.TopK, logits, temperature=temperature)\n",
    "            token_ids = torch.cat((token_ids, idx_next), dim=1)\n",
    "        return token_ids\n",
    "\n",
    "    def prompt(self, text: str, max_tokens:int=10, temperature:float=0.8) -> str:\n",
    "        encoded = self.tokenizer.encode(text)\n",
    "        encoded_tensor = torch.tensor(encoded).unsqueeze(0).to(self.device)\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        out = self.generate_text_simple(\n",
    "            encoded_tensor,\n",
    "            max_tokens,\n",
    "            self.cfg[\"context_length\"],\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        decoded_text = self.tokenizer.decode(out.squeeze(0).tolist())\n",
    "        return decoded_text\n",
    "\n",
    "    def generate_and_print_sample(self, prompt: str) -> None:\n",
    "        print(self.prompt(prompt))\n",
    "\n",
    "    def train_loader(self, training_loader: DataLoader, evaluation_text: str, evaluation_prompt: str, epochs:int=1):\n",
    "        torch.manual_seed(123)\n",
    "        self.model.to(self.device)\n",
    "        self.tokens_seen = 0\n",
    "        loss_summaries = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            for input_batch, target_batch in training_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.loss_for_batch(\n",
    "                    input_batch, target_batch\n",
    "                )\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.tokens_seen += input_batch.numel()\n",
    "                self.global_step += 1\n",
    "                if self.global_step % self.eval_frequency == 0:\n",
    "                    summary = self.evaluate(evaluation_text, 256, 128, epoch, evaluation_prompt)\n",
    "                    summary[\"tokens_seen\"] = self.tokens_seen\n",
    "                    print(summary)\n",
    "                    loss_summaries.append(summary)\n",
    "                    self.save(self.name)\n",
    "\n",
    "    def train(self, text: str, max_length:int=0, stride:int=0, epochs:int=10, prompt:str=\"Hello, I am \"):\n",
    "        torch.manual_seed(123)\n",
    "        loss_summaries = []\n",
    "        training_loader, _ = self.loaders(text, max_length, stride)\n",
    "        self.model.to(self.device)\n",
    "        self.tokens_seen = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            for input_batch, target_batch in training_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.loss_for_batch(\n",
    "                    input_batch, target_batch\n",
    "                )\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.tokens_seen += input_batch.numel()\n",
    "                self.global_step += 1\n",
    "\n",
    "                if self.global_step % self.eval_frequency == 0:\n",
    "                    summary = self.evaluate(text, max_length, stride, epoch, prompt)\n",
    "                    summary[\"tokens_seen\"] = self.tokens_seen\n",
    "                    print(summary)\n",
    "                    loss_summaries.append(summary)\n",
    "        \n",
    "        self.generate_and_print_sample(prompt)\n",
    "        return loss_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53c474f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verdict_train():\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "    text_data = \"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode('utf-8')\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "    trainable_model = TrainGPT(GPT_CONFIG_MINI, force_cpu=False, eval_frequency=5)\n",
    "    return trainable_model.train(text_data, epochs=10, prompt=\"Every effort moves you\")\n",
    "\n",
    "# verdict_train() # uncomment to see the results of training this LLM on \"The Verdict\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb011898",
   "metadata": {},
   "source": [
    "# Training on Project Gutenberg\n",
    "\n",
    "To see how far I can take this, I'm going to try to train a model on more and more text. I don't really know how this is going to go, and I'm way beyond either the book or the lectures now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6aff3e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8ad38c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "import textwrap\n",
    "\n",
    "class LazyTokenDatasetPG19(Dataset):\n",
    "    GUTENBERG_END_RE = re.compile(r\"(?i)end of (the )?project gutenberg.*\", re.DOTALL)\n",
    "    TOO_MANY_NEWLINES_RE = re.compile(r\"\\n{3,}\")\n",
    "    LEADING_NEWLINES_RE = re.compile(r\"^\\n+\")\n",
    "    \n",
    "    def __init__(self, context_len:int=256):\n",
    "        super().__init__()\n",
    "        self.context_len = context_len\n",
    "        self.preprocess()\n",
    "        self.file_paths = glob.glob(\"tokens/*.pt\")\n",
    "        self.samples: list[tuple[int, int]] = []\n",
    "        print(\"Loading data from tokens directory\")\n",
    "        for i, path in enumerate(self.file_paths):\n",
    "            length = torch.load(path, map_location=\"cpu\").shape[0]\n",
    "            for j in range(0, length - context_len, context_len):\n",
    "                self.samples.append((i, j))\n",
    "            if i % 5_000 == 0:\n",
    "                print(f\"Loaded up to book {i}...\")\n",
    "        print(\"Loading complete\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        book_idx, start = self.samples[idx]\n",
    "        tokens = torch.load(self.file_paths[book_idx], map_location=\"cpu\")\n",
    "        input_ids = tokens[start : start + self.context_len]\n",
    "        target_ids = tokens[start + 1 : start + self.context_len + 1]\n",
    "        return input_ids, target_ids\n",
    "\n",
    "    def preprocess(self):\n",
    "        ds = load_dataset(\"deepmind/pg19\", split=\"train\")\n",
    "        os.makedirs(\"tokens\", exist_ok=True)\n",
    "        existing_filepaths = glob.glob(\"tokens/book_*.pt\")\n",
    "        if len(existing_filepaths) >= (ds.num_rows - 5_000): # type: ignore[attr-defined]\n",
    "            print(\"Preprocessing not needed.\")\n",
    "            return\n",
    "        print(\"Preprocessing data to tokens directory.\")\n",
    "        tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        for i, book in enumerate(ds):\n",
    "            path = Path(f\"tokens/book_{i}.pt\")\n",
    "            if path.exists():\n",
    "                continue\n",
    "            text = self.clean_text(book['text'])\n",
    "            if len(text) < self.context_len + 1:\n",
    "                continue\n",
    "            tokens = tokenizer.encode(text)\n",
    "            torch.save(torch.tensor(tokens, dtype=torch.long), path)\n",
    "            if i % 1_000 == 0:\n",
    "                print(f\"Completed preprocessing book {i}\")\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        # Remove Gutenberg end matter\n",
    "        text = self.GUTENBERG_END_RE.split(text)[0]\n",
    "        # Remove leading newlines/whitespace\n",
    "        text = self.LEADING_NEWLINES_RE.sub(\"\", text)\n",
    "        # Collapse 3+ newlines into exactly 2 (paragraph break)\n",
    "        text = self.TOO_MANY_NEWLINES_RE.sub(\"\\n\\n\", text)\n",
    "        # Eliminate chapter:verse markings\n",
    "        text = re.sub(r'\\b\\d+:\\d+\\b', '', text)\n",
    "        # Unwrap lines in each paragraph, but preserve paragraphs\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        unwrapped_paragraphs = [re.sub(r\"\\n\", \" \", p) for p in paragraphs]\n",
    "        text = '\\n\\n'.join(unwrapped_paragraphs)\n",
    "        # don't allow multiple spaces in a row\n",
    "        text = re.sub(r\"[^\\S\\n]+\", \" \", text)\n",
    "        return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e8f90188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data to tokens directory.\n",
      "Completed preprocessing book 0\n",
      "Completed preprocessing book 1000\n",
      "Completed preprocessing book 2000\n",
      "Completed preprocessing book 3000\n",
      "Completed preprocessing book 4000\n",
      "Completed preprocessing book 5000\n",
      "Completed preprocessing book 6000\n",
      "Completed preprocessing book 7000\n",
      "Completed preprocessing book 8000\n",
      "Completed preprocessing book 9000\n",
      "Completed preprocessing book 10000\n",
      "Completed preprocessing book 11000\n",
      "Completed preprocessing book 12000\n",
      "Completed preprocessing book 13000\n",
      "Completed preprocessing book 14000\n",
      "Completed preprocessing book 15000\n",
      "Completed preprocessing book 16000\n",
      "Completed preprocessing book 17000\n",
      "Completed preprocessing book 18000\n",
      "Completed preprocessing book 19000\n",
      "Completed preprocessing book 20000\n",
      "Completed preprocessing book 21000\n",
      "Completed preprocessing book 22000\n",
      "Completed preprocessing book 23000\n",
      "Completed preprocessing book 24000\n",
      "Completed preprocessing book 25000\n",
      "Completed preprocessing book 26000\n",
      "Completed preprocessing book 27000\n",
      "Completed preprocessing book 28000\n",
      "Loading data from tokens directory\n",
      "Loaded up to book 0...\n",
      "Loaded up to book 5000...\n",
      "Loaded up to book 10000...\n",
      "Loaded up to book 15000...\n",
      "Loaded up to book 20000...\n",
      "Loaded up to book 25000...\n",
      "Loading complete\n"
     ]
    }
   ],
   "source": [
    "ltds = LazyTokenDatasetPG19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5112be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrainGPT(GPT_CONFIG_MINI, eval_frequency=1000, name=\"pg19_preprocessed\")\n",
    "model.load(\"pg19_preprocessed\")\n",
    "pgv = load_dataset(\"deepmind/pg19\", split='validation')\n",
    "walden = pgv[0]['text'][63:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8860a6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training_loss': 9.85320260969259, 'validation_loss': 9.848266124725342, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I, and the, and the, and the,', 'tokens_seen': 2048}\n",
      "{'training_loss': 6.7455797033794855, 'validation_loss': 6.539387067159017, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I am going to the door.\\n\\n\"I', 'tokens_seen': 2050048}\n",
      "{'training_loss': 6.734731641866393, 'validation_loss': 6.4798515637715655, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the same time, and I was', 'tokens_seen': 4098048}\n",
      "{'training_loss': 6.666740013381182, 'validation_loss': 6.4115034739176435, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been a man who had been a little.', 'tokens_seen': 6146048}\n",
      "{'training_loss': 6.664215500071897, 'validation_loss': 6.353466113408406, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the house.\\n\\n\"I', 'tokens_seen': 8194048}\n",
      "{'training_loss': 6.516300767155017, 'validation_loss': 6.255221525828044, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the same time, and I had', 'tokens_seen': 10242048}\n",
      "{'training_loss': 6.381236981537382, 'validation_loss': 6.122494538625081, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the morning, and I had a', 'tokens_seen': 12290048}\n",
      "{'training_loss': 6.371453301381257, 'validation_loss': 6.113394101460774, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the morning. I was in the morning', 'tokens_seen': 14338048}\n",
      "{'training_loss': 6.3310872498205155, 'validation_loss': 6.023895184199016, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a good deal of the time, and I', 'tokens_seen': 16386048}\n",
      "{'training_loss': 6.046819961677163, 'validation_loss': 5.826356093088786, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little more than a little more than a', 'tokens_seen': 18434048}\n",
      "{'training_loss': 6.062206494606148, 'validation_loss': 5.859798272450765, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I am not a man of a man who is a', 'tokens_seen': 20482048}\n",
      "{'training_loss': 6.048032615144374, 'validation_loss': 5.85898772875468, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the presence of the house, I was', 'tokens_seen': 22530048}\n",
      "{'training_loss': 6.032901254750914, 'validation_loss': 5.839427550633748, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the house, I was not a little', 'tokens_seen': 24578048}\n",
      "{'training_loss': 5.841881008471473, 'validation_loss': 5.631335337956746, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the midst of the room, and', 'tokens_seen': 26626048}\n",
      "{'training_loss': 5.903112775188381, 'validation_loss': 5.639952977498372, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I have been in the hands of the city, I', 'tokens_seen': 28674048}\n",
      "{'training_loss': 5.8241158259117, 'validation_loss': 5.589769681294759, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the morning, and I was in the', 'tokens_seen': 30722048}\n",
      "{'training_loss': 5.757999218116372, 'validation_loss': 5.529593865076701, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I am going to see you. I am going to', 'tokens_seen': 32770048}\n",
      "{'training_loss': 5.715974145016427, 'validation_loss': 5.554795026779175, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the habit of getting up, I', 'tokens_seen': 34818048}\n",
      "{'training_loss': 5.6232021541918735, 'validation_loss': 5.535338719685872, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the room, I was not in', 'tokens_seen': 36866048}\n",
      "{'training_loss': 5.590811026298393, 'validation_loss': 5.5108795166015625, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a man, and I was a man,', 'tokens_seen': 38914048}\n",
      "{'training_loss': 5.404352406323967, 'validation_loss': 5.3656322956085205, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the house, I could not but', 'tokens_seen': 40962048}\n",
      "{'training_loss': 5.565279637352895, 'validation_loss': 5.528658390045166, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the house, I was in the', 'tokens_seen': 43010048}\n",
      "{'training_loss': 5.519297704858295, 'validation_loss': 5.407913605372111, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the office, I had to go', 'tokens_seen': 45058048}\n",
      "{'training_loss': 5.475592823351844, 'validation_loss': 5.3668928146362305, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little girl, I was a little girl', 'tokens_seen': 47106048}\n",
      "{'training_loss': 5.604958081649522, 'validation_loss': 5.461734612782796, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little boy, and I was a little', 'tokens_seen': 49154048}\n",
      "{'training_loss': 5.60751353280019, 'validation_loss': 5.468863566716512, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the house, and I was not', 'tokens_seen': 51202048}\n",
      "{'training_loss': 5.382911940752449, 'validation_loss': 5.322160482406616, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the habit of seeing the people, I', 'tokens_seen': 53250048}\n",
      "{'training_loss': 5.246397624581547, 'validation_loss': 5.228255112965901, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the habit of seeing the young man', 'tokens_seen': 55298048}\n",
      "{'training_loss': 5.407951993457342, 'validation_loss': 5.325484434763591, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little late, and I was very glad', 'tokens_seen': 57346048}\n",
      "{'training_loss': 5.41734029478946, 'validation_loss': 5.404935359954834, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the house, I was in the house', 'tokens_seen': 59394048}\n",
      "{'training_loss': 5.465350377357613, 'validation_loss': 5.421512603759766, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the service of the governor, I', 'tokens_seen': 61442048}\n",
      "{'training_loss': 5.365237737106065, 'validation_loss': 5.3272294998168945, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little boy, and I was a little', 'tokens_seen': 63490048}\n",
      "{'training_loss': 5.2851401830123645, 'validation_loss': 5.278679768244426, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little boy I was a little boy,', 'tokens_seen': 65538048}\n",
      "{'training_loss': 5.221986439268468, 'validation_loss': 5.2703220049540205, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a boy, I was a boy, and', 'tokens_seen': 67586048}\n",
      "{'training_loss': 5.2538106562727585, 'validation_loss': 5.309883753458659, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the habit of the day, I was', 'tokens_seen': 69634048}\n",
      "{'training_loss': 5.354218822414592, 'validation_loss': 5.3962721824646, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I am a little tired, and I am sure I', 'tokens_seen': 71682048}\n",
      "{'training_loss': 5.442393100867837, 'validation_loss': 5.422969261805217, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the habit of eating it, I was', 'tokens_seen': 73730048}\n",
      "{'training_loss': 5.2892504869881325, 'validation_loss': 5.2385164101918535, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little boy, and I was glad I', 'tokens_seen': 75778048}\n",
      "{'training_loss': 5.287092685699463, 'validation_loss': 5.284123420715332, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been a few days in the country, I', 'tokens_seen': 77826048}\n",
      "{'training_loss': 5.407594616130247, 'validation_loss': 5.224161148071289, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the house, I had not been', 'tokens_seen': 79874048}\n",
      "{'training_loss': 5.177501419843254, 'validation_loss': 5.250322500864665, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was going to the station, I found the old', 'tokens_seen': 81922048}\n",
      "{'training_loss': 5.06800982911708, 'validation_loss': 5.088263591130574, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the habit of getting a little more than', 'tokens_seen': 83970048}\n",
      "{'training_loss': 5.272580017477779, 'validation_loss': 5.29364275932312, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been a boy, I had been a boy', 'tokens_seen': 86018048}\n",
      "{'training_loss': 5.307178513478425, 'validation_loss': 5.297147512435913, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had a good time to get a good time to', 'tokens_seen': 88066048}\n",
      "{'training_loss': 5.2351509110402255, 'validation_loss': 5.285568793614705, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little girl, I was not a little', 'tokens_seen': 90114048}\n",
      "{'training_loss': 5.0342076430886475, 'validation_loss': 5.077740112940471, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a boy, I was a boy, and', 'tokens_seen': 92162048}\n",
      "{'training_loss': 5.2734916250584485, 'validation_loss': 5.2744678656260175, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the train, I was in the train', 'tokens_seen': 94210048}\n",
      "{'training_loss': 5.050009428444556, 'validation_loss': 5.1009383996327715, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had finished my work, I was obliged to take', 'tokens_seen': 96258048}\n",
      "{'training_loss': 5.1156008930529575, 'validation_loss': 5.181474526723226, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a boy, I was a boy, and', 'tokens_seen': 98306048}\n",
      "{'training_loss': 5.117173720214327, 'validation_loss': 5.162949879964192, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little tired, I was very tired,', 'tokens_seen': 100354048}\n",
      "{'training_loss': 5.110716165122339, 'validation_loss': 5.090650002161662, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the room, I was surprised to hear', 'tokens_seen': 102402048}\n",
      "{'training_loss': 5.222262843180511, 'validation_loss': 5.271502097447713, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the house, I had been in', 'tokens_seen': 104450048}\n",
      "{'training_loss': 5.172529414548713, 'validation_loss': 5.1849532922108965, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a boy, I was a boy, and', 'tokens_seen': 106498048}\n",
      "{'training_loss': 5.177435018248477, 'validation_loss': 5.128170887629191, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a boy, I was a boy, and', 'tokens_seen': 108546048}\n",
      "{'training_loss': 5.05568640919055, 'validation_loss': 5.115597168604533, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the house, I was told that', 'tokens_seen': 110594048}\n",
      "{'training_loss': 5.075376947047347, 'validation_loss': 5.155094941457112, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a boy, I was a man of the', 'tokens_seen': 112642048}\n",
      "{'training_loss': 5.240047689211571, 'validation_loss': 5.320480108261108, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was going to tell her, I was not to', 'tokens_seen': 114690048}\n",
      "{'training_loss': 4.94392861350108, 'validation_loss': 5.058734178543091, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a boy, I was a boy, I', 'tokens_seen': 116738048}\n",
      "{'training_loss': 5.118492902335474, 'validation_loss': 5.1017210483551025, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the habit of sitting down to rest', 'tokens_seen': 118786048}\n",
      "{'training_loss': 5.116439762762037, 'validation_loss': 5.151132186253865, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the house, I was obliged to', 'tokens_seen': 120834048}\n",
      "{'training_loss': 5.137614751266221, 'validation_loss': 5.072667519251506, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a boy, I was a boy. I', 'tokens_seen': 122882048}\n",
      "{'training_loss': 5.125229075803595, 'validation_loss': 5.109426339467366, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the habit of going to the theatre,', 'tokens_seen': 124930048}\n",
      "{'training_loss': 5.070716259843212, 'validation_loss': 5.096232255299886, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the house, I was not afraid', 'tokens_seen': 126978048}\n",
      "{'training_loss': 5.1519087532819325, 'validation_loss': 5.1566117604573565, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the habit of seeing the place, I', 'tokens_seen': 129026048}\n",
      "{'training_loss': 5.049070697719768, 'validation_loss': 5.097021023432414, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the habit of taking a seat in the', 'tokens_seen': 131074048}\n",
      "{'training_loss': 5.083148940134857, 'validation_loss': 5.112266302108765, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was at the door, I was startled by the', 'tokens_seen': 133122048}\n",
      "{'training_loss': 5.016139394145901, 'validation_loss': 5.024813890457153, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the house, I was not sure of', 'tokens_seen': 135170048}\n",
      "{'training_loss': 5.150305570182153, 'validation_loss': 5.109111229578654, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the habit of eating, I was not', 'tokens_seen': 137218048}\n",
      "{'training_loss': 5.079888521614722, 'validation_loss': 5.1106579303741455, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the habit of doing so I was not', 'tokens_seen': 139266048}\n",
      "{'training_loss': 5.10437149920706, 'validation_loss': 5.085722128550212, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the house, I was in the house', 'tokens_seen': 141314048}\n",
      "{'training_loss': 5.066155668032372, 'validation_loss': 4.981384992599487, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I am in the habit of being in the habit of', 'tokens_seen': 143362048}\n",
      "{'training_loss': 5.048420332245907, 'validation_loss': 5.006102959314982, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a young man, and I was a young', 'tokens_seen': 145410048}\n",
      "{'training_loss': 5.036908553818525, 'validation_loss': 5.034546931584676, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a boy, I was a boy, and', 'tokens_seen': 147458048}\n",
      "{'training_loss': 5.524324465606172, 'validation_loss': 5.317558526992798, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had to the, and the hand. I was', 'tokens_seen': 149506048}\n",
      "{'training_loss': 4.9512631206189175, 'validation_loss': 4.9067542552948, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the habit of doing so, I', 'tokens_seen': 151554048}\n",
      "{'training_loss': 5.168671074560133, 'validation_loss': 5.027886311213176, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the service of the Emperor Napoleon I', 'tokens_seen': 153602048}\n",
      "{'training_loss': 4.980024612556069, 'validation_loss': 5.002913475036621, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the house, I was in the house', 'tokens_seen': 155650048}\n",
      "{'training_loss': 5.309613147024381, 'validation_loss': 5.119669993718465, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the habit of seeing the great man', 'tokens_seen': 157698048}\n",
      "{'training_loss': 5.190066402241335, 'validation_loss': 5.1808920701344805, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the house, I was not able', 'tokens_seen': 159746048}\n",
      "{'training_loss': 5.060533693281271, 'validation_loss': 5.129150946935018, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the country, I had been in', 'tokens_seen': 161794048}\n",
      "{'training_loss': 5.119116597256418, 'validation_loss': 5.152327219645183, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the house, I was in the', 'tokens_seen': 163842048}\n",
      "{'training_loss': 4.9976528377856235, 'validation_loss': 5.034699122111003, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little boy, I was a little boy', 'tokens_seen': 165890048}\n",
      "{'training_loss': 4.9566971568738, 'validation_loss': 5.048414389292399, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the habit of taking the child to', 'tokens_seen': 167938048}\n",
      "{'training_loss': 5.0561082080259165, 'validation_loss': 5.129711230595906, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a child, I was a child, and', 'tokens_seen': 169986048}\n",
      "{'training_loss': 4.936443167217707, 'validation_loss': 4.990743637084961, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little afraid of the old man, I', 'tokens_seen': 172034048}\n",
      "{'training_loss': 4.827413934772298, 'validation_loss': 4.869651635487874, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little girl, I was a little girl', 'tokens_seen': 174082048}\n",
      "{'training_loss': 4.902021561638784, 'validation_loss': 4.955428123474121, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I am here, I shall be here at the end', 'tokens_seen': 176130048}\n",
      "{'training_loss': 4.951780456607625, 'validation_loss': 5.020157893498738, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I am here, I am here, and I am', 'tokens_seen': 178178048}\n",
      "{'training_loss': 4.923922894364696, 'validation_loss': 5.001956860224406, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the habit of taking a seat in the', 'tokens_seen': 180226048}\n",
      "{'training_loss': 5.052766226105771, 'validation_loss': 5.109542449315389, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I am a man, and I am a man,', 'tokens_seen': 182274048}\n",
      "{'training_loss': 4.905958280724994, 'validation_loss': 4.9473082224528, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a boy, I was a boy, and', 'tokens_seen': 184322048}\n",
      "{'training_loss': 5.0511495380078335, 'validation_loss': 5.002947171529134, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little bit tired of the time I was', 'tokens_seen': 186370048}\n",
      "{'training_loss': 4.923578448214774, 'validation_loss': 4.952955722808838, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a boy, I had no idea of the', 'tokens_seen': 188418048}\n",
      "{'training_loss': 5.072673490491964, 'validation_loss': 5.072919607162476, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little tired, I was quite tired,', 'tokens_seen': 190466048}\n",
      "{'training_loss': 5.038331355078745, 'validation_loss': 5.035179456075032, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I have been in the city, I have been in', 'tokens_seen': 192514048}\n",
      "{'training_loss': 5.122451539766991, 'validation_loss': 5.134290138880412, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the house of the King of the Romans', 'tokens_seen': 194562048}\n",
      "{'training_loss': 4.8814033896236095, 'validation_loss': 4.886360804239909, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a boy, I was a boy, and', 'tokens_seen': 196610048}\n",
      "{'training_loss': 4.7719344850313865, 'validation_loss': 4.8014412720998125, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had seen him, I was very much afraid of', 'tokens_seen': 198658048}\n",
      "{'training_loss': 4.9202853623083085, 'validation_loss': 4.952813704808553, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had come to the house, I found that I', 'tokens_seen': 200706048}\n",
      "{'training_loss': 4.993075920363604, 'validation_loss': 4.962827761967977, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a child, I was a child, and', 'tokens_seen': 202754048}\n",
      "{'training_loss': 4.883011389586885, 'validation_loss': 4.930568377176921, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a young man, I was a young man', 'tokens_seen': 204802048}\n",
      "{'training_loss': 4.98637679471808, 'validation_loss': 5.001402695973714, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the house, I had not been', 'tokens_seen': 206850048}\n",
      "{'training_loss': 4.824457099882223, 'validation_loss': 4.887988090515137, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the house I had not seen him', 'tokens_seen': 208898048}\n",
      "{'training_loss': 4.794369729898744, 'validation_loss': 4.838008642196655, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I am ready to go to bed, I will not', 'tokens_seen': 210946048}\n",
      "{'training_loss': 5.013297662896625, 'validation_loss': 5.042021036148071, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a boy, I was a boy, and', 'tokens_seen': 212994048}\n",
      "{'training_loss': 4.858189712136479, 'validation_loss': 4.944854259490967, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the habit of going to the country', 'tokens_seen': 215042048}\n",
      "{'training_loss': 5.001208580146401, 'validation_loss': 4.96134599049886, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the house, I was very much', 'tokens_seen': 217090048}\n",
      "{'training_loss': 4.965231790380963, 'validation_loss': 4.974815448125203, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the habit of making a little girl,', 'tokens_seen': 219138048}\n",
      "{'training_loss': 4.990571620100636, 'validation_loss': 4.992393175760905, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the habit of going to the house,', 'tokens_seen': 221186048}\n",
      "{'training_loss': 4.950558977611994, 'validation_loss': 4.9811975955963135, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a boy, I was a boy, and', 'tokens_seen': 223234048}\n",
      "{'training_loss': 4.862255419714976, 'validation_loss': 4.830143531163533, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the house, I found that I had', 'tokens_seen': 225282048}\n",
      "{'training_loss': 5.0004833674026745, 'validation_loss': 5.028424104054769, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had been in the habit of taking a fancy to', 'tokens_seen': 227330048}\n",
      "{'training_loss': 5.057999699802722, 'validation_loss': 5.079721132914226, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little boy, I was a little boy', 'tokens_seen': 229378048}\n",
      "{'training_loss': 4.7997078976388705, 'validation_loss': 4.824994484583537, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little boy, I was a little boy', 'tokens_seen': 231426048}\n",
      "{'training_loss': 4.873784841117212, 'validation_loss': 4.893712997436523, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a boy, I was a boy, and', 'tokens_seen': 233474048}\n",
      "{'training_loss': 4.8925520686779995, 'validation_loss': 4.899731874465942, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I am not in the least surprised, I am not', 'tokens_seen': 235522048}\n",
      "{'training_loss': 5.080597384501312, 'validation_loss': 5.082147439320882, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a boy, I was a boy, and', 'tokens_seen': 237570048}\n",
      "{'training_loss': 4.926370426759881, 'validation_loss': 4.965878486633301, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was about to leave, I found the door open', 'tokens_seen': 239618048}\n",
      "{'training_loss': 4.987062389567747, 'validation_loss': 5.036612192789714, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little late, I was in a little', 'tokens_seen': 241666048}\n",
      "{'training_loss': 4.752509189864337, 'validation_loss': 4.777925491333008, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the house, and was sitting in the', 'tokens_seen': 243714048}\n",
      "{'training_loss': 4.91785732366271, 'validation_loss': 4.893348058064778, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had finished, I had been thinking of the time', 'tokens_seen': 245762048}\n",
      "{'training_loss': 4.959014601626639, 'validation_loss': 4.934069951375325, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a boy, I was a boy, and', 'tokens_seen': 247810048}\n",
      "{'training_loss': 4.994090031769316, 'validation_loss': 5.027459780375163, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little boy, I was a little boy', 'tokens_seen': 249858048}\n",
      "{'training_loss': 4.9587920075756005, 'validation_loss': 4.964541753133138, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little boy, I was a little boy', 'tokens_seen': 251906048}\n",
      "{'training_loss': 5.049713587356826, 'validation_loss': 5.115740219751994, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I am a little more than a little more than a', 'tokens_seen': 253954048}\n",
      "{'training_loss': 4.934383465071856, 'validation_loss': 4.933902025222778, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little boy, I was a little boy', 'tokens_seen': 256002048}\n",
      "{'training_loss': 4.917019092430503, 'validation_loss': 4.964247862497966, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the habit of sitting in the house,', 'tokens_seen': 258050048}\n",
      "{'training_loss': 4.896474535182371, 'validation_loss': 4.917405843734741, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the habit of writing, I was obliged', 'tokens_seen': 260098048}\n",
      "{'training_loss': 4.853025274761652, 'validation_loss': 4.9057191212972, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the house, I was in the house', 'tokens_seen': 262146048}\n",
      "{'training_loss': 4.923584938049316, 'validation_loss': 4.962397972742717, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little girl, and I was a little', 'tokens_seen': 264194048}\n",
      "{'training_loss': 4.841891369577182, 'validation_loss': 4.86089301109314, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little girl, and I was a little', 'tokens_seen': 266242048}\n",
      "{'training_loss': 5.033937825994976, 'validation_loss': 5.109264691670735, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little boy, I had a little boy', 'tokens_seen': 268290048}\n",
      "{'training_loss': 4.8699648420689465, 'validation_loss': 4.933459202448527, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little boy, I was a little boy', 'tokens_seen': 270338048}\n",
      "{'training_loss': 4.982349613965568, 'validation_loss': 5.046619415283203, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I had done this, I had not the least idea', 'tokens_seen': 272386048}\n",
      "{'training_loss': 4.947216017771575, 'validation_loss': 5.006850560506185, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a boy, I was a boy, and', 'tokens_seen': 274434048}\n",
      "{'training_loss': 5.091353214393227, 'validation_loss': 5.052757104237874, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little girl, I was a little girl', 'tokens_seen': 276482048}\n",
      "{'training_loss': 4.991390301009356, 'validation_loss': 4.937494834264119, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a child, I was a child, and', 'tokens_seen': 278530048}\n",
      "{'training_loss': 5.010401782342943, 'validation_loss': 5.035318215688069, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little girl, and I was a little', 'tokens_seen': 280578048}\n",
      "{'training_loss': 4.970516414965614, 'validation_loss': 4.993996699651082, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I am a little girl, I am a little girl', 'tokens_seen': 282626048}\n",
      "{'training_loss': 4.880820993649757, 'validation_loss': 4.9269176324208575, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the room, I heard the sound of', 'tokens_seen': 284674048}\n",
      "{'training_loss': 4.844176995552193, 'validation_loss': 4.876907110214233, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little girl, she was a little girl', 'tokens_seen': 286722048}\n",
      "{'training_loss': 5.1757590972771075, 'validation_loss': 5.164202133814494, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little boy, and I was a little', 'tokens_seen': 288770048}\n",
      "{'training_loss': 4.955690731436519, 'validation_loss': 5.038031657536824, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a boy, I was a boy, and', 'tokens_seen': 290818048}\n",
      "{'training_loss': 4.802233599000058, 'validation_loss': 4.87324055035909, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was in the room, I was surprised to find', 'tokens_seen': 292866048}\n",
      "{'training_loss': 4.8058832459530585, 'validation_loss': 4.828701575597127, 'device_type': 'cuda', 'epoch': 0, 'example_output': 'When I was a little late, I was not a little', 'tokens_seen': 294914048}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m training_loader = DataLoader(\n\u001b[32m      2\u001b[39m     ltds,\n\u001b[32m      3\u001b[39m     shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      4\u001b[39m     batch_size=\u001b[32m8\u001b[39m,\n\u001b[32m      5\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwalden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhen I\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 195\u001b[39m, in \u001b[36mTrainGPT.train_loader\u001b[39m\u001b[34m(self, training_loader, evaluation_text, evaluation_prompt, epochs)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m    192\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.loss_for_batch(\n\u001b[32m    193\u001b[39m     input_batch, target_batch\n\u001b[32m    194\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.tokens_seen += input_batch.numel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llm-from-scratch/.venv/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llm-from-scratch/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llm-from-scratch/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "training_loader = DataLoader(\n",
    "    ltds,\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    ")\n",
    "model.train_loader(training_loader=training_loader, evaluation_text=walden, evaluation_prompt=\"When I\", epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a8493b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lom), Archibald Macdonald, Dorothy Brown, Cicely Macdonald, Iain Dubh Iain 'Ic. Ailein (b. c. 1665), the Aosdan Matheson (one of his poems was rendered in English by Sir Walter Scott under the title of \"\n",
      "----------------------------------------\n",
      " and sombre symbolism of the Cross owed its power over gross minds to its very repudiation of the joy of life, but the soul cannot healthily concentrate on death, nor can “Holy Dying” replace “Holy Living.” Those early purple and gold mosaics of the Master with His hand\n",
      "----------------------------------------\n",
      "race his steps, without giving a thought to the enormous physical exertion involved. This way was beset with difficulties; the bed of the creek was heaped with the tangled trunks brought down by the freshets. But Jack set his teeth doggedly, and attacking these obstacles, put them behind him one after\n",
      "----------------------------------------\n",
      "s. lēone, lēonan, asf. lēo, and dp. lēonum _‘lion,’ lioness_, _AO_, Æ. [_L._]\n",
      "\n",
      "lēoc (WW 283^21) = l\n",
      "----------------------------------------\n",
      " cloak is prevalent, we see a mixture of all kinds, from the dress of the English or French belle to that of the poorest of our poor in a country town....\n",
      "\n",
      "_Saturday, July 15th._--The architecture is a mixture of Gothic and Grecian. Three orders of pillars, one above\n",
      "----------------------------------------\n",
      " serve them with service true. 70\n",
      "\n",
      " My skill little wit doth give me this combat that here befell, In fitting words and knightly, from beginning to end to tell. But the eye of each flashed triumph as the coming foe he saw, And the heart of each knight waxed joyful, as they nearer\n",
      "----------------------------------------\n",
      " we can better read at first hand.'\n",
      "\n",
      "Nevertheless, she began to turn the pages and to scan here and there through her dainty gold-framed spectacles, while Miss Jenrys began to interrogate me concerning the mysteries of Midway Plaisance.\n",
      "\n",
      "'We hear such very contradictory stories\n",
      "----------------------------------------\n",
      "-Neophyte, thoughtfully accenting the substituted expression.\n",
      "\n",
      "\"Hush, rash Boy,\" said the Seer, sternly. \"Would you oppose your feeble knowledge to the infinite intelligence of the Unmistakable? A word, and you are lost forever.\"\n",
      "\n",
      "The Boy breathed a silent prayer\n",
      "----------------------------------------\n",
      " Democrats, so that this party easily controlled the political situation; from which circumstance a certain brief campaign ended in a most amusing manner. It happened that Thomas H. Workman, brother of William H., once ran for County Clerk, although he was not a Democrat. Billy was naturally much interested in his brother's candidacy,\n",
      "----------------------------------------\n",
      " you only meant things in general? Nothing about Laura?'\n",
      "\n",
      "'Things in general,' repeated Philip; 'bright promises blighted or thrown away--'\n",
      "\n",
      "But he spoke absently, and his eye was following Laura. Amy thought he was thinking of his sister, and was sorry for him. He spoke no\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "dl = DataLoader(\n",
    "    ltds,\n",
    "    shuffle=True,\n",
    "    batch_size=None\n",
    ")\n",
    "def sample_loader(dataloader, n):\n",
    "    i = n\n",
    "    for input_batch, target_batch in dataloader:\n",
    "        if i == 0:\n",
    "            break\n",
    "        i -= 1\n",
    "        text = tokenizer.decode(input_batch.tolist()[:64])\n",
    "        print(text)\n",
    "        print(\"----------------------------------------\")\n",
    "\n",
    "sample_loader(dl, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8802d6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man's best friend is the one who keeps him in the dark.\"  \"But I am a\n",
      "good friend,\" said the King.  \"I am, and I will find out,\" said the\n",
      "King, \"I have a hundred of you on both shores, for they are a great\n",
      "deal more than I am; and that is in a great many places, but I will be\n",
      "here in the afternoon.\"  The King walked to the place where the King\n",
      "was, but he did not stir.  \"I have been waiting for you, Prince,\" said\n",
      "the King, \"to see your good friends; but I know you are in danger\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(model.prompt(\n",
    "    'Man\\'s best friend is',\n",
    "      max_tokens=128, temperature=0.8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca3a686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
