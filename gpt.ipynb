{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9053ed3e",
   "metadata": {},
   "source": [
    "# LLM From Scratch\n",
    "\n",
    "This is a notebook I'm using to re-create the GPT-2 style architecture from the book \"Build a Large Language Model (From Scratch).\"\n",
    "I'm trying to do as much as possible from memory, other than having some notes on what classes and methods to implement.\n",
    "\n",
    "**Required classes:**\n",
    "1. `LayerNorm`\n",
    "2. `GELU`\n",
    "3. `GPT_CONFIG_124M`\n",
    "4. `FeedForward`\n",
    "5. `MultiHeadAttention`\n",
    "6. `TransformerBlock`\n",
    "7. `GPTModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63992f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch and nn.Module for class definitions\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de899cae",
   "metadata": {},
   "source": [
    "## 1. LayerNorm\n",
    "\n",
    "This class is responsible for layer normalization, which takes place _multiple times_ in the GPT architecture.\n",
    "Its purpose is to keep gradient magnitudes within a certain range, to avoid the problems of vanishing gradients and exploding gradients.\n",
    "The concrete goal is to adjust the outputs to have a mean of zero and a variance of one.\n",
    "\n",
    "To accomplish this, we need two values:\n",
    "- the mean: $\\mu = \\frac{(x_1 + x_2 + ... + x_n)}{n}$\n",
    "- the variance: $v = \\frac{(x_1 + \\mu)^2 + (x_2 + \\mu)^2 + ... + (x_n + \\mu)^2}{n} + \\epsilon$\n",
    "\n",
    "The normalized vector is then: $[\\frac{(x_1 - µ)}{\\sqrt{v}}, \\frac{(x_2 - µ)}{\\sqrt{v}}, ..., \\frac{(x_n - µ)}{\\sqrt{v}}]$\n",
    "\n",
    "NOTE: we're dividing by both n and $\\sqrt{v}$ and we need to make sure we never divide by zero. We know that n (the embedding dimension) will never be zero, but the variance could be. For that reason, we add a miniscule value epsilon to the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46c343c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim: int):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.epsilon = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        variance = x.var(dim=-1, keepdim=True, unbiased=False) + self.epsilon\n",
    "        norm = (x - mean) / torch.sqrt(variance)\n",
    "        return self.scale * norm + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a8ea67",
   "metadata": {},
   "source": [
    "## 2. GELU\n",
    "\n",
    "GELU, or Gaussian Error Linear Unit, is the activation function we'll be using. It's similar to RELU, but it's differentiable everywhere (even at zero, where RELU has a sharp corner discontinuity). GELU is also slightly negative between -2 and 0, rather than flatly zero like RELU. This provides a richer range of values for the network to train on.\n",
    "\n",
    "Calculating the GELU for real would take us out of closed-form math, so we'll use a very close approximation here instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f0b6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * 0.5 * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d1c13b",
   "metadata": {},
   "source": [
    "## 3. GPT_CONFIG_124M\n",
    "The configuration paramters for our GPT-2 implementation. These come directly from the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3da9131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class GPTConfigDict(TypedDict):\n",
    "    vocab_size: int        # the number of tokens in the vocabulary\n",
    "    context_length: int    # the maximum number of token vectors to consider at once\n",
    "    emb_dim: int           # the width of the token vectors\n",
    "    n_heads: int           # the number of heads to use for multi-head attention\n",
    "    n_layers: int          # the number of transformer layers to use\n",
    "    drop_rate: float       # the dropout percentage rate\n",
    "    qkv_bias: bool         # whether to use the bias setting for the KQV matrices.\n",
    "\n",
    "GPT_CONFIG_124M: GPTConfigDict = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d86c8",
   "metadata": {},
   "source": [
    "## 4. FeedForward\n",
    "\n",
    "The feed-forward network (or multi-layer perceptron) is the fundamental neural network used in the GPT model.\n",
    "It expands the number of outputs in a hidden layer before shrinking back down to the original size for the output.\n",
    "This allows the network to explore a richer space, while preserving the input and output dimensions to keep the overall architecture simple.\n",
    "\n",
    "In this example, we'll expand the dimensions by a factor of 4 for the internal layer. I would normally say that should be configurable, but the book just has it fixed at 4. Anyway, that means that our 768 parameters will expand to 3,072, then shrink back down to 768 for output.\n",
    "\n",
    "### How many layers?\n",
    "\n",
    "If you look at a diagram of a feed-forward network, you'll see three layers:\n",
    "1. a left-most layer with n weights\n",
    "2. a middle layer with n*4 weights (or some other factor)\n",
    "3. a right-most layer with n weights again.\n",
    "\n",
    "However, if you look at the implementation below, it kind of seems like there are two linear layers.\n",
    "Well, as you might guess, the middle layer is really the connection between the first and the second layers.\n",
    "The first layer has `dim_internal` outputs, and the second layer has `dim_internal` inputs. These represent overlapping,\n",
    "connected points—just as you might see in the diagram.\n",
    "\n",
    "You could think about like this: each `nn.Linear` has two sides, and of the four total sides there are two that overlap in the center. Thus you get three layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ee38523",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg: GPTConfigDict): \n",
    "        super().__init__()\n",
    "        expansion_factor = 4\n",
    "        dim_external = cfg[\"emb_dim\"]\n",
    "        dim_internal = expansion_factor * dim_external\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(dim_external, dim_internal),\n",
    "            GELU(),\n",
    "            nn.Linear(dim_internal, dim_external),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b36cd1c",
   "metadata": {},
   "source": [
    "## 5. MultiHeadAttention\n",
    "\n",
    "This is the heart of what makes GPT different to earlier language models. The attention mechanism tweaks context vectors in response to earlier tokens in the sequence, shifting their \"meaning\" to become much richer and more specific than a single word could be.\n",
    "\n",
    "### Motivating Examples\n",
    "\n",
    "For example, take the sentence \"the cat sat on the mat because it was warm.\" The word \"it\" has one particular vector embedding in the vocabulary, which might relate loosely to concepts like \"noun\" and \"non-human.\" That's not enough to capture the meaning of \"it\" in this sentence, where it most likely refers to \"mat.\" Attention allows the system to change the vector for the \"it\" token to resemble the vector for \"mat,\" clarifying its meaning in the context of the sentence.\n",
    "\n",
    "That's about the simplest possible example, but in reality each token is pushed and pulled in much more subtle ways by many more tokens in the sequence, so that by the end it somehow represents the meaning of the entire sequence of text. Ultimately, the attention-modulated vector of the final token in the sequence is _the only input needed_ to predict the next token. That's pretty wild.\n",
    "\n",
    "For a more contrived example of what this means, take another example sequence: \"This gritty, romantic, windswept, ornate, melancholic city is none other than\". The word \"than\" has nothing to do with any particular city or place, but by the time its vector is modulated by this long series of words preceding it, it will be something that appears close (in embedding space) to cities like Lisbon and Istanbul. Indeed, those are the two most likely predictions for the final word in the sequence from GPT-3.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Multi-head attention was first described in \"Attention is All You Need\" (2017), in sections 3.2.1 (scaled dot-product attention) and 3.2.2 (extending to multiple heads). I'll be using that paper as a reference for the following two sections.\n",
    "\n",
    "#### Scaled Dot-Product Attention\n",
    "\n",
    "Each attention head is an instance of something called \"scaled dot-product attention,\" which is given by:\n",
    "\n",
    "$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "\n",
    "That is, the attention weights given matrices K, Q, and V are the result of applying softmax to the product of Q times K-transpose over the square root of the embedding size of K, all multiplied by V.\n",
    "\n",
    "I'll try to break that down a bit more:\n",
    "- Q, K, and V are trainable matrix parameters with the same dimensions as the token embedding vectors. They are short for Query, Key, and Value.\n",
    "  - I think of the Query parameter as representing what a token is \"looking for\" to know if another token is worth attending to.\n",
    "  - To continue that metaphor, the Key parameter is what other tokens \"look like\" to the Query.\n",
    "  - The Value is the real identity of the tokens that are found, their deeper reality beneath the appearance presented by the Key.\n",
    "  - To sum up, a token's Query is used to examine every other token's Key to see if it's a good match. If it is, we use that token's Value in attention weight.\n",
    "- Multiplying Q by the transpose of K gives us the dot product of every Query row against every Key row. In other words, it tells us how aligned every Query is with every Key.\n",
    "- We scale that by the inverse square root of the Key dimensions to counteract a known issue with dot-product attention: \"for large values of d_k, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.\" (\"Attention is All You Need,\" p. 4). In other words, the dot product of two rows is going to tend to get larger the more columns you have, and these large values make it hard for training to adjust weights effectively. Scaling by the square root of the number of columns helps to solve this.\n",
    "- Applying softmax turns these scaled dot products into weights.\n",
    "- Multiplying by V translates the weights by Key into weights by Value.\n",
    "\n",
    "Note: it's not described in detail in the paper, but there's an important step carried out here called masking. Essentially, we only want Queries to find Keys that _precede_ them in the sequence. We accomplish this by zeroing out values above the main diagonal. To make sure that these values are zero _after_ softmax, we first set them to minus-infinity.\n",
    "\n",
    "#### Multi-Head Attention\n",
    "\n",
    "In single-headed dot-product attention, Q, K, and V all have the same dimensions as the input and output embeddings. To use multiple heads, we divide the width of each parameter by the number of heads and concatenate them together. This results in the same overall dimensions, but with different sets of columns relating to different Value vectors:\n",
    "\n",
    "$\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, ..., head_h)W^O$\n",
    "\n",
    "$\\text{ where } head_i = \\text{Attention}(Q_iW_i^Q, K_iW_i^K, V_iW_i^V)$\n",
    "\n",
    "$\\text{ where } W_i^Q \\in \\mathbb{R}^{d_{model} \\times d_k}$, $W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}$, $ W_i^V \\in \\mathbb{R}^{d_{model} \\times d_v}$, $W_i^O \\in \\mathbb{R}^{hd_{model} \\times d_{model}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf643cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in: int, d_out: int, context_length: int, dropout: float, num_heads: int, qkv_bias: bool=False):\n",
    "        super().__init__()\n",
    "        if d_out % num_heads != 0:\n",
    "            raise ValueError(\"The number of heads must evenly divide d_out.\")\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_width = d_out // num_heads\n",
    "        self.qkv_bias = qkv_bias\n",
    "\n",
    "        # construct the weights for Q, K, and V.\n",
    "        # these will be registered as trainable parameters automatically.\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # and the output projection, also trainable.\n",
    "        self.w_out = nn.Linear(d_out, d_out)\n",
    "        \n",
    "        # and the dropout layer. not trainable, just drops random values\n",
    "        # to zero with a probability determined by the dropout parameter\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # and the mask, which prevents each token from \"seeing\" later ones\n",
    "        mask = torch.triu( # an upper triangular matrix\n",
    "            torch.ones(context_length, context_length), # consisting of ones\n",
    "            diagonal=1, # starting one row above the diagonal, leaving the diagonal itself as zeroes.\n",
    "        )\n",
    "        self.register_buffer(\"mask\", mask) # register this tensor as non-trainable, but keep it on the same device\n",
    "        self.mask: torch.Tensor # to make the type-checker happy\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch, num_tokens, d_in = x.shape\n",
    "        queries = self.w_query(x)\n",
    "        keys = self.w_key(x)\n",
    "        values = self.w_value(x)\n",
    "\n",
    "        # Split the last dimension of the tensors into multiple heads\n",
    "        q_heads = queries.view(batch, num_tokens, self.num_heads, self.head_width)\n",
    "        k_heads = keys.view(batch, num_tokens, self.num_heads, self.head_width)\n",
    "        v_heads = values.view(batch, num_tokens, self.num_heads, self.head_width)\n",
    "\n",
    "        #                                  [  0  ,     1     ,    2     ,      3    ]\n",
    "        # {q,k,v}_heads now have the shape [batch, num_tokens, num_heads, head_width],\n",
    "        # but we want them to be:          [batch, num_heads, num_tokens, head_width]\n",
    "        q_heads = q_heads.transpose(1, 2)\n",
    "        k_heads = k_heads.transpose(1, 2)\n",
    "        v_heads = v_heads.transpose(1, 2)\n",
    "\n",
    "        # now we need to calculate the raw dot-product attention scores between Q and K^T,\n",
    "        # where K^T has the shape [batch, num_heads, head_width, num_tokens].\n",
    "        # that gives attention_scores the shape [batch, num_heads, num_tokens, num_tokens]\n",
    "        attention_scores = q_heads @ k_heads.transpose(2, 3)\n",
    "        # and apply the causal mask\n",
    "        mask = self.mask[:num_tokens, :num_tokens]\n",
    "        attention_scores = attention_scores.masked_fill(mask == 1, float('-inf'))\n",
    "\n",
    "        # and we construct the weights using softmax on the scaled final dimension\n",
    "        attention_weights = torch.softmax(attention_scores / self.head_width**0.5, dim=-1)\n",
    "        # and apply dropout\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        #                                 [  0  ,     1    ,     2     ,     3     ]\n",
    "        # attention_weights has the shape [batch, num_heads, num_tokens, num_tokens]\n",
    "        # v_heads has the shape:          [batch, num_heads, num_tokens, head_width]\n",
    "        # if we multiply them, we get:    [batch, num_heads, num_tokens, head_width]\n",
    "        # but in the end, we want:        [batch, num_tokens, d_out]\n",
    "        context = attention_weights @ v_heads # [batch, num_heads, num_tokens, head_width]\n",
    "\n",
    "        # so we need to first transpose and get [batch, num_tokens, num_heads, head_width]\n",
    "        context = context.transpose(1, 2)\n",
    "        # and then concatenate the last two dimensions together to get d_out\n",
    "        context = context.contiguous().view(batch, num_tokens, self.d_out)\n",
    "        # and multiply by the output projection\n",
    "        return self.w_out(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2047eee4",
   "metadata": {},
   "source": [
    "## 6. TransformerBlock\n",
    "\n",
    "This version of the transformer block is loosely based on \"Attention is All You Need\" section 3, but includes _only_ the decoder stack. The encoder stack is omitted from the GPT architecture, and thus from the Build a Large Language Model (From Scratch) book.\n",
    "\n",
    "The transformer block goes a little something like this:\n",
    "```\n",
    "Tokenized Text -> LayerNorm 1 -> MultiHeadAttention -> Dropout -> (+) -> LayerNorm 2 -> FeedForward -> Dropout -> (+) -> Output\n",
    "```\n",
    "\n",
    "Where `(+)` represents a shortcut connection, where a previous state is added back in to reinforce weights that are getting very small.\n",
    "\n",
    "As far as requirements:\n",
    "- I've already implemented the the LayerNorm, MultiHeadAttention, and FeedForward classes.\n",
    "- `nn.Dropout` is provided by PyTorch.\n",
    "- Shortcut connections just use ordinary variables and addition.\n",
    "\n",
    "So we're all set to put these elements together below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3fa3ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single GPT-2 transformer block.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: GPTConfigDict):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.attention = MultiHeadAttention(\n",
    "            cfg[\"emb_dim\"],\n",
    "            cfg[\"emb_dim\"],\n",
    "            cfg[\"context_length\"],\n",
    "            cfg[\"drop_rate\"],\n",
    "            cfg[\"n_heads\"],\n",
    "            cfg[\"qkv_bias\"],\n",
    "        )\n",
    "        self.drop_rate = cfg[\"drop_rate\"]\n",
    "        self.layer_norm_2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.feedforward = FeedForward(cfg)\n",
    "        self.dropout = nn.Dropout(self.drop_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        shortcut = x\n",
    "        x = self.layer_norm_1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.layer_norm_2(x)\n",
    "        x = self.feedforward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec0224e",
   "metadata": {},
   "source": [
    "## 7. GPTModel\n",
    "\n",
    "This is the big one, where everything comes together.\n",
    "The hard parts are all pretty much done, this is going to be just a bit more glue.\n",
    "\n",
    "The flow here goes like:\n",
    "```\n",
    "Tokenized Text -> Token Embedding Layer -> Positional Embedding Layer -> Dropout -> TransformerBlocks -> LayerNorm -> Output\n",
    "```\n",
    "\n",
    "Or, in detail:\n",
    "1. Tokenized Text: the tokenizer is outside of this module; we'll get to that later.\n",
    "2. Token Embedding Layer: this is a trainable `nn.Embedding` layer that starts out with random weights. It maps tokens to the embedding space.\n",
    "3. Positional Embedding Layer: very similar to the Token Embedding Layer, but encodes positional information rather than \"semantic\" content.\n",
    "4. Dropout: provided by `nn.Dropout` with a configurable drop rate.\n",
    "5. TransformerBlocks: implemented above. We'll have a number of these set by config, and they run in serial.\n",
    "6. LayerNorm: also implemented above. This keeps all values in the tensors in a range of [-1, 1], with a mean of 0.\n",
    "7. Output: the outputs are called \"logits,\" and they represent the likelihood that the following token will be the one with a given ID. In order to project these from the previous LayerNorm, we'll need the size to be $\\text{emb\\_dim} \\times \\text{vocab\\_size}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7369c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Top-level GPT-2 model.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: GPTConfigDict):\n",
    "        \"\"\"Initialize model with config.\"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.positional_embedding = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.layer_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.output = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass: input indices to logits.\"\"\"\n",
    "        batch_size, sequence_length = in_idx.shape\n",
    "        token_embeddings = self.token_embedding(in_idx)\n",
    "        positional_embeddings = self.positional_embedding(\n",
    "            # get the first N positional embeddings, where N is the sequence length\n",
    "            torch.arange(sequence_length, device=in_idx.device)\n",
    "        )\n",
    "\n",
    "        x = token_embeddings + positional_embeddings\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.output(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311222f7",
   "metadata": {},
   "source": [
    "# Smoke Test\n",
    "\n",
    "If everything above has worked, then we should be able to exactly replicate the results from the book as long as we use the same seed (123).\n",
    "\n",
    "Use the `smoke_test` function below to get the predicted completion for a given prompt from the _untrained_ LLM.\n",
    "\n",
    "Note: because the LLM is still untrained, the result will be total garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db9e0bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    \"\"\"\n",
    "    A helper function used by smoke_test. It's easier to pass the prompt to smoke_test, rather than call this directly.\n",
    "    \"\"\"\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probabilities, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n",
    "\n",
    "def smoke_test(prompt):\n",
    "    \"\"\"\n",
    "    Pass the prompt to the (untrained) GPT model with a manual seed. Should correspond to the expected output.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(123)\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    encoded = tokenizer.encode(prompt)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    model.eval()\n",
    "    out = generate_text_simple(\n",
    "        model,\n",
    "        encoded_tensor,\n",
    "        6,\n",
    "        GPT_CONFIG_124M[\"context_length\"]\n",
    "    )\n",
    "    decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "    print(decoded_text)\n",
    "\n",
    "smoke_test(\"Hello, I am\") # should output \"Hello, I am Featureiman Byeswickattribute argue\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed1431c",
   "metadata": {},
   "source": [
    "# Training a smaller GPT-2\n",
    "\n",
    "What follows is the code to train a version of this architecture. Because training is computationally expensive, I'm going to reduce the context length to a more manageable 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2416bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_MINI: GPTConfigDict = {**GPT_CONFIG_124M, \"context_length\": 256} # 1024 is just too big to train locally\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_MINI)\n",
    "model.eval(); # disables dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bf31ff",
   "metadata": {},
   "source": [
    "## Convenience Functions and Example\n",
    "\n",
    "We're adding a few functions to make it easier to interact with the model. These might've been useful in the smoke test above, so maybe I'll refactor a bit later.\n",
    "\n",
    "Also, another quick example of how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c38a43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "def text_to_token_ids(text: str, tokenizer: tiktoken.Encoding) -> torch.Tensor:\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids: torch.Tensor, tokenizer: tiktoken.Encoding) -> str:\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "# example:\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_MINI[\"context_length\"],\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf453bb",
   "metadata": {},
   "source": [
    "# Calculating Loss\n",
    "\n",
    "This is a temporary helper to show the training and validation loss scores for a given corpus. It still only uses the untrained model, so the results are guaranteed to be garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fed6fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text: str, tokenizer: tiktoken.Encoding, max_length: int, stride: int):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            start = i\n",
    "            end = start + max_length\n",
    "            input_chunk = token_ids[start:end]\n",
    "            target_chunk = token_ids[start+1:end+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "class LossCalculator:\n",
    "    def __init__(self, cfg: GPTConfigDict, train_ratio:float=0.9):\n",
    "        self.cfg = cfg\n",
    "        self.model = GPTModel(self.cfg)\n",
    "        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.train_ratio = train_ratio\n",
    "        self.device = self.get_device()\n",
    "\n",
    "    def get_device(self) -> torch.device:\n",
    "        if torch.cuda.is_available(): # type: ignore[attr-defined]\n",
    "            return torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available(): # type: ignore[attr-defined]\n",
    "            return torch.device(\"mps\")\n",
    "        else:\n",
    "            return torch.device(\"cpu\")\n",
    "\n",
    "    def create_dataloader(self, text: str, batch_size:int=4, max_length:int=256, stride:int=128, shuffle:bool=True, drop_last:bool=True, num_workers:int=0) -> DataLoader:\n",
    "        dataset = GPTDatasetV1(text, self.tokenizer, max_length, stride)\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            drop_last=drop_last,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "\n",
    "    def loss_for_batch(self, input_batch: torch.Tensor, target_batch: torch.Tensor):\n",
    "        input_batch, target_batch = input_batch.to(self.device), target_batch.to(self.device)\n",
    "        logits = self.model(input_batch)\n",
    "        return nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "\n",
    "    def calc_loss_loader(self, data_loader, num_batches=None):\n",
    "        total_loss = 0\n",
    "        if len(data_loader) == 0:\n",
    "            return float(\"nan\")\n",
    "        elif num_batches is None:\n",
    "            num_batches = len(data_loader)\n",
    "        else:\n",
    "            num_batches = min(num_batches, len(data_loader))\n",
    "        \n",
    "        for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "            if i < num_batches:\n",
    "                loss = self.loss_for_batch(input_batch, target_batch)\n",
    "                total_loss += loss.item()\n",
    "            else:\n",
    "                break\n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def run(self, text: str, max_length:int=0, stride:int=0):\n",
    "        split_idx = int(self.train_ratio * len(text))\n",
    "        train_data = text[:split_idx]\n",
    "        validation_data = text[split_idx:]\n",
    "        torch.manual_seed(123)\n",
    "        if stride == 0:\n",
    "            stride = self.cfg[\"context_length\"]\n",
    "        if max_length == 0:\n",
    "            max_length = self.cfg[\"context_length\"]\n",
    "        train_loader = self.create_dataloader(\n",
    "            train_data,\n",
    "            batch_size=2,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "        )\n",
    "        validation_loader = self.create_dataloader(\n",
    "            validation_data,\n",
    "            batch_size=2,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            training_loss = self.calc_loss_loader(train_loader)\n",
    "            validation_loss = self.calc_loss_loader(validation_loader)\n",
    "        elapsed = time.time() - start\n",
    "        return {\n",
    "            \"training_loss\": training_loss,\n",
    "            \"validation_loss\": validation_loss,\n",
    "            \"device_type\": self.device.type,\n",
    "            \"time_seconds\": elapsed\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea9024",
   "metadata": {},
   "source": [
    "# Example of Loss\n",
    "\n",
    "The `verdict_loss()` function is basically another smoke test. It loads the public domain book _The Verdict_ and passes it to the untrained model to calculate the loss metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aaed8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_loss': 10.98558121919632,\n",
       " 'validation_loss': 10.998483152950511,\n",
       " 'device_type': 'cuda',\n",
       " 'time_seconds': 4.2821033000946045}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "def verdict_loss():\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "    text_data = \"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode('utf-8')\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "    lc = LossCalculator(GPT_CONFIG_MINI)\n",
    "    return lc.run(text_data, max_length=256, stride=8)\n",
    "\n",
    "verdict_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f7ff51",
   "metadata": {},
   "source": [
    "# Training a Model\n",
    "\n",
    "The following class is nearly a copy of the LossCalculator class above, but it actually trains the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7019ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import json\n",
    "\n",
    "class DecodingStrategy(Enum):\n",
    "    Greedy = 1\n",
    "    Multinomial = 2\n",
    "    TopK = 3\n",
    "\n",
    "class TrainGPT:\n",
    "    def __init__(self, cfg: GPTConfigDict, eval_frequency:int=5, train_ratio:float=0.9, force_cpu:bool=False):\n",
    "        self.cfg = cfg\n",
    "        self.model = GPTModel(self.cfg)\n",
    "        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.train_ratio = train_ratio\n",
    "        self.force_cpu = force_cpu\n",
    "        self.device = self.get_device()\n",
    "        self.optimizer = torch.optim.AdamW( # type: ignore[attr-defined]\n",
    "            self.model.parameters(),\n",
    "            lr=0.0004,\n",
    "            weight_decay=0.1,\n",
    "        )\n",
    "        self.eval_frequency = eval_frequency\n",
    "        self.tokens_seen, self.global_step = 0, -1\n",
    "\n",
    "    def save(self, name: str):\n",
    "        torch.save({\n",
    "            \"model_state_dict\": self.model.state_dict(),\n",
    "            \"optimizer_state_dict\": self.optimizer.state_dict()\n",
    "        },\n",
    "        f\"{name}.pth\")\n",
    "\n",
    "    def load(self, name: str):\n",
    "        checkpoint = torch.load(f\"{name}.pth\")\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "    def get_device(self) -> torch.device:\n",
    "        if self.force_cpu:\n",
    "            return torch.device(\"cpu\")\n",
    "        if torch.cuda.is_available(): # type: ignore[attr-defined]\n",
    "            return torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available(): # type: ignore[attr-defined]\n",
    "            return torch.device(\"mps\")\n",
    "        else:\n",
    "            return torch.device(\"cpu\")\n",
    "\n",
    "    def create_dataloader(self, text: str, batch_size:int=4, max_length:int=256, stride:int=128, shuffle:bool=True, drop_last:bool=True, num_workers:int=0) -> DataLoader:\n",
    "        dataset = GPTDatasetV1(text, self.tokenizer, max_length, stride)\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            drop_last=drop_last,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "\n",
    "    def loss_for_batch(self, input_batch: torch.Tensor, target_batch: torch.Tensor) -> torch.Tensor:\n",
    "        input_batch, target_batch = input_batch.to(self.device), target_batch.to(self.device)\n",
    "        logits = self.model(input_batch)\n",
    "        return nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten()).to(self.device)\n",
    "\n",
    "    def calc_loss_loader(self, data_loader, num_batches=None) -> float:\n",
    "        total_loss = 0\n",
    "        if len(data_loader) == 0:\n",
    "            return float(\"nan\")\n",
    "        elif num_batches is None:\n",
    "            num_batches = len(data_loader)\n",
    "        else:\n",
    "            num_batches = min(num_batches, len(data_loader))\n",
    "        \n",
    "        for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "            if i < num_batches:\n",
    "                loss = self.loss_for_batch(input_batch, target_batch)\n",
    "                total_loss += loss.item()\n",
    "            else:\n",
    "                break\n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def loaders(self, text: str, max_length:int=0, stride:int=0) -> tuple[DataLoader, DataLoader]:\n",
    "        split_idx = int(self.train_ratio * len(text))\n",
    "        train_data = text[:split_idx]\n",
    "        validation_data = text[split_idx:]\n",
    "        torch.manual_seed(123)\n",
    "        if stride == 0:\n",
    "            stride = self.cfg[\"context_length\"]\n",
    "        if max_length == 0:\n",
    "            max_length = self.cfg[\"context_length\"]\n",
    "        train_loader = self.create_dataloader(\n",
    "            train_data,\n",
    "            batch_size=2,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "        )\n",
    "        validation_loader = self.create_dataloader(\n",
    "            validation_data,\n",
    "            batch_size=2,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        return (train_loader, validation_loader)\n",
    "    \n",
    "    def evaluate(self, text: str, max_length:int=0, stride:int=0, epoch:int=0, prompt:str=\"\"):\n",
    "        train_loader, validation_loader = self.loaders(text, max_length, stride)\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            training_loss = self.calc_loss_loader(train_loader)\n",
    "            validation_loss = self.calc_loss_loader(validation_loader)\n",
    "        elapsed = time.time() - start\n",
    "        summary = {\n",
    "            \"training_loss\": training_loss,\n",
    "            \"validation_loss\": validation_loss,\n",
    "            \"device_type\": self.device.type,\n",
    "            \"time_seconds\": elapsed,\n",
    "            \"epoch\": epoch\n",
    "        }\n",
    "        if len(prompt) > 0:\n",
    "            example_output = self.prompt(prompt)\n",
    "            summary[\"example_output\"] = example_output\n",
    "        return summary\n",
    "\n",
    "    def choose(self, strategy: DecodingStrategy, logits: torch.Tensor, temperature:float=1.0, k:int=10):\n",
    "        match strategy:\n",
    "            case DecodingStrategy.Greedy:\n",
    "                probabilities = torch.softmax(logits, dim=-1)\n",
    "                result = torch.argmax(probabilities, dim=-1, keepdim=True)\n",
    "                return result\n",
    "            case DecodingStrategy.Multinomial:\n",
    "                scaled = logits / temperature\n",
    "                probabilities = torch.softmax(scaled, dim=-1)\n",
    "                result = torch.multinomial(probabilities, num_samples=1)\n",
    "                return result\n",
    "            case DecodingStrategy.TopK:\n",
    "                top_logits, top_pos = torch.topk(logits, k)\n",
    "                filtered = torch.full_like(\n",
    "                    logits, -torch.inf\n",
    "                )\n",
    "                filtered[top_pos] = logits[top_pos]\n",
    "                scaled = filtered / temperature\n",
    "                probabilities = torch.softmax(scaled, dim=-1)\n",
    "                return torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "    def generate_text_simple(self, token_ids: torch.Tensor, max_new_tokens, context_size):\n",
    "        self.model.to(self.device)\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = token_ids[:, -context_size:]\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            idx_next = self.choose(DecodingStrategy.Greedy, logits, temperature=0.1)\n",
    "            token_ids = torch.cat((token_ids, idx_next), dim=1)\n",
    "        return token_ids\n",
    "\n",
    "    def prompt(self, text: str, max_tokens:int=10) -> str:\n",
    "        encoded = self.tokenizer.encode(text)\n",
    "        encoded_tensor = torch.tensor(encoded).unsqueeze(0).to(self.device)\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        out = self.generate_text_simple(\n",
    "            encoded_tensor,\n",
    "            max_tokens,\n",
    "            self.cfg[\"context_length\"],\n",
    "        )\n",
    "        decoded_text = self.tokenizer.decode(out.squeeze(0).tolist())\n",
    "        return decoded_text\n",
    "\n",
    "    def generate_and_print_sample(self, prompt: str) -> None:\n",
    "        print(self.prompt(prompt))\n",
    "\n",
    "    def train(self, text: str, max_length:int=0, stride:int=0, epochs:int=10, prompt:str=\"Hello, I am \"):\n",
    "        torch.manual_seed(123)\n",
    "        loss_summaries = []\n",
    "        training_loader, _ = self.loaders(text, max_length, stride)\n",
    "        self.model.to(self.device)\n",
    "        self.tokens_seen = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            for input_batch, target_batch in training_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.loss_for_batch(\n",
    "                    input_batch, target_batch\n",
    "                )\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.tokens_seen += input_batch.numel()\n",
    "                self.global_step += 1\n",
    "\n",
    "                if self.global_step % self.eval_frequency == 0:\n",
    "                    summary = self.evaluate(text, max_length, stride, epoch, prompt)\n",
    "                    summary[\"tokens_seen\"] = self.tokens_seen\n",
    "                    print(summary)\n",
    "                    loss_summaries.append(summary)\n",
    "        \n",
    "        self.generate_and_print_sample(prompt)\n",
    "        return loss_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53c474f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verdict_train():\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "    text_data = \"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode('utf-8')\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "    trainable_model = TrainGPT(GPT_CONFIG_MINI, force_cpu=False, eval_frequency=5)\n",
    "    return trainable_model.train(text_data, epochs=10, prompt=\"Every effort moves you\")\n",
    "\n",
    "# verdict_train() # uncomment to see the results of training this LLM on \"The Verdict\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb011898",
   "metadata": {},
   "source": [
    "# Training on Project Gutenberg\n",
    "\n",
    "To see how far I can take this, I'm going to try to train a model on more and more text. I don't really know how this is going to go, and I'm way beyond either the book or the lectures now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0ff1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigDatasetTrainer:\n",
    "    def __init__(self, trainable_model: TrainGPT, dataset):\n",
    "        self.trainable_model = trainable_model\n",
    "        self.dataset = dataset\n",
    "        self.next_item = 0\n",
    "        self.done = []\n",
    "        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    def save(self, name: str):\n",
    "        self.trainable_model.save(name)\n",
    "        json_state = {\n",
    "            'done': self.done,\n",
    "            'next_item': self.next_item,\n",
    "        }\n",
    "        with open(f\"{name}.json\", 'w') as f:\n",
    "            f.write(json.dumps(json_state))\n",
    "        print(f\"Saved {name}.pth and {name}.json\")\n",
    "    \n",
    "    def load(self, name: str):\n",
    "        self.trainable_model.load(name)\n",
    "        json_state = {}\n",
    "        with open(f\"{name}.json\", 'r') as f:\n",
    "            contents = f.read()\n",
    "            json_state = json.loads(contents)\n",
    "        self.done = json_state['done']\n",
    "        self.next_item = json_state['next_item']\n",
    "        print(f\"Restored state from {name}.pth and {name}.json\")\n",
    "    \n",
    "    def train_idx(self, n:int):\n",
    "        self.next_item = n\n",
    "        self.train_next() # feels a little dirty to do it this way, but I can refactor later\n",
    "    \n",
    "    def validate(self, text: str) -> bool:\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        if len(tokens) < 1000:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def train_next(self):\n",
    "        if self.next_item in self.done:\n",
    "            print(f\"Skipping {self.next_item}: already done.\")\n",
    "            return\n",
    "        text = self.dataset[\"train\"][self.next_item][\"text\"]\n",
    "        if not self.validate(text):\n",
    "            print(f\"Skipping {self.next_item}: too short.\")\n",
    "        self.trainable_model.train(text, max_length=256, stride=128, epochs=1, prompt=\"It is good\")\n",
    "        self.done.append(self.next_item)\n",
    "        self.next_item += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "575a4786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset and creating model.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "pg = load_dataset(\"deepmind/pg19\")\n",
    "def reload_trainable_model():\n",
    "    print(\"Loading dataset and creating model.\")\n",
    "    trained_model = TrainGPT(cfg=GPT_CONFIG_MINI, eval_frequency=500)\n",
    "    return BigDatasetTrainer(trained_model, pg)\n",
    "\n",
    "try:\n",
    "    trainer\n",
    "    print(\"trainer is already set\")\n",
    "except NameError:\n",
    "    trainer = reload_trainable_model()\n",
    "    complete = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6bfd4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored state from backup.pth and backup.json\n",
      "Skipping 70, already done.\n",
      "Done: The Atlantic Monthly Vol. 2 No. 9 July 1858 by Various\n",
      "Saved backup.pth and backup.json\n",
      "{'training_loss': 5.876355979915186, 'validation_loss': 6.343317410656225, 'device_type': 'cuda', 'time_seconds': 14.66250228881836, 'epoch': 0, 'example_output': 'It is good,\\nand the _conde_, the', 'tokens_seen': 512}\n",
      "{'training_loss': 4.044766002880173, 'validation_loss': 4.6313856071400865, 'device_type': 'cuda', 'time_seconds': 14.842806816101074, 'epoch': 0, 'example_output': \"It is good,\\nAnd I am Leah'd, and the\", 'tokens_seen': 256512}\n",
      "It is good,\n",
      "And if thou mayst not know,\n",
      "Done: Divine Comedy Complete by Dante Alighieri\n",
      "{'training_loss': 4.635450719462501, 'validation_loss': 5.335124212762584, 'device_type': 'cuda', 'time_seconds': 3.0099611282348633, 'epoch': 0, 'example_output': 'It is good.\\n\\nThe woman-power is a woman', 'tokens_seen': 52736}\n",
      "It is good, and that it is not only a\n",
      "woman\n",
      "Done: Mobilizing Woman-Power by Harriot Stanton Blatch\n",
      "It is good,\" said Jack, \"but I am afraid I\n",
      "Done: The Boy Allies at Jutland by Robert L. Drake\n",
      "{'training_loss': 3.944064716157459, 'validation_loss': 4.148298079507393, 'device_type': 'cuda', 'time_seconds': 8.617332696914673, 'epoch': 0, 'example_output': 'It is good. It is a man who\\nwas a man', 'tokens_seen': 84480}\n",
      "It is good. I'll tell you everything to me.\"\n",
      "\n",
      "Done: The Hampstead Mystery by John R. Watson and Arthur J. Rees\n",
      "{'training_loss': 4.240872779747475, 'validation_loss': 4.5253335217298085, 'device_type': 'cuda', 'time_seconds': 8.884032249450684, 'epoch': 0, 'example_output': 'It is good.\"\\n\\n\"I don\\'t know what you', 'tokens_seen': 71680}\n",
      "It is good; but I'm not the only one of the\n",
      "Done: The House of the Whispering Pines by Anna Katharine Green\n",
      "{'training_loss': 4.017240055023678, 'validation_loss': 4.271419938872842, 'device_type': 'cuda', 'time_seconds': 5.1805689334869385, 'epoch': 0, 'example_output': 'It is good, and I have not\\nnot to go to', 'tokens_seen': 50688}\n",
      "It is good, and the lynx, and the\n",
      "first\n",
      "Done: Kazan by James Oliver Curwood\n",
      "{'training_loss': 3.818585033735264, 'validation_loss': 3.789516878979547, 'device_type': 'cuda', 'time_seconds': 8.38621711730957, 'epoch': 0, 'example_output': \"It is good.\\n  And the Moorish lady's hand\", 'tokens_seen': 145408}\n",
      "It is good, and I will give you the\n",
      "name of\n",
      "Done: Moorish Literature by Anonymous\n",
      "{'training_loss': 3.534642525405505, 'validation_loss': 4.314879051474637, 'device_type': 'cuda', 'time_seconds': 6.620145082473755, 'epoch': 0, 'example_output': \"It is good to be done, an' I'm goin\", 'tokens_seen': 140800}\n",
      "It is good, an' I'm not\n",
      "nothin'\n",
      "Done: The Minute Boys of the Mohawk Valley by James Otis\n",
      "It is good, an' I'm a-goin'\n",
      "Done: Old Lady Number 31 by Louise Forsslund\n",
      "{'training_loss': 4.533682511207905, 'validation_loss': 5.2265056272347765, 'device_type': 'cuda', 'time_seconds': 7.665142059326172, 'epoch': 0, 'example_output': 'It is good to be\\nunfirmly, and the', 'tokens_seen': 102912}\n",
      "It is good, and the\n",
      "_Mandinga_,\n",
      "Done: Thaumaturgia by An Oxonian\n",
      "Saved backup.pth and backup.json\n",
      "It is good,\n",
      "  And the folk that will be the\n",
      "Done: Elves and Heroes by Donald A. MacKenzie\n",
      "{'training_loss': 1.9327688031726413, 'validation_loss': 1.0549428617132122, 'device_type': 'cuda', 'time_seconds': 3.095980644226074, 'epoch': 0, 'example_output': 'It is good, and the\\n_principal_,', 'tokens_seen': 70656}\n",
      "It is good-by, and the\n",
      "_Answer_.--\n",
      "Done: Punchinello Vol. 2 No. 31 October 29 1870 by Various\n",
      "It is good. I\n",
      "would have a great deal of the\n",
      "Done: Punchinello Vol. 2 No. 30 October 22 1870 by Various\n",
      "It is good. I don't think I can't\n",
      "think\n",
      "Done: The History of Gutta-Percha Willie by George MacDonald\n",
      "{'training_loss': 4.366145265681665, 'validation_loss': 4.144724656235088, 'device_type': 'cuda', 'time_seconds': 6.435007095336914, 'epoch': 0, 'example_output': 'It is good, and I will have thought of it. I', 'tokens_seen': 22016}\n",
      "It is good, and I am sure I\n",
      "believe you\n",
      "Done: A Soldier of Virginia by Burton Egbert Stevenson\n",
      "{'training_loss': 4.389673263832713, 'validation_loss': 5.260158061981201, 'device_type': 'cuda', 'time_seconds': 6.883020639419556, 'epoch': 0, 'example_output': 'It is good to the\\ncountry of the Church, and the', 'tokens_seen': 77824}\n",
      "It is good. Thou hast not the\n",
      "thou hast not\n",
      "Done: The Twilight of the Gods and Other Tales by Richard Garnett\n",
      "It is good.\n",
      "\n",
      "HECUBA.\n",
      "\n",
      "\n",
      "Done: The Trojan Women by Euripides\n",
      "{'training_loss': 4.319928305930104, 'validation_loss': 4.751637582425718, 'device_type': 'cuda', 'time_seconds': 3.8334522247314453, 'epoch': 0, 'example_output': 'It is good. It is a great\\ngreat number of the', 'tokens_seen': 62976}\n",
      "It is good, and\n",
      "that the war is not only to\n",
      "Done: The Healing of Nations and the Hidden Sources of Their Strife\n",
      "It is good, and the\n",
      "French, the French, the\n",
      "Done: Towards The Goal by Mrs. Humphry Ward\n",
      "{'training_loss': 4.337584528214035, 'validation_loss': 5.33144201040268, 'device_type': 'cuda', 'time_seconds': 5.6671302318573, 'epoch': 0, 'example_output': 'It is good, and the\\nworld of the world. The', 'tokens_seen': 72192}\n",
      "It is good, and the\n",
      "poet of the poet's\n",
      "Done: Byron by John Nichol\n",
      "Saved backup.pth and backup.json\n",
      "It is good, and it is a\n",
      "little while he was\n",
      "Done: A Little Boy Lost by W. H. Hudson\n",
      "{'training_loss': 4.02929068707272, 'validation_loss': 4.533890694988017, 'device_type': 'cuda', 'time_seconds': 7.484314918518066, 'epoch': 0, 'example_output': 'It is good, and I am not to\\ncome to the', 'tokens_seen': 58880}\n",
      "It is good, and I am\n",
      "unconscious.\"\n",
      "\n",
      "\n",
      "Done: The Czar's Spy by William Le Queux\n",
      "{'training_loss': 1.6996834232507507, 'validation_loss': 1.005002784729004, 'device_type': 'cuda', 'time_seconds': 3.1571788787841797, 'epoch': 0, 'example_output': 'It is good, and the\\n_Dauntless_ is', 'tokens_seen': 80896}\n",
      "It is good to do with you, and\n",
      "here I have\n",
      "Done: Punchinello Vol. 2 No. 32 November 5 1870 by Various\n",
      "It is good, and we are not a\n",
      "bramble\n",
      "Done: Punchinello Vol. 2 No. 33 November 12 1870 by Various\n",
      "It is good,\n",
      "        \n",
      "Done: Punchinello Vol. 2 No. 34 November 19 1870 by Various\n",
      "{'training_loss': 3.909603261399543, 'validation_loss': 4.637163484418714, 'device_type': 'cuda', 'time_seconds': 5.698509931564331, 'epoch': 0, 'example_output': 'It is good. I am not\\na very good thing.', 'tokens_seen': 70656}\n",
      "It is good. I'm not a\n",
      "serious, sir,\n",
      "Done: The Postmaster's Daughter by Louis Tracy\n",
      "{'training_loss': 3.8946042841556023, 'validation_loss': 4.767266368865966, 'device_type': 'cuda', 'time_seconds': 8.364136219024658, 'epoch': 0, 'example_output': 'It is good\\nto be a good scholar, and I am', 'tokens_seen': 148480}\n",
      "It is good to see the\n",
      "gentry, and I am\n",
      "Done: Boys and Girls from Thackeray by Kate Dickinson Sweetser\n",
      "It is good-minded, and\n",
      "the United States, and\n",
      "Done: American Political Ideas Viewed From The Standpoint\n",
      "{'training_loss': 4.544019237066252, 'validation_loss': 5.382697895169258, 'device_type': 'cuda', 'time_seconds': 13.103914737701416, 'epoch': 0, 'example_output': 'It is good to do so, and\\nthe truth, and', 'tokens_seen': 65536}\n",
      "{'training_loss': 3.7113162555622057, 'validation_loss': 5.072656999031703, 'device_type': 'cuda', 'time_seconds': 13.092965364456177, 'epoch': 0, 'example_output': 'It is good to be the case, and that the\\ngreat', 'tokens_seen': 321536}\n",
      "It is good to\n",
      "be a man of the world, and\n",
      "Done: The Great Events by Famous Historians Vol. 2 by Various\n",
      "{'training_loss': 3.622341277426451, 'validation_loss': 4.270353335887194, 'device_type': 'cuda', 'time_seconds': 9.726306200027466, 'epoch': 0, 'example_output': 'It is good, and the \\nworld, and the grace', 'tokens_seen': 173568}\n",
      "It is good, and is not \n",
      "to be good.\n",
      "Done: All Saints' Day and Other Sermons by Charles Kingsley\n",
      "Saved backup.pth and backup.json\n",
      "{'training_loss': 3.8551506134881928, 'validation_loss': 4.630035244900247, 'device_type': 'cuda', 'time_seconds': 6.863947868347168, 'epoch': 0, 'example_output': 'It is good for the\\nflower of the dead-tree,', 'tokens_seen': 125952}\n",
      "It is good in the\n",
      "favour of the gods, and\n",
      "Done: The Folk-lore of Plants by T. F. Thiselton-Dyer\n",
      "It is good for the\n",
      "same time. The first is the\n",
      "Done: Adonais by Shelley\n",
      "{'training_loss': 4.659192440268226, 'validation_loss': 4.901036500930786, 'device_type': 'cuda', 'time_seconds': 7.427138328552246, 'epoch': 0, 'example_output': 'It is good for the\\nhouse of the church, and the', 'tokens_seen': 28672}\n",
      "It is good to be that the\n",
      "church of the Roman highway\n",
      "Done: England of My Heart - Spring by Edward Hutton\n",
      "{'training_loss': 3.8879929012931855, 'validation_loss': 4.550847273606521, 'device_type': 'cuda', 'time_seconds': 4.128201723098755, 'epoch': 0, 'example_output': 'It is good to\\nbe a native of the tribe of the', 'tokens_seen': 52224}\n",
      "It is good to be the only one who has\n",
      "will be\n",
      "Done: Oriental Literature by Anonymous\n",
      "It is good,\n",
      "  And I've killed my heart.\n",
      "Done: Fairies and Fusiliers by Robert Graves\n",
      "It is good to be a\n",
      "boy, and I'm sure\n",
      "Done: Aunt Jane's Nieces by Edith Van Dyne\n",
      "{'training_loss': 4.197673453941001, 'validation_loss': 4.380303963370945, 'device_type': 'cuda', 'time_seconds': 3.626094341278076, 'epoch': 0, 'example_output': 'It is good to be a\\ndisgraceful, and', 'tokens_seen': 19456}\n",
      "It is good to be a\n",
      "time, and I'm afraid\n",
      "Done: Aunt Jane's Nieces and Uncle John by Edith Van Dyne\n",
      "{'training_loss': 3.9814442171339404, 'validation_loss': 5.024527100955739, 'device_type': 'cuda', 'time_seconds': 7.50281023979187, 'epoch': 0, 'example_output': 'It is good to be\\nthe same time. I am glad', 'tokens_seen': 161792}\n",
      "It is good to be a\n",
      "mannet to me. I\n",
      "Done: The Best Letters of Charles Lamb ed: Edward Gilpin Johnson\n",
      "It is good to be able to get any longer.\n",
      "\n",
      "\n",
      "Done: Home-Life of the Lancashire Factory Folk during the Cotton Famine\n",
      "{'training_loss': 5.002416546403225, 'validation_loss': 5.454586971096877, 'device_type': 'cuda', 'time_seconds': 12.07648253440857, 'epoch': 0, 'example_output': 'It is good to be found in the\\nworld.\\n\\n', 'tokens_seen': 22016}\n",
      "{'training_loss': 3.7693018955495767, 'validation_loss': 5.1654159848282974, 'device_type': 'cuda', 'time_seconds': 12.070234775543213, 'epoch': 0, 'example_output': 'It is good to be\\nto be a great extent that it', 'tokens_seen': 278016}\n",
      "It is good, and\n",
      "he is a man, and a\n",
      "Done: Great Events by Famous Historians Vol. 17 by Charles Francis Horne\n",
      "Saved backup.pth and backup.json\n",
      "{'training_loss': 3.386301245109264, 'validation_loss': 4.829988997323173, 'device_type': 'cuda', 'time_seconds': 5.5005943775177, 'epoch': 0, 'example_output': 'It is good to be a practical man, and I have\\n', 'tokens_seen': 158208}\n",
      "It is good to be a practical man, but I can't\n",
      "Done: The First Men In The Moon by H. G. Wells\n",
      "{'training_loss': 3.8167217774469346, 'validation_loss': 5.080806478857994, 'device_type': 'cuda', 'time_seconds': 17.239564657211304, 'epoch': 0, 'example_output': 'It is good to be a\\nfond. I was a', 'tokens_seen': 241664}\n",
      "{'training_loss': 3.3921380837375197, 'validation_loss': 5.0960598438978195, 'device_type': 'cuda', 'time_seconds': 17.22574543952942, 'epoch': 0, 'example_output': 'It is good, and that I have\\ngot no power to', 'tokens_seen': 497664}\n",
      "It is good, and that I am\n",
      "    \n",
      "Done: The Works of Charles and Mary Lamb Vol. 3 Books for Children\n",
      "It is good,\n",
      "        \n",
      "Done: The Vigil of Venus and Other Poems by Q\n",
      "{'training_loss': 3.6682644811169856, 'validation_loss': 4.718410521137471, 'device_type': 'cuda', 'time_seconds': 7.161687850952148, 'epoch': 0, 'example_output': 'It is good, and\\nyou are not to be sure I', 'tokens_seen': 173056}\n",
      "It is good, and so\n",
      "to be so much as to\n",
      "Done: The Great English Short-Story Writers Vol. 1 by Various\n",
      "{'training_loss': 3.4520941494579014, 'validation_loss': 5.03692710197578, 'device_type': 'cuda', 'time_seconds': 49.92945384979248, 'epoch': 0, 'example_output': 'It is good, and the\\n      ', 'tokens_seen': 206336}\n",
      "{'training_loss': 3.118002223345164, 'validation_loss': 4.933644963927188, 'device_type': 'cuda', 'time_seconds': 49.90174961090088, 'epoch': 0, 'example_output': 'It is good for the\\n    use of the meat', 'tokens_seen': 462336}\n",
      "{'training_loss': 2.916098830567509, 'validation_loss': 4.89850353143983, 'device_type': 'cuda', 'time_seconds': 49.893678426742554, 'epoch': 0, 'example_output': 'It is good for the\\n    time, and is', 'tokens_seen': 718336}\n",
      "{'training_loss': 2.763597486157741, 'validation_loss': 4.881527687331378, 'device_type': 'cuda', 'time_seconds': 49.89329481124878, 'epoch': 0, 'example_output': 'It is good to be eaten as a\\n    as', 'tokens_seen': 974336}\n",
      "{'training_loss': 2.638360049477565, 'validation_loss': 4.901655842086016, 'device_type': 'cuda', 'time_seconds': 49.8905348777771, 'epoch': 0, 'example_output': 'It is good, and the\\n    most esteemed.', 'tokens_seen': 1230336}\n",
      "{'training_loss': 2.5351917260496135, 'validation_loss': 4.908834730568579, 'device_type': 'cuda', 'time_seconds': 49.882972955703735, 'epoch': 0, 'example_output': 'It is good for the reception of the\\n    ', 'tokens_seen': 1486336}\n",
      "It is good for the purpose of the time, and then it\n",
      "Done: The Book of Household Management by Mrs. Isabella Beeton\n",
      "{'training_loss': 3.6758728794667914, 'validation_loss': 5.320246323295262, 'device_type': 'cuda', 'time_seconds': 7.114990234375, 'epoch': 0, 'example_output': 'It is good enough to be\\nto be sure to have a', 'tokens_seen': 165888}\n",
      "It is good to see the\n",
      "whom it is to be\n",
      "Done: The Atlantic Monthly Vol. 1 No. 2 December 1857 by Various\n",
      "{'training_loss': 3.5613532398320453, 'validation_loss': 4.864636500676473, 'device_type': 'cuda', 'time_seconds': 6.46069073677063, 'epoch': 0, 'example_output': 'It is good to the\\nsoul, and that the Church', 'tokens_seen': 199168}\n",
      "It is good to see how\n",
      "the Church is not the mother\n",
      "Done: The Faith of the Millions (2nd series) by George Tyrrell\n",
      "It is good. He\n",
      "had been a man, and he\n",
      "Done: The Lure of the Dim Trails by B. M. Bower\n",
      "It is good, but the\n",
      "poet is not the characteristic\n",
      "Done: Rhetoric and Poetry in the Renaissance by Donald Lemen Clark\n",
      "It is good to make the\n",
      "explanck, and the\n",
      "Done: A Discourse on the Life Character and Writings of Gulian\n",
      "Saved backup.pth and backup.json\n",
      "{'training_loss': 3.6196972551480147, 'validation_loss': 4.996738354365031, 'device_type': 'cuda', 'time_seconds': 1.1568846702575684, 'epoch': 0, 'example_output': \"It is good to be a\\ndinner. I'm sorry\", 'tokens_seen': 22528}\n",
      "It is good. I'm afraid I'm afraid I'm afraid\n",
      "Done: Punch Vol. 153 July 11 1917 ed. by Sir Owen Seaman\n",
      "It is good to be a\n",
      "very good man.\n",
      "\n",
      "\n",
      "Done: Punchinello Vol. 2 No. 35 November 26 1870 by Various\n",
      "It is good to be\n",
      "more than the most important than the\n",
      "Done: Turkey: A Past and a Future by Arnold Joseph Toynbee\n",
      "{'training_loss': 3.633769500148189, 'validation_loss': 4.052558098809194, 'device_type': 'cuda', 'time_seconds': 9.070514917373657, 'epoch': 0, 'example_output': 'It is good\\nman, and I will take thee a good', 'tokens_seen': 98816}\n",
      "It is good friend of the Lea, and I know that\n",
      "Done: The Merry Adventures of Robin Hood by Howard Pyle\n",
      "{'training_loss': 4.05981878930596, 'validation_loss': 4.285261159593409, 'device_type': 'cuda', 'time_seconds': 12.90430760383606, 'epoch': 0, 'example_output': 'It is good, and\\nthat is not so much as a', 'tokens_seen': 70656}\n",
      "{'training_loss': 3.273879436322838, 'validation_loss': 4.111514191735875, 'device_type': 'cuda', 'time_seconds': 12.89708662033081, 'epoch': 0, 'example_output': 'It is good enough to be\\nmore readily, and that the', 'tokens_seen': 326656}\n",
      "It is good enough to be expected to\n",
      "be a good deal\n",
      "Done: Home as Found by James Fenimore Cooper\n",
      "{'training_loss': 3.603963099723928, 'validation_loss': 4.313583676494769, 'device_type': 'cuda', 'time_seconds': 10.05189847946167, 'epoch': 0, 'example_output': 'It is good for the first time.\\n\\nWe were in', 'tokens_seen': 180736}\n",
      "It is good to be found.\n",
      "\n",
      "\"It's a\n",
      "Done: The Oregon Trail by Francis Parkman Jr.\n",
      "{'training_loss': 3.262447334989144, 'validation_loss': 4.579634753863017, 'device_type': 'cuda', 'time_seconds': 4.492247819900513, 'epoch': 0, 'example_output': 'It is good enough to get\\naway. I shall not be', 'tokens_seen': 122880}\n",
      "It is good enough to\n",
      "get it. I shall not have\n",
      "Done: Dracula's Guest by Bram Stoker\n",
      "{'training_loss': 3.821510219829309, 'validation_loss': 4.922996465549913, 'device_type': 'cuda', 'time_seconds': 12.32233476638794, 'epoch': 0, 'example_output': 'It is good to be the first time of the\\nKing of', 'tokens_seen': 238592}\n",
      "It is good to\n",
      "be.\n",
      "\n",
      "The crusaders were\n",
      "Done: The Great Events by Famous Historians Volume 5 by Various\n",
      "It is good to\n",
      "say that it is a thing that it\n",
      "Done: England and the War by Walter Raleigh\n",
      "{'training_loss': 3.3690355059541304, 'validation_loss': 4.076745086246067, 'device_type': 'cuda', 'time_seconds': 1.351733922958374, 'epoch': 0, 'example_output': 'It is good to\\nthe understanding of the understanding.  (', 'tokens_seen': 20480}\n",
      "It is good to know that the idea of the idea of the\n",
      "Done: On the Improvement of the Understanding by Baruch Spinoza\n",
      "Saved backup.pth and backup.json\n",
      "{'training_loss': 3.7062393019423006, 'validation_loss': 3.9449543857574465, 'device_type': 'cuda', 'time_seconds': 10.771330118179321, 'epoch': 0, 'example_output': 'It is good to be found in the\\n vernal of', 'tokens_seen': 235008}\n",
      "It is good,\n",
      "  And now the same is Love.\n",
      "Done: English Poets of the Eighteenth Century by Various\n",
      "{'training_loss': 3.455665015632456, 'validation_loss': 4.901874934925752, 'device_type': 'cuda', 'time_seconds': 7.257386207580566, 'epoch': 0, 'example_output': 'It is good to be\\nto be sure that he has not', 'tokens_seen': 168960}\n",
      "It is good for us.\n",
      "\n",
      "[-29-]\n",
      "Done: Dio's Rome Vol. 3 by Cassius Dio\n",
      "It is good to be found in the\n",
      "question of the Q\n",
      "Done: Mohammedanism by C. Snouck Hurgronje\n",
      "It is good for you, and I am going to see you\n",
      "Done: The Black Creek Stopping-House by Nellie McClung\n",
      "{'training_loss': 4.946226366026039, 'validation_loss': 4.97457168879134, 'device_type': 'cuda', 'time_seconds': 12.896258115768433, 'epoch': 0, 'example_output': 'It is good for us, but\\nthe only thing that we', 'tokens_seen': 6656}\n",
      "{'training_loss': 3.6130494897849714, 'validation_loss': 4.401272990730371, 'device_type': 'cuda', 'time_seconds': 12.892517566680908, 'epoch': 0, 'example_output': 'It is good to be\\nto be done. I have told', 'tokens_seen': 262656}\n",
      "It is good to me, and I can hardly have\n",
      "been\n",
      "Done: Across the Zodiac by Percy Greg\n",
      "{'training_loss': 3.6569220663345967, 'validation_loss': 4.908241316329601, 'device_type': 'cuda', 'time_seconds': 6.374872207641602, 'epoch': 0, 'example_output': 'It is good to\\nbe a single instance of the German Government', 'tokens_seen': 117760}\n",
      "It is good to be found in the\n",
      "Fatherland.\n",
      "\n",
      "Done: What Germany Thinks by Thomas F. A. Smith\n",
      "It is good to me,\n",
      "      And\n",
      "Done: Poems by Currer Ellis and Acton Bell\n",
      "{'training_loss': 3.905643350475437, 'validation_loss': 4.761862013075087, 'device_type': 'cuda', 'time_seconds': 4.4338459968566895, 'epoch': 0, 'example_output': \"It is good to be. I'll tell you how I'll\", 'tokens_seen': 54784}\n",
      "It is good. I've got to\n",
      "put it out of\n",
      "Done: The Tragedy of Pudd'nhead Wilson by Mark Twain\n",
      "It is good to see it.\n",
      "     \n",
      "Done: Sword Blades and Poppy Seed by Amy Lowell\n",
      "{'training_loss': 3.8498913991451262, 'validation_loss': 5.24218008366037, 'device_type': 'cuda', 'time_seconds': 6.607975721359253, 'epoch': 0, 'example_output': 'It is good to be a great deal of the\\nstudy of', 'tokens_seen': 78848}\n",
      "It is good enough to\n",
      "bear the same.\n",
      "\n",
      "\"\n",
      "Done: Maria Mitchell: Life Letters and Journals by Maria Mitchell\n",
      "Saved backup.pth and backup.json\n",
      "It is good enough to be read or read or chanted,\n",
      "\n",
      "Done: The Congo and Other Poems by Vachel Lindsay\n",
      "{'training_loss': 3.944504398210295, 'validation_loss': 4.256064986282924, 'device_type': 'cuda', 'time_seconds': 7.635370969772339, 'epoch': 0, 'example_output': 'It is good. I am not\\nnot a prisoner, and', 'tokens_seen': 66048}\n",
      "It is good, and the only way. I have been\n",
      "\n",
      "Done: Wolves of the Sea by Randall Parrish\n",
      "{'training_loss': 3.3159153101056122, 'validation_loss': 4.326461950937907, 'device_type': 'cuda', 'time_seconds': 4.211623191833496, 'epoch': 0, 'example_output': 'It is good enough to be a good deal\\nof the show', 'tokens_seen': 84480}\n",
      "It is good, and the\n",
      "show is a good thing to\n",
      "Done: Peck's Bad Boy at the Circus by George W. Peck\n",
      "It is good,\n",
      "  And the world is not to blame\n",
      "Done: Poems by Marietta Holley\n",
      "It is good to the\n",
      "tradition of the desertness\n",
      "Done: The Land Of Little Rain by Mary Hunter Austin\n",
      "{'training_loss': 3.057565576080823, 'validation_loss': 4.094936670881979, 'device_type': 'cuda', 'time_seconds': 14.383674144744873, 'epoch': 0, 'example_output': 'It is good to the\\n       ', 'tokens_seen': 43520}\n",
      "{'training_loss': 2.305481745511698, 'validation_loss': 3.4633268613493846, 'device_type': 'cuda', 'time_seconds': 14.379035234451294, 'epoch': 0, 'example_output': 'It is good\\n         ', 'tokens_seen': 299520}\n",
      "It is good,\n",
      "        \n",
      "Done: The Poetical Works of William Wordsworth Volume 1\n",
      "It is good, and it is very cold, and it is\n",
      "Done: Daddy Takes Us Skating by Howard R. Garis\n",
      "{'training_loss': 3.878433405838276, 'validation_loss': 4.160080302506685, 'device_type': 'cuda', 'time_seconds': 9.512520551681519, 'epoch': 0, 'example_output': 'It is good to be the only means of\\nthe first encounter', 'tokens_seen': 68096}\n",
      "It is good to you, and to feel the\n",
      "kindness\n",
      "Done: The Pilgrims of New England by Mrs. J. B. Webb\n",
      "{'training_loss': 3.5861567878723144, 'validation_loss': 5.239390595753988, 'device_type': 'cuda', 'time_seconds': 2.0857505798339844, 'epoch': 0, 'example_output': 'It is good\\nto be a great deal of the best of', 'tokens_seen': 27648}\n",
      "It is good to see the\n",
      "people of the Esquimo\n",
      "Done: Kalitan Our Little Alaskan Cousin by Mary F. Nixon-Roulet\n",
      "It is good to the\n",
      "people, but the Germans have not\n",
      "Done: Their Crimes by Various\n",
      "Saved backup.pth and backup.json\n",
      "{'training_loss': 2.9858694732050886, 'validation_loss': 3.8492810311524766, 'device_type': 'cuda', 'time_seconds': 7.046386241912842, 'epoch': 0, 'example_output': \"It is good to me. I don't think it is a\", 'tokens_seen': 182272}\n",
      "It is good for the\n",
      "time. I am going to see\n",
      "Done: Beautiful Joe by Marshall Saunders\n",
      "{'training_loss': 3.803846192827412, 'validation_loss': 3.957777975773325, 'device_type': 'cuda', 'time_seconds': 29.297160148620605, 'epoch': 0, 'example_output': 'It is good to me. I am sure I am not to', 'tokens_seen': 217600}\n",
      "{'training_loss': 3.4875606146847167, 'validation_loss': 3.883730939456395, 'device_type': 'cuda', 'time_seconds': 29.298881769180298, 'epoch': 0, 'example_output': 'It is good for the\\npresent time. I have no doubt', 'tokens_seen': 473600}\n",
      "{'training_loss': 3.2666940790598464, 'validation_loss': 3.8737984251002877, 'device_type': 'cuda', 'time_seconds': 29.295896530151367, 'epoch': 0, 'example_output': 'It is good enough to be. I am glad to see you', 'tokens_seen': 729600}\n",
      "It is good to me. I have been so much to be\n",
      "Done: Bleak House by Charles Dickens\n",
      "{'training_loss': 3.779424345429568, 'validation_loss': 4.214615380764007, 'device_type': 'cuda', 'time_seconds': 6.124096632003784, 'epoch': 0, 'example_output': 'It is good to me, and I will not\\nsee you', 'tokens_seen': 71680}\n",
      "It is good; it is a ghostly\n",
      "thing. It\n",
      "Done: Old Creole Days by George Washington Cable\n",
      "{'training_loss': 3.8993245541794326, 'validation_loss': 4.343275218070308, 'device_type': 'cuda', 'time_seconds': 11.535607814788818, 'epoch': 0, 'example_output': 'It is good enough to be\\nwitnessed, and I', 'tokens_seen': 136192}\n",
      "It is good to me; I am sure I should have been\n",
      "Done: The Wrecker by Robert Louis Stevenson and Lloyd Osbourne\n",
      "{'training_loss': 3.8942927826775446, 'validation_loss': 4.309920112291972, 'device_type': 'cuda', 'time_seconds': 3.681636095046997, 'epoch': 0, 'example_output': 'It is good, and that it is, in the same way', 'tokens_seen': 33280}\n",
      "It is good enough to be able to use it.  \n",
      "Done: Town Geology by Charles Kingsley\n",
      "It is good to be a good fellow, and I'm\n",
      "\n",
      "Done: The Outdoor Chums by Captain Quincy Allen\n",
      "{'training_loss': 3.1800968823954463, 'validation_loss': 3.6137595687593733, 'device_type': 'cuda', 'time_seconds': 4.199619770050049, 'epoch': 0, 'example_output': \"It is good to me, and I think I can't help\", 'tokens_seen': 58880}\n",
      "It is good enough to go to school, and I'm sure\n",
      "Done: Patty at Home by Carolyn Wells\n",
      "{'training_loss': 3.219533054862773, 'validation_loss': 4.035172762694182, 'device_type': 'cuda', 'time_seconds': 8.032742977142334, 'epoch': 0, 'example_output': \"It is good to me. I'm not afraid of you.\", 'tokens_seen': 183808}\n",
      "It is good\n",
      "to me. I don't know. I\n",
      "Done: The Lone Star Ranger by Zane Grey\n",
      "It is good to be \n",
      "good to be a good man\n",
      "Done: Sermons on Evil-Speaking by Isaac Barrow\n",
      "{'training_loss': 3.9091067282499465, 'validation_loss': 4.663388738826829, 'device_type': 'cuda', 'time_seconds': 7.411135911941528, 'epoch': 0, 'example_output': 'It is good to me, and I am not sure of it', 'tokens_seen': 83456}\n",
      "It is good enough to earn a\n",
      "curious notion of the\n",
      "Done: The Professor by Currer Bell\n",
      "Saved backup.pth and backup.json\n",
      "{'training_loss': 3.143670629371296, 'validation_loss': 4.8165304395887585, 'device_type': 'cuda', 'time_seconds': 4.298010587692261, 'epoch': 0, 'example_output': 'It is good for the first time. I am not\\naf', 'tokens_seen': 108032}\n",
      "It is good to see. I am a good man, and\n",
      "Done: The Night-Born by Jack London\n",
      "It is good to say, \"Are we not to be\n",
      "\n",
      "Done: In The Fourth Year by H.G. Wells\n",
      "It is good enough for the\n",
      "world to be done. The\n",
      "Done: Punchinello Vol. 2 No. 36 December 3 1870 by Various\n",
      "It is good,\n",
      "  And the little girl's pretty maid\n",
      "Done: Verse and Prose for Beginners in Reading by Various\n",
      "{'training_loss': 4.308226755787344, 'validation_loss': 4.036102282373529, 'device_type': 'cuda', 'time_seconds': 5.587254762649536, 'epoch': 0, 'example_output': 'It is good to say, and the\\nworld is not so', 'tokens_seen': 9728}\n",
      "It is good to\n",
      "the world.\"\n",
      "\n",
      "\"You are\n",
      "Done: Around the World in 80 Days by Jules Verne\n",
      "{'training_loss': 2.876634350267507, 'validation_loss': 4.833088786103005, 'device_type': 'cuda', 'time_seconds': 7.614980220794678, 'epoch': 0, 'example_output': 'It is good to us,\\n      ', 'tokens_seen': 91648}\n",
      "It is good to be,\n",
      "      \n",
      "Done: The Cavalier Songs and Ballads of England by Various\n",
      "It is good to see\n",
      "       \n",
      "Done: Sonnets by Michael Angelo Buonarroti & Tommaso Campanella\n",
      "{'training_loss': 4.773779419463451, 'validation_loss': 5.394075906404885, 'device_type': 'cuda', 'time_seconds': 12.646381855010986, 'epoch': 0, 'example_output': 'It is good to be found that the\\npoem is not', 'tokens_seen': 8192}\n",
      "{'training_loss': 3.070170626552712, 'validation_loss': 5.132797307865594, 'device_type': 'cuda', 'time_seconds': 12.641409397125244, 'epoch': 0, 'example_output': 'It is good to be\\nthe king of Persia.\" Rustem', 'tokens_seen': 264192}\n",
      "It is good to know what the king of Persia has done.\n",
      "Done: Persian Literature Volume 1 Comprising The Shah Nameh\n",
      "It is good enough to do so. But I am not\n",
      "\n",
      "Done: Damon and Delia by William Godwin\n",
      "{'training_loss': 3.261601016100715, 'validation_loss': 4.0059778690338135, 'device_type': 'cuda', 'time_seconds': 4.171243667602539, 'epoch': 0, 'example_output': \"It is good, just as it is, it's all right\", 'tokens_seen': 57344}\n",
      "It is good, and I'm glad I'm not.\"\n",
      "\n",
      "Done: Dave Darrin's Third Year at Annapolis by H. Irving Hancock\n",
      "Saved backup.pth and backup.json\n",
      "It is good to be naughty, and she is a very\n",
      "\n",
      "Done: Dotty Dimple at Play by Sophie May\n",
      "{'training_loss': 3.3464071984724564, 'validation_loss': 3.637148775038172, 'device_type': 'cuda', 'time_seconds': 9.030009508132935, 'epoch': 0, 'example_output': 'It is good to be a good time to be\\nwonder', 'tokens_seen': 118272}\n",
      "It is good to be a good\n",
      "wife to be a Christian\n",
      "Done: Miss Prudence by Jennie Maria (Drinkwater) Conklin\n",
      "{'training_loss': 2.8529667107152266, 'validation_loss': 3.663561542828878, 'device_type': 'cuda', 'time_seconds': 4.640862464904785, 'epoch': 0, 'example_output': 'It is good,\" said Tom. \"I am going to do', 'tokens_seen': 92672}\n",
      "It is good to be,\" said Dick. \"I don't\n",
      "Done: The Rover Boys at College by Edward Stratemeyer\n",
      "It is good.  And if we\n",
      "have not seen that\n",
      "Done: The Gospel of the Pentateuch by Charles Kingsley\n",
      "It is good and true, and\n",
      "that it is a good\n",
      "Done: David by Charles Kingsley\n",
      "{'training_loss': 3.7627901397173917, 'validation_loss': 4.519553363323212, 'device_type': 'cuda', 'time_seconds': 2.583833694458008, 'epoch': 0, 'example_output': 'It is good and true,\\n      ', 'tokens_seen': 28672}\n",
      "It is good--\n",
      "        \n",
      "Done: Poems by Walter R. Cassels\n",
      "It is good,\n",
      "        \n",
      "Done: Snubby Nose and Tippy Toes by Laura Rountree Smith\n",
      "It is good, and she has a\n",
      "month, and she\n",
      "Done: Rose O' the River by Kate Douglas Wiggin\n",
      "{'training_loss': 3.9013236412462198, 'validation_loss': 4.4066343126089675, 'device_type': 'cuda', 'time_seconds': 13.9055917263031, 'epoch': 0, 'example_output': \"It is good, and I'm not going to work\\nto\", 'tokens_seen': 89088}\n",
      "{'training_loss': 3.0770260787235117, 'validation_loss': 4.231259053168089, 'device_type': 'cuda', 'time_seconds': 13.897772550582886, 'epoch': 0, 'example_output': 'It is good to be able to do so. But, besides', 'tokens_seen': 345088}\n",
      "It is good-by, I know, I'm not\n",
      "\n",
      "Done: Fruitfulness by Emile Zola\n",
      "It is good to be the best of\n",
      "the best. The\n",
      "Done: The Mirror of Literature Amusement and Instruction by Various\n",
      "Saved backup.pth and backup.json\n",
      "It is good to be\n",
      "the best of our own. The\n",
      "Done: The Mirror of Literature Amusement and Instruction by Various\n",
      "It is good to be the best of the people of the\n",
      "\n",
      "Done: The European Anarchy by G. Lowes Dickinson\n",
      "{'training_loss': 4.356934822983479, 'validation_loss': 4.788159120650518, 'device_type': 'cuda', 'time_seconds': 3.2540690898895264, 'epoch': 0, 'example_output': 'It is good to be, and the child is not the\\n', 'tokens_seen': 25088}\n",
      "It is good to\n",
      "knowing that it is a fact that\n",
      "Done: Children's Rights by Kate Douglas Wiggin and Nora A. Smith\n",
      "{'training_loss': 3.3393123951825228, 'validation_loss': 5.274090934883464, 'device_type': 'cuda', 'time_seconds': 12.686169147491455, 'epoch': 0, 'example_output': 'It is good to be a man,\\nAnd he is not', 'tokens_seen': 179200}\n",
      "It is good to be a man,\n",
      "And that he is\n",
      "Done: A Select Collection of Old English Plays Vol. 7 (4th edn.) by Various\n",
      "{'training_loss': 2.935447161225067, 'validation_loss': 4.514676041073269, 'device_type': 'cuda', 'time_seconds': 1.4395639896392822, 'epoch': 0, 'example_output': 'It is good to be used for to be used by the American', 'tokens_seen': 40960}\n",
      "It is good to be used by the American\n",
      "officers.\n",
      "Done: With the Turks in Palestine by Alexander Aaronsohn\n",
      "It is good to\n",
      "the _Halbrane_, and\n",
      "Done: An Antarctic Mystery by Jules Verne\n",
      "{'training_loss': 3.5296407775445418, 'validation_loss': 3.9575935816153502, 'device_type': 'cuda', 'time_seconds': 5.781930208206177, 'epoch': 0, 'example_output': \"It is good for me, and I'm glad to get him\", 'tokens_seen': 43008}\n",
      "It is good for any thing else. I've got to get\n",
      "Done: Dab Kinzer by William O. Stoddard\n",
      "{'training_loss': 4.226112019384804, 'validation_loss': 5.121365356445312, 'device_type': 'cuda', 'time_seconds': 11.808537006378174, 'epoch': 0, 'example_output': 'It is good to be\\nmade up to the world. The', 'tokens_seen': 118784}\n",
      "It is good for us to-day.\n",
      "\n",
      "The great\n",
      "Done: Great Events by Famous Historians Volume 21 by Various\n",
      "{'training_loss': 4.382478928172981, 'validation_loss': 4.160133880376816, 'device_type': 'cuda', 'time_seconds': 5.973335027694702, 'epoch': 0, 'example_output': 'It is good for us to do so.\\n\\n\"We', 'tokens_seen': 7168}\n",
      "It is good, Marcos, and I will go for help.\"\n",
      "Done: The Velvet Glove by Henry Seton Merriman\n",
      "{'training_loss': 4.557631901857823, 'validation_loss': 5.235624735035113, 'device_type': 'cuda', 'time_seconds': 17.32334017753601, 'epoch': 0, 'example_output': 'It is good\\nto be a little more than a little more', 'tokens_seen': 76800}\n",
      "{'training_loss': 3.63154667207797, 'validation_loss': 4.857571043185334, 'device_type': 'cuda', 'time_seconds': 17.314708471298218, 'epoch': 0, 'example_output': 'It is good to be said to be a\\ncousin', 'tokens_seen': 332800}\n",
      "It is good to be a thing of the world, and\n",
      "\n",
      "Done: The Works of Charles and Mary Lamb Vol. 2: Elia by Charles Lamb\n",
      "Saved backup.pth and backup.json\n",
      "{'training_loss': 4.07678904182393, 'validation_loss': 4.439355927128946, 'device_type': 'cuda', 'time_seconds': 9.143909215927124, 'epoch': 0, 'example_output': 'It is good to\\nthe most beautiful, and the most beautiful', 'tokens_seen': 57856}\n",
      "It is good, and it is\n",
      "that the people are not\n",
      "Done: Domestic Manners of the Americans by Fanny Trollope\n",
      "{'training_loss': 2.693009523587806, 'validation_loss': 4.633960088094075, 'device_type': 'cuda', 'time_seconds': 1.7346866130828857, 'epoch': 0, 'example_output': 'It is good,\\n       And you', 'tokens_seen': 28672}\n",
      "It is good,\n",
      "     And all the world\n",
      "Done: The Man Against the Sky by Edwin Arlington Robinson\n",
      "{'training_loss': 3.52071144752111, 'validation_loss': 4.692680340187223, 'device_type': 'cuda', 'time_seconds': 14.479153633117676, 'epoch': 0, 'example_output': 'It is good, that it is not to be doubted, that', 'tokens_seen': 229888}\n",
      "It is good, and\n",
      "that it is not the case of\n",
      "Done: The Works of Samuel Johnson Volume 6 by Samuel Johnson\n",
      "{'training_loss': 3.7432754804799844, 'validation_loss': 3.772480981690543, 'device_type': 'cuda', 'time_seconds': 14.917593479156494, 'epoch': 0, 'example_output': 'It is good to be\\nfound, that the house of Austria', 'tokens_seen': 36864}\n",
      "{'training_loss': 2.9738514425990346, 'validation_loss': 3.5979733515758903, 'device_type': 'cuda', 'time_seconds': 14.904757022857666, 'epoch': 0, 'example_output': 'It is good to be\\nconceived, that it is not', 'tokens_seen': 292864}\n",
      "It is good to be the practice of\n",
      "the minister, and\n",
      "Done: The Works of Samuel Johnson Volume 10 by Samuel Johnson\n",
      "{'training_loss': 3.348198138811312, 'validation_loss': 3.57432486279176, 'device_type': 'cuda', 'time_seconds': 14.88649868965149, 'epoch': 0, 'example_output': 'It is good, my lords, that it is not\\nnecessary', 'tokens_seen': 82944}\n",
      "{'training_loss': 2.7526614767411797, 'validation_loss': 3.479492378706979, 'device_type': 'cuda', 'time_seconds': 14.876572847366333, 'epoch': 0, 'example_output': 'It is good, my lords, that the\\npublick', 'tokens_seen': 338944}\n",
      "It is good, that the\n",
      "nation is not to be preserved\n",
      "Done: The Works of Samuel Johnson Volume 11 by Samuel Johnson\n",
      "{'training_loss': 2.6862592506044694, 'validation_loss': 4.665614111670132, 'device_type': 'cuda', 'time_seconds': 4.30538010597229, 'epoch': 0, 'example_output': 'It is good, and the people of the\\nMoorish', 'tokens_seen': 131584}\n",
      "It is good, and the people of Morocco are\n",
      "not very\n",
      "Done: Travels in Morocco Volume 1 by James Richardson\n",
      "{'training_loss': 3.408326808243884, 'validation_loss': 4.139219806744502, 'device_type': 'cuda', 'time_seconds': 18.55495858192444, 'epoch': 0, 'example_output': 'It is good, and that\\nthe whole world is not in', 'tokens_seen': 253440}\n",
      "{'training_loss': 2.8591455729194384, 'validation_loss': 4.054657789377066, 'device_type': 'cuda', 'time_seconds': 18.538673400878906, 'epoch': 0, 'example_output': 'It is good to be\\ndone, and I hope I shall', 'tokens_seen': 509440}\n",
      "It is good as you can. I am very sorry to see\n",
      "Done: Life Of Johnson Volume 4 of 6 by Boswell\n",
      "{'training_loss': 3.2347248135126225, 'validation_loss': 4.481708501776059, 'device_type': 'cuda', 'time_seconds': 7.308207988739014, 'epoch': 0, 'example_output': 'It is good, but I am not to be\\ntoo much', 'tokens_seen': 189952}\n",
      "It is good to be\n",
      "detected. I am, I\n",
      "Done: The Boss of Little Arcady by Harry Leon Wilson\n",
      "It is good, miss. I'm sure you're a miser\n",
      "Done: Aunt Jane's Nieces at Millville by Edith Van Dyne\n",
      "{'training_loss': 3.558601786273186, 'validation_loss': 4.618157594100289, 'device_type': 'cuda', 'time_seconds': 6.978238582611084, 'epoch': 0, 'example_output': \"It is good to me, and I'll be a\\nlong\", 'tokens_seen': 86528}\n",
      "It is good to you, Joe. I never saw a man\n",
      "Done: Joe Wilson and His Mates by Henry Lawson\n",
      "Saved backup.pth and backup.json\n",
      "{'training_loss': 3.3324810909319527, 'validation_loss': 4.600922607794041, 'device_type': 'cuda', 'time_seconds': 6.4410240650177, 'epoch': 0, 'example_output': 'It is good to be happy. I am so glad to tell', 'tokens_seen': 124416}\n",
      "It is good to\n",
      "do it, and I'll do it\n",
      "Done: Kitty's Class Day And Other Stories by Louisa M. Alcott\n",
      "It is good to\n",
      "have any existence. But if we realize\n",
      "Done: The Creative Process in the Individual by Thomas Troward\n",
      "{'training_loss': 3.1950374870484577, 'validation_loss': 4.949956720525568, 'device_type': 'cuda', 'time_seconds': 3.387077569961548, 'epoch': 0, 'example_output': 'It is good to us, and to be sure that the\\n', 'tokens_seen': 94208}\n",
      "It is good to\n",
      "keep a new job to the whole world\n",
      "Done: Sketches of the East Africa Campaign by Robert Valentine Dolbey\n",
      "{'training_loss': 3.100320896745905, 'validation_loss': 3.8308042899156227, 'device_type': 'cuda', 'time_seconds': 11.63708758354187, 'epoch': 0, 'example_output': 'It is good to\\nbe a Christian to do it.\"\\n', 'tokens_seen': 244224}\n",
      "It is good to say, that the\n",
      "master of the Lag\n",
      "Done: The Bravo by J. Fenimore Cooper\n",
      "{'training_loss': 3.3575971077535756, 'validation_loss': 4.225118353449065, 'device_type': 'cuda', 'time_seconds': 8.773859977722168, 'epoch': 0, 'example_output': 'It is good to be a \\nman, and to be', 'tokens_seen': 137216}\n",
      "It is good for me.  I am afraid, sir,\n",
      "Done: Yeast: A Problem by Charles Kingsley\n",
      "{'training_loss': 3.5526881461240807, 'validation_loss': 3.9414760601229784, 'device_type': 'cuda', 'time_seconds': 12.08692455291748, 'epoch': 0, 'example_output': 'It is good to be done, and it is a great deal', 'tokens_seen': 119296}\n",
      "{'training_loss': 2.6997449522926695, 'validation_loss': 3.8918098356665634, 'device_type': 'cuda', 'time_seconds': 12.078502416610718, 'epoch': 0, 'example_output': 'It is good to be done for the\\nlatter to take', 'tokens_seen': 375296}\n",
      "It is good to be\n",
      "worse, and to be sure\n",
      "Done: Precaution by James Fenimore Cooper\n",
      "It is good to be.\n",
      "\n",
      "The sun-bree\n",
      "Done: Poems by Sir John Carr\n",
      "It is good for me to do so. I have never had\n",
      "Done: The Vizier of the Two-Horned Alexander by Frank R. Stockton\n",
      "{'training_loss': 3.6647252134831025, 'validation_loss': 4.234538942575455, 'device_type': 'cuda', 'time_seconds': 11.629192113876343, 'epoch': 0, 'example_output': 'It is good to me, and I am glad that I am', 'tokens_seen': 98304}\n",
      "{'training_loss': 2.649977655653913, 'validation_loss': 4.091762313246727, 'device_type': 'cuda', 'time_seconds': 11.622196674346924, 'epoch': 0, 'example_output': 'It is good to be done in the\\nway of the day', 'tokens_seen': 354304}\n",
      "It is good to be done in the\n",
      "way of our visit\n",
      "Done: Memoir and Diary of John Yeardley Minister of the Gospel\n",
      "It is good to be done by the\n",
      "Lord Jesus Christ.\"\n",
      "Done: The Life of John Bunyan by Edmund Venables\n",
      "Saved backup.pth and backup.json\n",
      "{'training_loss': 3.3914619300073, 'validation_loss': 4.633883053796334, 'device_type': 'cuda', 'time_seconds': 7.951096296310425, 'epoch': 0, 'example_output': 'It is good to be\\na matter of the matter. The', 'tokens_seen': 119296}\n",
      "It is good, and I want to know of it.\"\n",
      "\n",
      "Done: Sustained honor by John R. Musick\n",
      "{'training_loss': 2.9069694941492865, 'validation_loss': 3.6682420939933964, 'device_type': 'cuda', 'time_seconds': 6.180305242538452, 'epoch': 0, 'example_output': 'It is good, and I\\'ll see you.\"\\n\\nHe', 'tokens_seen': 129024}\n",
      "It is good, I am not. I am not going to\n",
      "Done: The Cinema Murder by E. Phillips Oppenheim\n",
      "{'training_loss': 2.9338672620765234, 'validation_loss': 4.396813712120056, 'device_type': 'cuda', 'time_seconds': 7.7198874950408936, 'epoch': 0, 'example_output': \"It is good for me, but I'm\\nworsef\", 'tokens_seen': 192000}\n",
      "It is good enough to\n",
      "know that I'm a-go\n",
      "Done: Bunch Grass by Horace Annesley Vachell\n",
      "{'training_loss': 2.936521519067859, 'validation_loss': 4.575477118585624, 'device_type': 'cuda', 'time_seconds': 7.9115331172943115, 'epoch': 0, 'example_output': 'It is good to me.\\n\\n      ', 'tokens_seen': 206336}\n",
      "It is good to be\n",
      "wonderful to find God in\n",
      "Done: England's Antiphon by George MacDonald\n",
      "It is good,\n",
      "and I am afraid it is a very\n",
      "Done: The Evil Guest by J. Sheridan Le Fanu\n",
      "{'training_loss': 3.523976606641497, 'validation_loss': 4.519222439946355, 'device_type': 'cuda', 'time_seconds': 5.722715616226196, 'epoch': 0, 'example_output': 'It is good enough to be\\ncalled by the _Review_', 'tokens_seen': 92672}\n",
      "It is good, and I am not, as I am,\n",
      "Done: Autobiography by John Stuart Mill\n",
      "{'training_loss': 3.1832564224998956, 'validation_loss': 3.608056610538846, 'device_type': 'cuda', 'time_seconds': 12.292987823486328, 'epoch': 0, 'example_output': 'It is good enough to\\nbe to me to tell you.', 'tokens_seen': 169472}\n",
      "It is good enough to be. I am not so much obliged\n",
      "Done: At Love's Cost by Charles Garvice\n",
      "{'training_loss': 3.259663038783603, 'validation_loss': 4.671087299074445, 'device_type': 'cuda', 'time_seconds': 2.2170228958129883, 'epoch': 0, 'example_output': 'It is good enough to be.  The\\nword is a', 'tokens_seen': 43008}\n",
      "It is good, and the most difficult, the most difficult,\n",
      "Done: Style by Walter Raleigh\n",
      "{'training_loss': 3.313070441571232, 'validation_loss': 4.460980051019218, 'device_type': 'cuda', 'time_seconds': 12.893915176391602, 'epoch': 0, 'example_output': 'It is good to me, and I shall\\nnot be content', 'tokens_seen': 229888}\n",
      "It is good that I am come to thee, and I shall\n",
      "Done: Bible Stories and Religious Classics by Philip P. Wells\n",
      "{'training_loss': 3.7035114110567107, 'validation_loss': 4.607402804318596, 'device_type': 'cuda', 'time_seconds': 13.040872573852539, 'epoch': 0, 'example_output': 'It is good to me, I am not afraid of it.', 'tokens_seen': 84992}\n",
      "{'training_loss': 2.644190805234597, 'validation_loss': 4.487428247227388, 'device_type': 'cuda', 'time_seconds': 13.029857397079468, 'epoch': 0, 'example_output': 'It is good to me.\"\\n\\n\"I do not know', 'tokens_seen': 340992}\n",
      "It is good, it is you who are there. You are\n",
      "Done: The History of a Crime by Victor Hugo\n",
      "Saved backup.pth and backup.json\n",
      "It is good to me that I am\n",
      "across the border\n",
      "Done: Memoir of Wm Watts McNair by J. E. Howard\n",
      "{'training_loss': 2.6062387659194624, 'validation_loss': 4.479785330593586, 'device_type': 'cuda', 'time_seconds': 4.639030694961548, 'epoch': 0, 'example_output': 'It is good to be done, and the\\nchild is obliged', 'tokens_seen': 141312}\n",
      "It is good for its support, and the\n",
      "infant is\n",
      "Done: The Maternal Management of Children in Health and Disease\n",
      "It is good for them to be\n",
      "observed, that they\n",
      "Done: Thoughts On The Necessity Of Improving The Condition Of\n",
      "{'training_loss': 2.7881535053826294, 'validation_loss': 4.209381255697696, 'device_type': 'cuda', 'time_seconds': 6.843444585800171, 'epoch': 0, 'example_output': 'It is good to say that I am going to\\ngo with', 'tokens_seen': 175616}\n",
      "It is good for you,\" she answered.\n",
      "\n",
      "\"I\n",
      "Done: The Real America in Romance Volume 6 A Century Too Soon\n",
      "{'training_loss': 3.0274805704445167, 'validation_loss': 5.107117291291555, 'device_type': 'cuda', 'time_seconds': 8.536349058151245, 'epoch': 0, 'example_output': 'It is good that I am not.\\n\\n_Asc', 'tokens_seen': 218624}\n",
      "It is good, my Lord,\n",
      "And I am sure to\n",
      "Done: Old English Plays Vol. 1 by Various\n",
      "It is good enough for the\n",
      "children of the wolves to hold\n",
      "Done: Northern Trails Book 1 by William J. Long\n",
      "{'training_loss': 4.039004619243858, 'validation_loss': 4.695217464059432, 'device_type': 'cuda', 'time_seconds': 24.97749900817871, 'epoch': 0, 'example_output': 'It is good to see the country of the\\ncountry, and', 'tokens_seen': 137216}\n",
      "{'training_loss': 3.44285166573318, 'validation_loss': 4.559188188716052, 'device_type': 'cuda', 'time_seconds': 24.964820623397827, 'epoch': 0, 'example_output': 'It is good\\nto see the white man who occupies the soil', 'tokens_seen': 393216}\n",
      "{'training_loss': 3.0144824498816383, 'validation_loss': 4.43613952366426, 'device_type': 'cuda', 'time_seconds': 24.960405349731445, 'epoch': 0, 'example_output': 'It is good, and I am glad to hear it.\\n', 'tokens_seen': 649216}\n",
      "It is good, and I believe that, if I had\n",
      "\n",
      "Done: Missionary Travels and Researches in South Africa by David Livingstone\n",
      "It is good reason why the patient is\n",
      "the child of the\n",
      "Done: The Edinburgh Lectures on Mental Science by Thomas Troward\n",
      "{'training_loss': 3.6817460942871962, 'validation_loss': 4.693116324288504, 'device_type': 'cuda', 'time_seconds': 5.193424224853516, 'epoch': 0, 'example_output': 'It is good for the man who\\nthe man who has been', 'tokens_seen': 68096}\n",
      "It is good for\n",
      "the world to say that the Child is\n",
      "Done: The Wolf's Long Howl by Stanley Waterloo\n",
      "{'training_loss': 3.229651524333025, 'validation_loss': 4.5979790409406025, 'device_type': 'cuda', 'time_seconds': 8.787966012954712, 'epoch': 0, 'example_output': 'It is good enough to say that\\nyou have no right to', 'tokens_seen': 162304}\n",
      "It is good enough to say, but I will tell you\n",
      "\n",
      "Done: The Reminiscences Of Baron Sir Henry Hawkins Brampton\n",
      "Saved backup.pth and backup.json\n",
      "It is good enough for you to know.\"\n",
      "\n",
      "\"I\n",
      "Done: Stolen Treasure by Howard Pyle\n",
      "{'training_loss': 2.8707568645477295, 'validation_loss': 4.587877368927002, 'device_type': 'cuda', 'time_seconds': 0.9117136001586914, 'epoch': 0, 'example_output': 'It is good.\\n\\nThere is a life of happiness in', 'tokens_seen': 25088}\n",
      "It is good.\n",
      "\n",
      "There is a life of the world\n",
      "Done: Joy and Power by Henry van Dyke\n",
      "It is good. I'll never forget it.\"\n",
      "\n",
      "\"\n",
      "Done: Andy the Acrobat by Peter T. Harkness\n",
      "{'training_loss': 2.4452799600549042, 'validation_loss': 3.5307476094790866, 'device_type': 'cuda', 'time_seconds': 4.198086977005005, 'epoch': 0, 'example_output': 'It is good that\\nyou are playing with your daughters.\"\\n', 'tokens_seen': 128512}\n",
      "It is good enough to\n",
      "pay it out.\"\n",
      "\n",
      "\"\n",
      "Done: Affairs of State by Burton E. Stevenson\n",
      "It is good enough to be\n",
      "given to be a large portion\n",
      "Done: The American Child by Elizabeth McCracken\n",
      "It is good enough to\n",
      "go to the Dominion of Canada.\n",
      "Done: The Story of Louis Riel: The Rebel Chief by Joseph Edmund Collins\n",
      "{'training_loss': 4.381189031656398, 'validation_loss': 4.613267561968635, 'device_type': 'cuda', 'time_seconds': 2.803400993347168, 'epoch': 0, 'example_output': 'It is good enough to be said that the Government should\\nbe', 'tokens_seen': 3072}\n",
      "It is good for me to go to sea, and to make\n",
      "Done: Personal Memoir Of Daniel Drayton by Daniel Drayton\n",
      "It is good enough for me to go on.\"\n",
      "\n",
      "\"\n",
      "Done: A Man and His Money by Frederic Stewart Isham\n",
      "{'training_loss': 4.813223859438529, 'validation_loss': 4.985969386883636, 'device_type': 'cuda', 'time_seconds': 7.14200234413147, 'epoch': 0, 'example_output': 'It is good enough for us to get\\nout of the road', 'tokens_seen': 10752}\n",
      "It is good enough to be said that the\n",
      "\"Pare\n",
      "Done: Twixt France and Spain by E. Ernest Bilbrough\n",
      "{'training_loss': 3.4391546615334443, 'validation_loss': 4.95856404811778, 'device_type': 'cuda', 'time_seconds': 7.049465894699097, 'epoch': 0, 'example_output': 'It is good that the\\ntorpedo should be laid in', 'tokens_seen': 53760}\n",
      "It is good that the enemy should be\n",
      "sunk by the\n",
      "Done: The Crisis of the Naval War by John Rushworth Jellicoe\n",
      "Saved backup.pth and backup.json\n",
      "It is good,\n",
      "        \n",
      "Done: Shakespeare's Sonnets by William Shakespeare\n",
      "{'training_loss': 3.618695231371148, 'validation_loss': 3.6985473319103845, 'device_type': 'cuda', 'time_seconds': 5.642348289489746, 'epoch': 0, 'example_output': 'It is good that I have\\nbeen in the world. I', 'tokens_seen': 38912}\n",
      "It is good. I can trust you, and you will.\n",
      "Done: The Powers and Maxine by Charles Norris Williamson\n",
      "It is good that you are a\n",
      "good thing.\n",
      "\n",
      "\n",
      "Done: Love Life & Work by Elbert Hubbard\n",
      "{'training_loss': 3.3586923844473704, 'validation_loss': 3.761053963711387, 'device_type': 'cuda', 'time_seconds': 5.733234405517578, 'epoch': 0, 'example_output': \"It is good,--I'm sure you are so big,\", 'tokens_seen': 51200}\n",
      "It is good to be,--and--I'm awful'\n",
      "Done: The Money Moon by Jeffery Farnol\n",
      "It is good to be,\n",
      "   And is it that\n",
      "Done: A Reading of Life by George Meredith\n",
      "{'training_loss': 3.1605073550734857, 'validation_loss': 5.152317849072543, 'device_type': 'cuda', 'time_seconds': 3.2566356658935547, 'epoch': 0, 'example_output': 'It is good work, and that the poet is\\nintimately', 'tokens_seen': 82432}\n",
      "It is good that the public is not the only. It is\n",
      "Done: The Principles of Success in Literature by George Henry Lewes\n",
      "{'training_loss': 2.697165781272192, 'validation_loss': 4.2663787099031305, 'device_type': 'cuda', 'time_seconds': 7.735069513320923, 'epoch': 0, 'example_output': 'It is good to be denied.  It is not probable that', 'tokens_seen': 237056}\n",
      "It is good to be\n",
      "made to be so.  I\n",
      "Done: The Life of Lord Byron by John Galt\n",
      "It is good to know how little they can see.  And\n",
      "Done: Scientific Essays and Lectures by Charles Kingsley\n",
      "{'training_loss': 3.381722120826061, 'validation_loss': 4.671426998485218, 'device_type': 'cuda', 'time_seconds': 8.508065700531006, 'epoch': 0, 'example_output': 'It is good to say that the\\nearth is not so much', 'tokens_seen': 129024}\n",
      "It is good to say that the\n",
      "low-lying land begins\n",
      "Done: The Story of Evolution by Joseph McCabe\n",
      "It is good to mention, that the\n",
      "greatest part of\n",
      "Done: Trips to the Moon by Lucian\n",
      "Saved backup.pth and backup.json\n",
      "{'training_loss': 3.744397960603237, 'validation_loss': 4.131589740514755, 'device_type': 'cuda', 'time_seconds': 3.661984920501709, 'epoch': 0, 'example_output': 'It is good to be done. The\\nchildren were so frightened', 'tokens_seen': 22528}\n",
      "It is good to be done, and I can not go to\n",
      "Done: Thirty Years a Slave by Louis Hughes\n",
      "It is good to see the play, and I'm sure he\n",
      "Done: Aunt Jane's Nieces Out West by Edith Van Dyne\n",
      "{'training_loss': 3.7427481201207526, 'validation_loss': 4.258369452813092, 'device_type': 'cuda', 'time_seconds': 5.247210502624512, 'epoch': 0, 'example_output': \"It is good to us, and I'm sure it's\\n\", 'tokens_seen': 29184}\n",
      "It is good to me, and I'm going to\n",
      "see\n",
      "Done: A Flock of Girls and Boys by Nora Perry\n",
      "{'training_loss': 3.614930674720269, 'validation_loss': 4.126298609744297, 'device_type': 'cuda', 'time_seconds': 13.621952295303345, 'epoch': 0, 'example_output': 'It is good to say, Bob, that he is not a', 'tokens_seen': 120832}\n",
      "{'training_loss': 2.7596676914164653, 'validation_loss': 4.032655319471038, 'device_type': 'cuda', 'time_seconds': 13.616350650787354, 'epoch': 0, 'example_output': 'It is good to say, sir, that the Lord\\nWill', 'tokens_seen': 376832}\n",
      "It is good to say that you are not in the least\n",
      "\n",
      "Done: Wyandotte by James Fenimore Cooper\n",
      "{'training_loss': 2.847831849490895, 'validation_loss': 5.082474475211286, 'device_type': 'cuda', 'time_seconds': 6.983032703399658, 'epoch': 0, 'example_output': 'It is good to say that it is\\nnot the case with', 'tokens_seen': 206848}\n",
      "It is good to say that the\n",
      "thing is the case with\n",
      "Done: The Atlantic Monthly Vol. 2 No. 12 October 1858 by Various\n",
      "It is good to us, and\n",
      "that it is the most\n",
      "Done: Pulpit and Press by Mary Baker Eddy\n",
      "{'training_loss': 2.982800413637745, 'validation_loss': 4.152866675303533, 'device_type': 'cuda', 'time_seconds': 8.018551111221313, 'epoch': 0, 'example_output': 'It is good to know. I am not\\nquite sure it', 'tokens_seen': 189952}\n",
      "It is good to be\n",
      "Sunday.\"\n",
      "\n",
      "\"I don\n",
      "Done: Up the Hill and Over by Isabel Ecclestone Mackay\n",
      "It is good to be done. The climate is a\n",
      "place\n",
      "Done: From Yauco to Las Marias by Karl Stephen Herrman\n",
      "It is good to get back.  There are three\n",
      "h\n",
      "Done: Extract from Captain Stormfield's Visit to Heaven by Mark Twain\n",
      "{'training_loss': 2.7212649732265835, 'validation_loss': 4.4517805235726495, 'device_type': 'cuda', 'time_seconds': 4.333750247955322, 'epoch': 0, 'example_output': 'It is good to be done before you go.\"\\n\\n\"', 'tokens_seen': 108544}\n",
      "It is good to be\n",
      "for you to marry anybody.\"\n",
      "\n",
      "Done: The Green Mouse by Robert W. Chambers\n",
      "Saved backup.pth and backup.json\n",
      "{'training_loss': 2.9219925095734087, 'validation_loss': 4.099805549399494, 'device_type': 'cuda', 'time_seconds': 8.690382242202759, 'epoch': 0, 'example_output': 'It is good to be done.\\n\\nThe natives have been', 'tokens_seen': 228864}\n",
      "It is good enough to get up.\n",
      "\n",
      "The next day\n",
      "Done: American Big Game in Its Haunts by Various\n",
      "It is good enough to be done.  I have been\n",
      "\n",
      "Done: The Green Flag by Arthur Conan Doyle\n",
      "{'training_loss': 3.967398634107094, 'validation_loss': 4.577878035031832, 'device_type': 'cuda', 'time_seconds': 2.0852291584014893, 'epoch': 0, 'example_output': 'It is good enough to be done.\\n\\n\"There\\'s', 'tokens_seen': 23552}\n",
      "It is good for you.\"\n",
      "\n",
      "\"And you're a\n",
      "Done: October Vagabonds by Richard Le Gallienne\n",
      "It is good time coming,\n",
      "  And, on the occasion\n",
      "Done: The Anti-Slavery Harp by Various\n",
      "{'training_loss': 2.717055658003648, 'validation_loss': 3.7261153885296414, 'device_type': 'cuda', 'time_seconds': 8.53366208076477, 'epoch': 0, 'example_output': 'It is good to you, Mr. Craft, that you are', 'tokens_seen': 184320}\n",
      "It is good to me, but I don't care to see\n",
      "Done: Burnham Breaker by Homer Greene\n",
      "It is good to him, and he is not dead. \n",
      "Done: Venus and Adonis by William Shakespeare\n",
      "It is good time, but it's no use to\n",
      "allow\n",
      "Done: Punch Vol. 153 Aug. 22 1917 by Various\n",
      "{'training_loss': 3.9341362923848946, 'validation_loss': 4.288214352703834, 'device_type': 'cuda', 'time_seconds': 18.071488857269287, 'epoch': 0, 'example_output': 'It is good to me, but I am not to be afraid', 'tokens_seen': 89600}\n",
      "{'training_loss': 3.121711118709505, 'validation_loss': 4.0700884419818255, 'device_type': 'cuda', 'time_seconds': 18.063978910446167, 'epoch': 0, 'example_output': 'It is good to me, and I am not sorry that I', 'tokens_seen': 345600}\n",
      "It is good. I have no reason to believe it. I\n",
      "Done: Boswell's Life Of Johnson Volume 5 by James Boswell\n",
      "{'training_loss': 3.526757970268344, 'validation_loss': 3.8977079338497584, 'device_type': 'cuda', 'time_seconds': 6.653892517089844, 'epoch': 0, 'example_output': 'It is good to be\\nfor me to be sure that I', 'tokens_seen': 44032}\n",
      "It is good to be, and\n",
      "you know, and I\n",
      "Done: Peter's Mother by Mrs. Henry De La Pasture\n",
      "{'training_loss': 3.74561006000142, 'validation_loss': 5.1160096650595195, 'device_type': 'cuda', 'time_seconds': 12.275168418884277, 'epoch': 0, 'example_output': 'It is good to be done by the\\n    ', 'tokens_seen': 92672}\n",
      "{'training_loss': 2.5454230214971973, 'validation_loss': 5.009576435927507, 'device_type': 'cuda', 'time_seconds': 12.270082473754883, 'epoch': 0, 'example_output': 'It is good that the\\ntobacco-stuffs of', 'tokens_seen': 348672}\n",
      "It is good enough to do. It is\n",
      "the most important\n",
      "Done: A Practical Physiology by Albert F. Blaisdell\n",
      "Saved backup.pth and backup.json\n",
      "{'training_loss': 2.878633940081799, 'validation_loss': 4.233393941009254, 'device_type': 'cuda', 'time_seconds': 8.499711275100708, 'epoch': 0, 'example_output': 'It is good to me, my mother, that I have\\n', 'tokens_seen': 226304}\n",
      "It is good for thee, my Marco, for thy love is\n",
      "Done: A Golden Book of Venice by Mrs. Lawrence Turnbull\n",
      "{'training_loss': 2.8524015739120863, 'validation_loss': 4.937807194730069, 'device_type': 'cuda', 'time_seconds': 7.2136030197143555, 'epoch': 0, 'example_output': \"It is good to have been\\na-goin' to\", 'tokens_seen': 217088}\n",
      "It is good for me, and I am glad I was so\n",
      "Done: The Atlantic Monthly Vol. 2 No. 11 September 1857 by Various\n",
      "It is good to hear,\n",
      "  And the world is lost\n",
      "Done: The Lonely Dancer and Other Poems by Richard Le Gallienne\n",
      "It is good to see the sun\n",
      "and the sun, the\n",
      "Done: Three short works by Gustave Flaubert\n",
      "It is good to the world, and the\n",
      "people are like\n",
      "Done: The Celtic Twilight by W. B. Yeats\n",
      "{'training_loss': 3.51651188506875, 'validation_loss': 4.776104625902678, 'device_type': 'cuda', 'time_seconds': 3.0376555919647217, 'epoch': 0, 'example_output': 'It is good to the\\npeople that they are doing their utmost', 'tokens_seen': 55808}\n",
      "It is good to the poor, the\n",
      "confession of the\n",
      "Done: God The Invisible King by H. G. Wells\n",
      "{'training_loss': 2.567432101636995, 'validation_loss': 3.8639308496525415, 'device_type': 'cuda', 'time_seconds': 11.28646206855774, 'epoch': 0, 'example_output': 'It is good to\\nthe horses, but it is evident that', 'tokens_seen': 216576}\n",
      "It is good, and the horses are much distressed, and the\n",
      "Done: Journals of Australian Explorations by A C and F T Gregory\n",
      "{'training_loss': 3.4259096948724044, 'validation_loss': 4.245199504445811, 'device_type': 'cuda', 'time_seconds': 9.051487922668457, 'epoch': 0, 'example_output': 'It is good, that I am\\nnot a man of any', 'tokens_seen': 120832}\n",
      "It is good, I believe, that I am not\n",
      "happy\n",
      "Done: Clarissa Volume 4 of 9 by Samuel Richardson\n",
      "It is good fun to me, and I am not afraid of\n",
      "Done: The Little House in the Fairy Wood by Ethel Cook Eliot\n",
      "{'training_loss': 3.885425193768712, 'validation_loss': 3.7656157846036167, 'device_type': 'cuda', 'time_seconds': 3.492784023284912, 'epoch': 0, 'example_output': \"It is good to you, and I'll have to go home\", 'tokens_seen': 10240}\n",
      "It is good luck to you,\" said Betty, as she\n",
      "\n",
      "Done: The Outdoor Girls of Deepdale by Laura Lee Hope\n",
      "Saved backup.pth and backup.json\n",
      "It is good to be seen. I am a great deal of\n",
      "Done: Little Saint Elizabeth and Other Stories by Frances Hodgson Burnett\n",
      "{'training_loss': 3.7300425317330674, 'validation_loss': 4.6782524341192, 'device_type': 'cuda', 'time_seconds': 11.74846339225769, 'epoch': 0, 'example_output': 'It is good to be done.\\n\\nROB. H', 'tokens_seen': 89088}\n",
      "{'training_loss': 2.6308535309183716, 'validation_loss': 4.478643295092461, 'device_type': 'cuda', 'time_seconds': 11.744148015975952, 'epoch': 0, 'example_output': 'It is good to say,\\nAnd yet the king is merc', 'tokens_seen': 345088}\n",
      "It is good to say,\n",
      "And therefore, as I say\n",
      "Done: A Select Collection of Old English Plays Vol. VIII\n",
      "It is good to be true.\"\n",
      "\n",
      "\"I'm going\n",
      "Done: Aunt Jane's Nieces in Society by Edith Van Dyne\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m trainer.load(\u001b[33m\"\u001b[39m\u001b[33mbackup\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m70\u001b[39m, \u001b[32m10_000\u001b[39m): \n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     title = pg[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m][idx][\u001b[33m'\u001b[39m\u001b[33mshort_book_title\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      5\u001b[39m     complete.append(title)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mBigDatasetTrainer.train_idx\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_idx\u001b[39m(\u001b[38;5;28mself\u001b[39m, n:\u001b[38;5;28mint\u001b[39m):\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mself\u001b[39m.next_item = n\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mBigDatasetTrainer.train_next\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     36\u001b[39m text = \u001b[38;5;28mself\u001b[39m.dataset[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m][\u001b[38;5;28mself\u001b[39m.next_item][\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainable_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mIt is good\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mself\u001b[39m.done.append(\u001b[38;5;28mself\u001b[39m.next_item)\n\u001b[32m     39\u001b[39m \u001b[38;5;28mself\u001b[39m.next_item += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 180\u001b[39m, in \u001b[36mTrainGPT.train\u001b[39m\u001b[34m(self, text, max_length, stride, epochs, prompt)\u001b[39m\n\u001b[32m    178\u001b[39m torch.manual_seed(\u001b[32m123\u001b[39m)\n\u001b[32m    179\u001b[39m loss_summaries = []\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m training_loader, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mself\u001b[39m.model.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.tokens_seen = \u001b[32m0\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mTrainGPT.loaders\u001b[39m\u001b[34m(self, text, max_length, stride)\u001b[39m\n\u001b[32m     87\u001b[39m     max_length = \u001b[38;5;28mself\u001b[39m.cfg[\u001b[33m\"\u001b[39m\u001b[33mcontext_length\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     88\u001b[39m train_loader = \u001b[38;5;28mself\u001b[39m.create_dataloader(\n\u001b[32m     89\u001b[39m     train_data,\n\u001b[32m     90\u001b[39m     batch_size=\u001b[32m2\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     95\u001b[39m     num_workers=\u001b[32m0\u001b[39m,\n\u001b[32m     96\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m validation_loader = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m    105\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (train_loader, validation_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mTrainGPT.create_dataloader\u001b[39m\u001b[34m(self, text, batch_size, max_length, stride, shuffle, drop_last, num_workers)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, batch_size:\u001b[38;5;28mint\u001b[39m=\u001b[32m4\u001b[39m, max_length:\u001b[38;5;28mint\u001b[39m=\u001b[32m256\u001b[39m, stride:\u001b[38;5;28mint\u001b[39m=\u001b[32m128\u001b[39m, shuffle:\u001b[38;5;28mbool\u001b[39m=\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last:\u001b[38;5;28mbool\u001b[39m=\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers:\u001b[38;5;28mint\u001b[39m=\u001b[32m0\u001b[39m) -> DataLoader:\n\u001b[32m     48\u001b[39m     dataset = GPTDatasetV1(text, \u001b[38;5;28mself\u001b[39m.tokenizer, max_length, stride)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_workers\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llm-from-scratch/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:385\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         sampler = \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    387\u001b[39m         sampler = SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/llm-from-scratch/.venv/lib/python3.13/site-packages/torch/utils/data/sampler.py:156\u001b[39m, in \u001b[36mRandomSampler.__init__\u001b[39m\u001b[34m(self, data_source, replacement, num_samples, generator)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    152\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.replacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m     )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_samples <= \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    157\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    158\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "trainer.load(\"backup\")\n",
    "for idx in range(70, 10_000): \n",
    "    trainer.train_idx(idx)\n",
    "    title = pg[\"train\"][idx]['short_book_title']\n",
    "    complete.append(title)\n",
    "    print(f\"Done: {title}\")\n",
    "    if idx % 10 == 0:\n",
    "        trainer.save(f\"backup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9bd762",
   "metadata": {},
   "source": [
    "# next:\n",
    "\n",
    "- [ ] figure out why the device is always wrong when you first train a model (but not after)\n",
    "- [ ] figure out what's wrong with top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d97825ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n"
     ]
    }
   ],
   "source": [
    "for i in range(100, len(pg[\"train\"])):\n",
    "    if pg[\"train\"][i]['short_book_title'] == \"Aunt Jane's Nieces in Society by Edith Van Dyne\":\n",
    "        print(f\"{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "126d1cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2225,\n",
       " 45297,\n",
       " 54540,\n",
       " 57736,\n",
       " 62885,\n",
       " 63184,\n",
       " 64119,\n",
       " 65044,\n",
       " 65733,\n",
       " 68607,\n",
       " 70949,\n",
       " 73126,\n",
       " 73507,\n",
       " 73916,\n",
       " 74718,\n",
       " 75476,\n",
       " 76070,\n",
       " 76927,\n",
       " 77537,\n",
       " 79349,\n",
       " 80173,\n",
       " 80374,\n",
       " 81616,\n",
       " 81712,\n",
       " 82718,\n",
       " 84232,\n",
       " 90516,\n",
       " 91001,\n",
       " 94496,\n",
       " 99249,\n",
       " 100478,\n",
       " 104984,\n",
       " 105524,\n",
       " 108751,\n",
       " 109808,\n",
       " 111113,\n",
       " 112782,\n",
       " 113288,\n",
       " 113313,\n",
       " 113338,\n",
       " 113372,\n",
       " 113646,\n",
       " 113902,\n",
       " 114512,\n",
       " 114849,\n",
       " 114860,\n",
       " 114969,\n",
       " 116051,\n",
       " 116117,\n",
       " 118207,\n",
       " 118607,\n",
       " 118785,\n",
       " 119295,\n",
       " 119393,\n",
       " 122632,\n",
       " 123633,\n",
       " 125280,\n",
       " 126539,\n",
       " 129087,\n",
       " 130916,\n",
       " 132429,\n",
       " 132659,\n",
       " 137888,\n",
       " 140114,\n",
       " 142238,\n",
       " 145915,\n",
       " 146020,\n",
       " 146993,\n",
       " 148178,\n",
       " 151555,\n",
       " 152577,\n",
       " 155365,\n",
       " 155589,\n",
       " 155659,\n",
       " 160059,\n",
       " 161670,\n",
       " 162143,\n",
       " 163322,\n",
       " 166615,\n",
       " 171697,\n",
       " 174209,\n",
       " 175668,\n",
       " 178394,\n",
       " 179333,\n",
       " 183575,\n",
       " 186517,\n",
       " 187197,\n",
       " 190389,\n",
       " 194822,\n",
       " 196098,\n",
       " 197320,\n",
       " 198880,\n",
       " 198908,\n",
       " 202438,\n",
       " 203446,\n",
       " 203803,\n",
       " 204094,\n",
       " 204560,\n",
       " 206367,\n",
       " 206972,\n",
       " 207149,\n",
       " 208708,\n",
       " 214411,\n",
       " 215514,\n",
       " 216241,\n",
       " 218175,\n",
       " 222777,\n",
       " 223722,\n",
       " 226480,\n",
       " 226760,\n",
       " 228597,\n",
       " 235757,\n",
       " 236198,\n",
       " 237632,\n",
       " 242213,\n",
       " 243639,\n",
       " 244350,\n",
       " 245930,\n",
       " 246395,\n",
       " 247977,\n",
       " 248231,\n",
       " 250274,\n",
       " 254430,\n",
       " 254443,\n",
       " 260677,\n",
       " 266842,\n",
       " 267837,\n",
       " 268986,\n",
       " 269659,\n",
       " 270411,\n",
       " 270585,\n",
       " 274335,\n",
       " 274618,\n",
       " 276595,\n",
       " 278340,\n",
       " 278352,\n",
       " 278998,\n",
       " 280201,\n",
       " 280334,\n",
       " 280455,\n",
       " 283390,\n",
       " 284162,\n",
       " 285738,\n",
       " 286843,\n",
       " 287422,\n",
       " 288291,\n",
       " 288553,\n",
       " 289407,\n",
       " 289842,\n",
       " 290586,\n",
       " 292922,\n",
       " 293249,\n",
       " 297180,\n",
       " 299522,\n",
       " 305606,\n",
       " 308277,\n",
       " 329688,\n",
       " 334231,\n",
       " 337032,\n",
       " 343543,\n",
       " 343734,\n",
       " 343956,\n",
       " 349634,\n",
       " 353473,\n",
       " 353745,\n",
       " 366537,\n",
       " 367091,\n",
       " 368748,\n",
       " 370594,\n",
       " 370939,\n",
       " 372866,\n",
       " 374240,\n",
       " 376052,\n",
       " 376528,\n",
       " 377250,\n",
       " 378770,\n",
       " 379287,\n",
       " 381425,\n",
       " 388708,\n",
       " 405536,\n",
       " 406645,\n",
       " 409313,\n",
       " 415759,\n",
       " 416898,\n",
       " 419876,\n",
       " 421402,\n",
       " 430587,\n",
       " 438272,\n",
       " 439394,\n",
       " 439975,\n",
       " 440795,\n",
       " 445404,\n",
       " 447879,\n",
       " 448554,\n",
       " 449811,\n",
       " 455518,\n",
       " 464544,\n",
       " 466212,\n",
       " 467410,\n",
       " 474373,\n",
       " 474923,\n",
       " 475648,\n",
       " 475851,\n",
       " 480110,\n",
       " 480808,\n",
       " 481729,\n",
       " 483112,\n",
       " 483956,\n",
       " 485303,\n",
       " 486181,\n",
       " 486187,\n",
       " 486398,\n",
       " 487497,\n",
       " 488937,\n",
       " 490630,\n",
       " 498945,\n",
       " 501180,\n",
       " 509062,\n",
       " 510766,\n",
       " 511669,\n",
       " 518424,\n",
       " 519063,\n",
       " 527179,\n",
       " 531118,\n",
       " 532195,\n",
       " 535561,\n",
       " 537845,\n",
       " 546687,\n",
       " 546796,\n",
       " 548712,\n",
       " 553937,\n",
       " 555693,\n",
       " 558083,\n",
       " 569876,\n",
       " 579024,\n",
       " 587914,\n",
       " 588652,\n",
       " 590101,\n",
       " 592918,\n",
       " 598819,\n",
       " 599840,\n",
       " 610362,\n",
       " 611892,\n",
       " 618730,\n",
       " 626621,\n",
       " 628885,\n",
       " 635509,\n",
       " 652965,\n",
       " 653803,\n",
       " 664486,\n",
       " 683337,\n",
       " 688033,\n",
       " 693468,\n",
       " 722245,\n",
       " 765300,\n",
       " 779011,\n",
       " 794016,\n",
       " 794915,\n",
       " 808720,\n",
       " 832214,\n",
       " 837482,\n",
       " 839648,\n",
       " 840188,\n",
       " 841450,\n",
       " 853322,\n",
       " 862185,\n",
       " 864635,\n",
       " 872278,\n",
       " 876653,\n",
       " 886779,\n",
       " 888563,\n",
       " 892716,\n",
       " 899919,\n",
       " 924410,\n",
       " 938837,\n",
       " 939022,\n",
       " 979407,\n",
       " 984139,\n",
       " 1051215,\n",
       " 1113447,\n",
       " 1121258,\n",
       " 1121991,\n",
       " 1133295,\n",
       " 1134470,\n",
       " 1170695,\n",
       " 1722684,\n",
       " 1941618,\n",
       " 3027972,\n",
       " 4332639,\n",
       " 5443801]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([len(pg[\"train\"][i][\"text\"]) for i in range(0,290)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6aff3e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3656e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pg[\"train\"][284][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ec08ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 5]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([1, 5, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82442fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "796"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_text = pg[\"train\"][284][\"text\"]\n",
    "short_tokens = tokenizer.encode(short_text)\n",
    "len(short_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1dcbcb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtainer\u001b[49m.save(\u001b[33m\"\u001b[39m\u001b[33mbackup\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tainer' is not defined"
     ]
    }
   ],
   "source": [
    "tainer.save(\"backup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f15df82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
