{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643c7b1d",
   "metadata": {},
   "source": [
    "# Fine tuning the model to make a chat bot\n",
    "\n",
    "This is the big guacamole at the end of the rainbow. We'll be fine tuning one of the OpenAI models to be able to respond sort of like ChatGPT. I think there's an example of trying to do this on the foundation model in `openai.ipynb` without fine-tuning, and right now it _sucks_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6aec3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n",
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 19:30:56.933964: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-26 19:30:56.942069: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750991456.952373    5892 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750991456.955483    5892 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750991456.963275    5892 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750991456.963284    5892 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750991456.963285    5892 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750991456.963286    5892 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-26 19:30:56.965870: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import openai # type:ignore\n",
    "import gpt # type:ignore\n",
    "import torch\n",
    "import urllib\n",
    "import ssl\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from typing import TypedDict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "from functools import partial\n",
    "import textwrap\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available(): # type: ignore[attr-defined]\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available(): # type: ignore[attr-defined]\n",
    "        return torch.device(\"mps:0\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b948a0d4",
   "metadata": {},
   "source": [
    "## Download the instruction training data\n",
    "\n",
    "This is 1,100 instruction-response pairs (actually some have a third field called input) that were made specifically for the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f063394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n",
      "Example:\n",
      "{'input': 'He go to the park every day.',\n",
      " 'instruction': 'Edit the following sentence for grammar.',\n",
      " 'output': 'He goes to the park every day.'}\n"
     ]
    }
   ],
   "source": [
    "class InstructionExample(TypedDict):\n",
    "    instruction: str  # A description of the task to be performed\n",
    "    input: str        # Optional parameter for the task\n",
    "    output: str       # The expected result of performing the task\n",
    "\n",
    "def download_and_load_file(file_path: str, url: str) -> list[InstructionExample]:\n",
    "    ssl_context = ssl.create_default_context()\n",
    "    ssl_context.check_hostname = False\n",
    "    ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url, context=ssl_context) as response: # type:ignore\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    return data\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))\n",
    "print(\"Example:\")\n",
    "pprint(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab7037",
   "metadata": {},
   "source": [
    "## Convert the examples to Stanford Alpaca format\n",
    "\n",
    "The [format](https://github.com/tatsu-lab/stanford_alpaca) looks like this:\n",
    "\n",
    "```\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "```\n",
    "\n",
    "Or, if there's no input:\n",
    "\n",
    "```\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07052c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry: InstructionExample, include_response:bool=True) -> str:\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry[\"input\"]}\" if entry[\"input\"] else \"\"\n",
    "    response_text = f\"\\n\\n### Response:\\n{entry[\"output\"]}\" if include_response else \"\"\n",
    "\n",
    "    return instruction_text + input_text + response_text\n",
    "\n",
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94965cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data: list[InstructionExample], tokenizer: tiktoken.Encoding):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            full_text = format_input(entry)\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "    \n",
    "    def __getitem__(self, index) -> list[int]:\n",
    "        return self.encoded_texts[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb6fbc",
   "metadata": {},
   "source": [
    "## Custom collate function\n",
    "\n",
    "Passing in a custom collate function lets us easily pad out shorter sequences in each batch to match the longest one.\n",
    "Initially, the padding token will be `<|endoftext|>`, but we'll eventually set it up so that there's only one EOT token\n",
    "and the padding will be done with `-100`.\n",
    "\n",
    "The collate function is responsible for:\n",
    "1. Finding the longest sequence in the batch\n",
    "2. Padding and preparing inputs\n",
    "3. Removing the extra EOT tokens\n",
    "4. Converting the token list to a tensor and transferring it to the target device.\n",
    "\n",
    "\n",
    "### We're not masking the instructions\n",
    "\n",
    "We could use `-100` to mask out the instructions from each example. That would avoid rewarding the model for memorizing\n",
    "worthless bits like \"Below is a task‚Ä¶\", and some people think that's helpful. But it's controversial, and there's at least\n",
    "one paper, [\"Instruction Tuning with Loss Over Instructions,\"](https://arxiv.org/abs/2405.14394) that argues that it's\n",
    "better to train on the whole thing.\n",
    "\n",
    "Maybe I'll try adding instruction masking later, but for now it's not recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36bb3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "        batch: list[list[int]],\n",
    "        pad_token_id: int=50256, # i.e., <|endoftext|>\n",
    "        ignore_index: int=-100, # this is the default ignore index for torch.nn.CrossEntropyLoss\n",
    "        allowed_max_length: int|None=None,\n",
    "        device: str|torch.device=\"cpu\"\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    batch_max_length = max([len(item)+1 for item in batch])\n",
    "\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        mask = targets == pad_token_id # tensor([bool * max_length])\n",
    "        indices = torch.nonzero(mask).squeeze() # type:ignore\n",
    "        if indices.numel() > 1:\n",
    "            # Note: we only do this -100 thing in the targets tensor\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "        \n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "customized_collate_fn = partial(custom_collate_fn, device=get_device(), allowed_max_length=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0466eb49",
   "metadata": {},
   "source": [
    "## Create the Datasets and DataLoaders\n",
    "\n",
    "As a reminder:\n",
    "- **Dataset**: a class that exposes `__getitem__` and `__len__`, so it's like\n",
    "  a list or a vector. It's not really specialized for anything in particular, it's\n",
    "  just a convenient way to wrap data from some source.\n",
    "- **DataLoader**: a class that encapsulates logic for ordering (shuffle or not),\n",
    "  associating inputs with targets, batching, parallelization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e821471",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "gpt.manual_seed(123)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn, # ah, makes sense how partial would be useful now\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f552587f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n",
      "355M model loaded.\n"
     ]
    }
   ],
   "source": [
    "gpt355m = openai.load_openai_model(openai.GPT_CONFIG_355M, \"355M\").model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a30fe9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training_loss': 4.1935, 'validation_loss': 3.6623, 'tokens_seen': 472, 'example_output': '\\nThis time I will eat the french fries.', 'learning_rate': 4e-06}\n",
      "{'training_loss': 1.2623, 'validation_loss': 1.2967, 'tokens_seen': 11704, 'example_output': 'The chef cooked the meal.', 'learning_rate': 1.193103448275862e-05}\n",
      "{'training_loss': 0.8583, 'validation_loss': 0.9081, 'tokens_seen': 22808, 'example_output': 'The chef cooked the meal using the method of cooking', 'learning_rate': 1.986206896551724e-05}\n",
      "{'training_loss': 0.8168, 'validation_loss': 0.8077, 'tokens_seen': 33768, 'example_output': 'The chef cooked the meal for the guests.', 'learning_rate': 2.779310344827586e-05}\n",
      "{'training_loss': 0.7254, 'validation_loss': 0.7351, 'tokens_seen': 45000, 'example_output': 'The chef cooked the meal at his home.', 'learning_rate': 3.5724137931034486e-05}\n",
      "{'training_loss': 0.6829, 'validation_loss': 0.7109, 'tokens_seen': 56576, 'example_output': 'The chef cooked the meal.', 'learning_rate': 4.365517241379311e-05}\n",
      "üèÉ View run learning rate 5e-5, 2 epochs at: http://localhost:5000/#/experiments/0/runs/0f9d570db4674bf09fc8b200f3f2f9fa\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/0\n",
      "Training complete. Steps: 115, tokens: 64800\n"
     ]
    }
   ],
   "source": [
    "example_prompt = format_input({\n",
    "    'instruction': \"Convert this sentence to passive voice.\",\n",
    "    'input': 'The chef cooked the meal.',\n",
    "    'output': ''}, include_response=True)\n",
    "\n",
    "training_config: gpt.GPTTrainingConfig = gpt.DEFAULT_TRAINING_CONFIG.copy()\n",
    "training_config['gradient_clipping'] = False\n",
    "training_config['epochs'] = 2\n",
    "training_config['peak_lr'] = 5e-5\n",
    "training_config['initial_lr'] = 4e-6\n",
    "training_config['eval_frequency'] = 20\n",
    "\n",
    "config = openai.GPT_CONFIG_355M.copy()\n",
    "\n",
    "trainer = gpt.GPTModel(cfg=config, training_cfg=training_config, model=gpt355m)\n",
    "\n",
    "trainer.train_loader(train_loader, val_loader, example_prompt, run_name=\"learning rate 5e-5, 2 epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "609ddc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instruct(instruction, input='', temperature=0.8):\n",
    "    prompt = format_input({\n",
    "        'instruction': instruction,\n",
    "        'input': input,\n",
    "        'output': '',\n",
    "    }, include_response=True)\n",
    "    result = trainer.prompt(prompt, max_tokens=1024, temperature=temperature)\n",
    "    result = result[len(prompt):]\n",
    "    print(textwrap.fill(result, width=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26fedfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The healthier of the two foods is carrots.\n"
     ]
    }
   ],
   "source": [
    "instruct(\"Determine which of the two foods is healthier\", \"carrots, lard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facc4e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
