{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643c7b1d",
   "metadata": {},
   "source": [
    "# Fine tuning the model to make a chat bot\n",
    "\n",
    "This is the big guacamole at the end of the rainbow. We'll be fine tuning one of the OpenAI models to be able to respond sort of like ChatGPT. I think there's an example of trying to do this on the foundation model in `openai.ipynb` without fine-tuning, and right now it _sucks_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6aec3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
      "env: CUDA_LAUNCH_BLOCKING=1\n",
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 23:54:58.643331: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-27 23:54:58.651654: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751093698.661964   24584 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751093698.665131   24584 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751093698.672988   24584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751093698.672996   24584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751093698.672997   24584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751093698.672998   24584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-27 23:54:58.675515: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "\n",
    "import import_ipynb\n",
    "import openai # type:ignore\n",
    "import gpt # type:ignore\n",
    "import torch\n",
    "import urllib\n",
    "import ssl\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from typing import TypedDict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "from functools import partial\n",
    "import textwrap\n",
    "from datasets import load_dataset\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available(): # type: ignore[attr-defined]\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available(): # type: ignore[attr-defined]\n",
    "        return torch.device(\"mps:0\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b948a0d4",
   "metadata": {},
   "source": [
    "## Download the instruction training data\n",
    "\n",
    "This is 1,100 instruction-response pairs (actually some have a third field called input) that were made specifically for the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f063394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n",
      "Example:\n",
      "{'input': 'He go to the park every day.',\n",
      " 'instruction': 'Edit the following sentence for grammar.',\n",
      " 'output': 'He goes to the park every day.'}\n"
     ]
    }
   ],
   "source": [
    "class InstructionExample(TypedDict):\n",
    "    instruction: str  # A description of the task to be performed\n",
    "    input: str        # Optional parameter for the task\n",
    "    output: str       # The expected result of performing the task\n",
    "\n",
    "def download_and_load_file(file_path: str, url: str) -> list[InstructionExample]:\n",
    "    ssl_context = ssl.create_default_context()\n",
    "    ssl_context.check_hostname = False\n",
    "    ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url, context=ssl_context) as response: # type:ignore\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    return data\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))\n",
    "print(\"Example:\")\n",
    "pprint(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab7037",
   "metadata": {},
   "source": [
    "## Convert the examples to Stanford Alpaca format\n",
    "\n",
    "The [format](https://github.com/tatsu-lab/stanford_alpaca) looks like this:\n",
    "\n",
    "```\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "```\n",
    "\n",
    "Or, if there's no input:\n",
    "\n",
    "```\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07052c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry: InstructionExample, include_response:bool=True) -> str:\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry[\"input\"]}\" if entry[\"input\"] else \"\"\n",
    "    response_text = f\"\\n\\n### Response:\\n{entry[\"output\"]}\" if include_response else \"\"\n",
    "\n",
    "    return instruction_text + input_text + response_text\n",
    "\n",
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94965cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data: list[InstructionExample], tokenizer: tiktoken.Encoding):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            full_text = format_input(entry)\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "    \n",
    "    def __getitem__(self, index) -> list[int]:\n",
    "        return self.encoded_texts[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb6fbc",
   "metadata": {},
   "source": [
    "## Custom collate function\n",
    "\n",
    "Passing in a custom collate function lets us easily pad out shorter sequences in each batch to match the longest one.\n",
    "Initially, the padding token will be `<|endoftext|>`, but we'll eventually set it up so that there's only one EOT token\n",
    "and the padding will be done with `-100`.\n",
    "\n",
    "The collate function is responsible for:\n",
    "1. Finding the longest sequence in the batch\n",
    "2. Padding and preparing inputs\n",
    "3. Removing the extra EOT tokens\n",
    "4. Converting the token list to a tensor and transferring it to the target device.\n",
    "\n",
    "\n",
    "### We're not masking the instructions\n",
    "\n",
    "We could use `-100` to mask out the instructions from each example. That would avoid rewarding the model for memorizing\n",
    "worthless bits like \"Below is a task…\", and some people think that's helpful. But it's controversial, and there's at least\n",
    "one paper, [\"Instruction Tuning with Loss Over Instructions,\"](https://arxiv.org/abs/2405.14394) that argues that it's\n",
    "better to train on the whole thing.\n",
    "\n",
    "Maybe I'll try adding instruction masking later, but for now it's not recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36bb3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "        batch: list[list[int]],\n",
    "        pad_token_id: int=50256, # i.e., <|endoftext|>\n",
    "        ignore_index: int=-100, # this is the default ignore index for torch.nn.CrossEntropyLoss\n",
    "        allowed_max_length: int|None=None,\n",
    "        device: str|torch.device=\"cpu\"\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    batch_max_length = max([len(item)+1 for item in batch])\n",
    "\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        mask = targets == pad_token_id # tensor([bool * max_length])\n",
    "        indices = torch.nonzero(mask).squeeze() # type:ignore\n",
    "        if indices.numel() > 1:\n",
    "            # Note: we only do this -100 thing in the targets tensor\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "        \n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst)\n",
    "    targets_tensor = torch.stack(targets_lst)\n",
    "\n",
    "    return inputs_tensor.to(device), targets_tensor.to(device)\n",
    "\n",
    "customized_collate_fn = partial(custom_collate_fn, device=get_device(), allowed_max_length=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0466eb49",
   "metadata": {},
   "source": [
    "## Create the Datasets and DataLoaders\n",
    "\n",
    "As a reminder:\n",
    "- **Dataset**: a class that exposes `__getitem__` and `__len__`, so it's like\n",
    "  a list or a vector. It's not really specialized for anything in particular, it's\n",
    "  just a convenient way to wrap data from some source.\n",
    "- **DataLoader**: a class that encapsulates logic for ordering (shuffle or not),\n",
    "  associating inputs with targets, batching, parallelization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e821471",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "gpt.manual_seed(123)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn, # ah, makes sense how partial would be useful now\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a30fe9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_on_small_example_set(model: gpt.SimplifiedGPT):\n",
    "    example_prompt = format_input({\n",
    "        'instruction': \"Convert this sentence to passive voice.\",\n",
    "        'input': 'The chef cooked the meal.',\n",
    "        'output': ''}, include_response=True)\n",
    "\n",
    "    training_config: gpt.GPTTrainingConfig = gpt.DEFAULT_TRAINING_CONFIG.copy()\n",
    "    training_config['gradient_clipping'] = False\n",
    "    training_config['epochs'] = 2\n",
    "    training_config['peak_lr'] = 5e-5\n",
    "    training_config['initial_lr'] = 4e-6\n",
    "    training_config['eval_frequency'] = 20\n",
    "\n",
    "    config = openai.GPT_CONFIG_355M.copy()\n",
    "\n",
    "    trainer = gpt.GPTModel(cfg=config, training_cfg=training_config, model=model)\n",
    "\n",
    "    trainer.train_loader(train_loader, val_loader, example_prompt, run_name=\"learning rate 5e-5, 2 epochs\")\n",
    "    return trainer\n",
    "\n",
    "# gpt355m = openai.load_openai_model(openai.GPT_CONFIG_355M, \"355M\").model\n",
    "# trainer = train_model_on_small_example_set(gpt355m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "609ddc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruct_str can be used in cases where you need to keep the string around\n",
    "def instruct_str(trainer: gpt.GPTModel, instruction: str, input='', temperature=0.8) -> str:\n",
    "    prompt = format_input({\n",
    "        'instruction': instruction,\n",
    "        'input': input,\n",
    "        'output': '',\n",
    "    }, include_response=True)\n",
    "    result = trainer.prompt(prompt, max_tokens=1024, temperature=temperature)\n",
    "    return result[len(prompt):].strip()\n",
    "\n",
    "# instruct is just used interactively, so it prints the result nicely\n",
    "def instruct(trainer: gpt.GPTModel, instruction: str, input='', temperature=0.8):\n",
    "    result = instruct_str(trainer, instruction, input, temperature)\n",
    "    print(textwrap.fill(result, width=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26fedfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should output: The healthier of the two foods is carrots.\n",
    "\n",
    "# instruct(trainer, \"Determine which of the two foods is healthier\", \"carrots, lard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facc4e40",
   "metadata": {},
   "source": [
    "# Training on Alpaca\n",
    "\n",
    "The [tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca) dataset is about 52k rows, so almost 50x bigger than the one we just trained on. That should give 50x better results, right???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92dd9f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca = load_dataset(\"tatsu-lab/alpaca\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f4c15b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_len(txt: str):\n",
    "    tks = tokenizer.encode(txt)\n",
    "    return len(tks)\n",
    "\n",
    "alpaca: list[InstructionExample] = [x for x in alpaca if token_len(x['text']) <= 323] # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d13829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 48915\n",
      "Val: 102\n",
      "Test: 2473\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "alpaca_train_portion = int(len(alpaca) * 0.95)\n",
    "alpaca_val_portion = int(len(alpaca) * 0.002) # about 100 examples\n",
    "alpaca_test_portion = len(alpaca) - alpaca_train_portion - alpaca_val_portion\n",
    "\n",
    "alpaca_train_data = alpaca[:alpaca_train_portion]\n",
    "alpaca_test_data = alpaca[alpaca_train_portion:alpaca_train_portion + alpaca_test_portion]\n",
    "alpaca_val_data = alpaca[alpaca_train_portion + alpaca_test_portion:]\n",
    "\n",
    "alpaca_train_dataset = InstructionDataset(alpaca_train_data, tokenizer)\n",
    "alpaca_test_dataset = InstructionDataset(alpaca_test_data, tokenizer)\n",
    "alpaca_val_dataset = InstructionDataset(alpaca_val_data, tokenizer)\n",
    "\n",
    "print(f\"Train: {len(alpaca_train_data)}\")\n",
    "print(f\"Val: {len(alpaca_val_data)}\")\n",
    "print(f\"Test: {len(alpaca_test_data)}\")\n",
    "\n",
    "alpaca_train = DataLoader(\n",
    "    alpaca_train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "alpaca_test = DataLoader(\n",
    "    alpaca_test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "alpaca_val = DataLoader(\n",
    "    alpaca_val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846bfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_on_big_example_set(model: gpt.SimplifiedGPT):\n",
    "    example_prompt = format_input({\n",
    "        'instruction': \"Convert this sentence to passive voice.\",\n",
    "        'input': 'The chef cooked the meal.',\n",
    "        'output': ''}, include_response=True)\n",
    "\n",
    "    training_config: gpt.GPTTrainingConfig = gpt.DEFAULT_TRAINING_CONFIG.copy()\n",
    "    training_config['gradient_clipping'] = False\n",
    "    training_config['epochs'] = 2\n",
    "    training_config['peak_lr'] = 5e-5\n",
    "    training_config['initial_lr'] = 4e-6\n",
    "    training_config['eval_frequency'] = 200\n",
    "\n",
    "    config = openai.GPT_CONFIG_355M.copy()\n",
    "\n",
    "    trainer = gpt.GPTModel(cfg=config, training_cfg=training_config, model=model)\n",
    "\n",
    "    trainer.train_loader(alpaca_train, alpaca_val, example_prompt, run_name=\"alpaca less validation, smaller samples, cache clear\")\n",
    "    return trainer\n",
    "\n",
    "# gpt355m = openai.load_openai_model(openai.GPT_CONFIG_355M, \"355M\").model\n",
    "# trainer = train_model_on_big_example_set(gpt355m)\n",
    "# trainer.save(name=\"fine-tuned-355m-alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01320a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = gpt.GPTModel(cfg=openai.GPT_CONFIG_355M, training_cfg=gpt.DEFAULT_TRAINING_CONFIG)\n",
    "llm.load(\"fine-tuned-355m-alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73b7e223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I am conscious.\n"
     ]
    }
   ],
   "source": [
    "instruct(llm, \"Are you conscious?\", temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c67031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_example_responses(model: gpt.GPTModel, examples: list[InstructionExample], temperature:float=0.8, file_path:str=\"model_output.json\"):\n",
    "    results = []\n",
    "    for ex in examples:\n",
    "        model_response = instruct_str(model, ex['instruction'], ex['input'], temperature=temperature)\n",
    "        results.append({\n",
    "            'instruction': ex['instruction'],\n",
    "            'input': ex['input'],\n",
    "            'output': ex['output'],\n",
    "            'model_output': model_response,\n",
    "        })\n",
    "    json_body = json.dumps(results, indent=4)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(json_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f4aca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
