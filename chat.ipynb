{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643c7b1d",
   "metadata": {},
   "source": [
    "# Fine tuning the model to make a chat bot\n",
    "\n",
    "This is the big guacamole at the end of the rainbow. We'll be fine tuning one of the OpenAI models to be able to respond sort of like ChatGPT. I think there's an example of trying to do this on the foundation model in `openai.ipynb` without fine-tuning, and right now it _sucks_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6aec3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
     ]
    }
   ],
   "source": [
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "\n",
    "import import_ipynb\n",
    "import openai # type:ignore\n",
    "import gpt # type:ignore\n",
    "from gpt import get_device # type: ignore\n",
    "import torch\n",
    "import urllib\n",
    "import ssl\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from typing import TypedDict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "from functools import partial\n",
    "import textwrap\n",
    "from datasets import load_dataset\n",
    "import training # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b948a0d4",
   "metadata": {},
   "source": [
    "## Download the instruction training data\n",
    "\n",
    "This is 1,100 instruction-response pairs (actually some have a third field called input) that were made specifically for the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f063394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n",
      "Example:\n",
      "{'input': 'He go to the park every day.',\n",
      " 'instruction': 'Edit the following sentence for grammar.',\n",
      " 'output': 'He goes to the park every day.'}\n"
     ]
    }
   ],
   "source": [
    "class InstructionExample(TypedDict):\n",
    "    instruction: str  # A description of the task to be performed\n",
    "    input: str        # Optional parameter for the task\n",
    "    output: str       # The expected result of performing the task\n",
    "\n",
    "def download_and_load_file(file_path: str, url: str) -> list[InstructionExample]:\n",
    "    ssl_context = ssl.create_default_context()\n",
    "    ssl_context.check_hostname = False\n",
    "    ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url, context=ssl_context) as response: # type:ignore\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    return data\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))\n",
    "print(\"Example:\")\n",
    "pprint(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab7037",
   "metadata": {},
   "source": [
    "## Convert the examples to Stanford Alpaca format\n",
    "\n",
    "The [format](https://github.com/tatsu-lab/stanford_alpaca) looks like this:\n",
    "\n",
    "```\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "```\n",
    "\n",
    "Or, if there's no input:\n",
    "\n",
    "```\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07052c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry: InstructionExample, include_response:bool=True) -> str:\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry[\"input\"]}\" if entry[\"input\"] else \"\"\n",
    "    response_text = f\"\\n\\n### Response:\\n{entry[\"output\"]}\" if include_response else \"\"\n",
    "\n",
    "    return instruction_text + input_text + response_text\n",
    "\n",
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94965cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data: list[InstructionExample], tokenizer: tiktoken.Encoding):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            full_text = format_input(entry)\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "    \n",
    "    def __getitem__(self, index) -> list[int]:\n",
    "        return self.encoded_texts[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb6fbc",
   "metadata": {},
   "source": [
    "## Custom collate function\n",
    "\n",
    "Passing in a custom collate function lets us easily pad out shorter sequences in each batch to match the longest one.\n",
    "Initially, the padding token will be `<|endoftext|>`, but we'll eventually set it up so that there's only one EOT token\n",
    "and the padding will be done with `-100`.\n",
    "\n",
    "The collate function is responsible for:\n",
    "1. Finding the longest sequence in the batch\n",
    "2. Padding and preparing inputs\n",
    "3. Removing the extra EOT tokens\n",
    "4. Converting the token list to a tensor and transferring it to the target device.\n",
    "\n",
    "\n",
    "### We're not masking the instructions\n",
    "\n",
    "We could use `-100` to mask out the instructions from each example. That would avoid rewarding the model for memorizing\n",
    "worthless bits like \"Below is a taskâ€¦\", and some people think that's helpful. But it's controversial, and there's at least\n",
    "one paper, [\"Instruction Tuning with Loss Over Instructions,\"](https://arxiv.org/abs/2405.14394) that argues that it's\n",
    "better to train on the whole thing.\n",
    "\n",
    "Maybe I'll try adding instruction masking later, but for now it's not recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36bb3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "        batch: list[list[int]],\n",
    "        pad_token_id: int=50256, # i.e., <|endoftext|>\n",
    "        ignore_index: int=-100, # this is the default ignore index for torch.nn.CrossEntropyLoss\n",
    "        allowed_max_length: int|None=None,\n",
    "        device: str|torch.device=\"cpu\"\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    batch_max_length = max([len(item)+1 for item in batch])\n",
    "\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        mask = targets == pad_token_id # tensor([bool * max_length])\n",
    "        indices = torch.nonzero(mask).squeeze() # type:ignore\n",
    "        if indices.numel() > 1:\n",
    "            # Note: we only do this -100 thing in the targets tensor\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "        \n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst)\n",
    "    targets_tensor = torch.stack(targets_lst)\n",
    "\n",
    "    return inputs_tensor.to(device), targets_tensor.to(device)\n",
    "\n",
    "customized_collate_fn = partial(custom_collate_fn, device=get_device(), allowed_max_length=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0466eb49",
   "metadata": {},
   "source": [
    "## Create the Datasets and DataLoaders\n",
    "\n",
    "As a reminder:\n",
    "- **Dataset**: a class that exposes `__getitem__` and `__len__`, so it's like\n",
    "  a list or a vector. It's not really specialized for anything in particular, it's\n",
    "  just a convenient way to wrap data from some source.\n",
    "- **DataLoader**: a class that encapsulates logic for ordering (shuffle or not),\n",
    "  associating inputs with targets, batching, parallelization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e821471",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "custom_loader = partial(DataLoader, batch_size=batch_size, collate_fn=customized_collate_fn, shuffle=True, drop_last=True, num_workers=num_workers)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = custom_loader(train_dataset)\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = custom_loader(val_dataset)\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = custom_loader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30fe9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n",
      "355M model loaded.\n",
      "Parameter: \"epoch size\"=116\n",
      "Parameter: \"epochs\"=2\n",
      "Parameter: \"total training size\"=232\n",
      "Parameter: \"validation size\"=6\n",
      "Parameter: \"gradient clipping\"=False\n",
      "[30] Metric: \"training loss\"=1.1167079210281372\n",
      "[30] Metric: \"learning rate\"=4.986173490981773e-05\n",
      "[30] Metric: \"tokens seen\"=16696\n",
      "[30] Metric: \"epoch\"=0\n",
      "[30] Metric: \"progress percent\"=12.931034482758621\n",
      "[30] Example (example): \"curious.\"\n",
      "[30] Metric: \"validation loss\"=0.9243697226047516\n",
      "[60] Metric: \"training loss\"=0.7599226236343384\n",
      "[60] Metric: \"learning rate\"=4.6232121380780034e-05\n",
      "[60] Metric: \"tokens seen\"=33200\n",
      "[60] Metric: \"epoch\"=0\n",
      "[60] Metric: \"progress percent\"=25.862068965517242\n",
      "[60] Example (example): \"Fascinating in their own book.\"\n",
      "[60] Metric: \"validation loss\"=0.7909121612707773\n",
      "[90] Metric: \"training loss\"=0.8845332860946655\n",
      "[90] Metric: \"learning rate\"=3.8357573446201825e-05\n",
      "[90] Metric: \"tokens seen\"=50576\n",
      "[90] Metric: \"epoch\"=0\n",
      "[90] Metric: \"progress percent\"=38.793103448275865\n",
      "[90] Example (example): \"He was really curious about that new word he learned in German.\"\n",
      "[90] Metric: \"validation loss\"=0.7374753554662069\n",
      "[120] Metric: \"training loss\"=0.5996223092079163\n",
      "[120] Metric: \"learning rate\"=2.781244813127552e-05\n",
      "[120] Metric: \"tokens seen\"=67328\n",
      "[120] Metric: \"epoch\"=1\n",
      "[120] Metric: \"progress percent\"=51.724137931034484\n",
      "[120] Example (example): \"The captor must have had an exciting imagination!\"\n",
      "[120] Metric: \"validation loss\"=0.6761478334665298\n",
      "[150] Metric: \"training loss\"=0.7135917544364929\n",
      "[150] Metric: \"learning rate\"=1.670503054125621e-05\n",
      "[150] Metric: \"tokens seen\"=84024\n",
      "[150] Metric: \"epoch\"=1\n",
      "[150] Metric: \"progress percent\"=64.65517241379311\n",
      "[150] Example (example): \"The sentence begins with 'concerning' and the following words were included as verb conjugations that conveyed the request: fascinating, fascinating.\"\n",
      "[150] Metric: \"validation loss\"=0.6707961310942968\n",
      "[180] Metric: \"training loss\"=0.4603101313114166\n",
      "[180] Metric: \"learning rate\"=7.256024780033418e-06\n",
      "[180] Metric: \"tokens seen\"=100744\n",
      "[180] Metric: \"epoch\"=1\n",
      "[180] Metric: \"progress percent\"=77.58620689655173\n",
      "[180] Example (example): \"Plentiful\n",
      "\n",
      "### Instruction:\n",
      "Fancy?\"\n",
      "[180] Metric: \"validation loss\"=0.6588331063588461\n",
      "[210] Metric: \"training loss\"=0.5650301575660706\n",
      "[210] Metric: \"learning rate\"=1.3545689574841342e-06\n",
      "[210] Metric: \"tokens seen\"=117296\n",
      "[210] Metric: \"epoch\"=1\n",
      "[210] Metric: \"progress percent\"=90.51724137931035\n",
      "[210] Example (example): \"Explain what it is.\"\n",
      "[210] Metric: \"validation loss\"=0.6636720399061838\n",
      "Training finished successfully: final validation loss 0.680\n"
     ]
    }
   ],
   "source": [
    "class LlamaExampleGenerator(training.ExampleGenerator):\n",
    "    def __init__(self, instruction: str, input: str = \"\"):\n",
    "        self.prompt = format_input({\n",
    "            'instruction': instruction,\n",
    "            'input': input,\n",
    "            'output': \"\"\n",
    "        }, include_response=True)\n",
    "    \n",
    "    def generate(self, model: gpt.GPTModel) -> str:\n",
    "        result = training.text_completion_topk(model, initial_context=self.prompt, max_new_tokens=128, context_size=512, topk=50, temperature=1.5)\n",
    "        return result[len(self.prompt):].strip()\n",
    "\n",
    "def train_model_on_small_example_set(model: gpt.GPTModel):\n",
    "    example_prompt = format_input({\n",
    "        'instruction': \"Convert this sentence to passive voice.\",\n",
    "        'input': 'The chef cooked the meal.',\n",
    "        'output': ''}, include_response=True)\n",
    "\n",
    "    training_config = training.new_training_config(\n",
    "        gradient_clipping = False,\n",
    "        epochs = 2,\n",
    "        peak_lr = 5e-5,\n",
    "        initial_lr = 4e-6,\n",
    "        eval_freq = 30,\n",
    "    )\n",
    "\n",
    "    optimizer = training.default_optimizer(model, training_config)\n",
    "\n",
    "    training.train(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        training_loader=train_loader,\n",
    "        validation_loader=val_loader,\n",
    "        cfg=training_config,\n",
    "        metrics=training.StdoutMetrics(print_interval=30),\n",
    "        example_generator=LlamaExampleGenerator(instruction=\"Use this word in a sentence\", input=\"fascinating\")\n",
    "    )\n",
    "\n",
    "gpt355m = openai.load_openai_model(openai.GPT_CONFIG_355M, \"355M\")\n",
    "train_model_on_small_example_set(gpt355m)\n",
    "model = gpt355m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "609ddc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruct_str can be used in cases where you need to keep the string around\n",
    "def instruct_str(model: gpt.GPTModel, instruction: str, input='', temperature=0.8) -> str:\n",
    "    prompt = format_input({\n",
    "        'instruction': instruction,\n",
    "        'input': input,\n",
    "        'output': '',\n",
    "    }, include_response=True)\n",
    "    result = training.text_completion_topk(model, initial_context=prompt, max_new_tokens=1024, context_size=512, topk=50, temperature=temperature)\n",
    "    return result[len(prompt):].strip()\n",
    "\n",
    "# instruct is just used interactively, so it prints the result nicely\n",
    "def instruct(trainer: gpt.GPTModel, instruction: str, input='', temperature=0.8):\n",
    "    result = instruct_str(trainer, instruction, input, temperature)\n",
    "    print(textwrap.fill(result, width=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26fedfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dish made with the ingredients listed above would be a traditional Italian \"Pasta\".\n"
     ]
    }
   ],
   "source": [
    "# Should output: The healthier of the two foods is carrots.\n",
    "\n",
    "instruct(model, \"Describe a dish made with the following ingredients\", \"pasta, garlic, oil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facc4e40",
   "metadata": {},
   "source": [
    "# Training on Alpaca\n",
    "\n",
    "The [tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca) dataset is about 52k rows, so almost 50x bigger than the one we just trained on. That should give 50x better results, right???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd9f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca = load_dataset(\"tatsu-lab/alpaca\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4c15b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_len(txt: str):\n",
    "    tks = tokenizer.encode(txt)\n",
    "    return len(tks)\n",
    "\n",
    "alpaca: list[InstructionExample] = [x for x in alpaca if token_len(x['text']) <= 323] # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 48915\n",
      "Val: 102\n",
      "Test: 2473\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "alpaca_train_portion = int(len(alpaca) * 0.95)\n",
    "alpaca_val_portion = int(len(alpaca) * 0.002) # about 100 examples\n",
    "alpaca_test_portion = len(alpaca) - alpaca_train_portion - alpaca_val_portion\n",
    "\n",
    "alpaca_train_data = alpaca[:alpaca_train_portion]\n",
    "alpaca_test_data = alpaca[alpaca_train_portion:alpaca_train_portion + alpaca_test_portion]\n",
    "alpaca_val_data = alpaca[alpaca_train_portion + alpaca_test_portion:]\n",
    "\n",
    "alpaca_train_dataset = InstructionDataset(alpaca_train_data, tokenizer)\n",
    "alpaca_test_dataset = InstructionDataset(alpaca_test_data, tokenizer)\n",
    "alpaca_val_dataset = InstructionDataset(alpaca_val_data, tokenizer)\n",
    "\n",
    "print(f\"Train: {len(alpaca_train_data)}\")\n",
    "print(f\"Val: {len(alpaca_val_data)}\")\n",
    "print(f\"Test: {len(alpaca_test_data)}\")\n",
    "\n",
    "alpaca_train = DataLoader(\n",
    "    alpaca_train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "alpaca_test = DataLoader(\n",
    "    alpaca_test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "alpaca_val = DataLoader(\n",
    "    alpaca_val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846bfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_on_big_example_set(model: gpt.SimplifiedGPT):\n",
    "    example_prompt = format_input({\n",
    "        'instruction': \"Convert this sentence to passive voice.\",\n",
    "        'input': 'The chef cooked the meal.',\n",
    "        'output': ''}, include_response=True)\n",
    "\n",
    "    training_config: gpt.GPTTrainingConfig = gpt.DEFAULT_TRAINING_CONFIG.copy()\n",
    "    training_config['gradient_clipping'] = False\n",
    "    training_config['epochs'] = 2\n",
    "    training_config['peak_lr'] = 5e-5\n",
    "    training_config['initial_lr'] = 4e-6\n",
    "    training_config['eval_frequency'] = 200\n",
    "\n",
    "    config = openai.GPT_CONFIG_355M.copy()\n",
    "\n",
    "    trainer = gpt.GPTModel(cfg=config, training_cfg=training_config, model=model)\n",
    "\n",
    "    trainer.train_loader(alpaca_train, alpaca_val, example_prompt, run_name=\"alpaca less validation, smaller samples, cache clear\")\n",
    "    return trainer\n",
    "\n",
    "# gpt355m = openai.load_openai_model(openai.GPT_CONFIG_355M, \"355M\").model\n",
    "# trainer = train_model_on_big_example_set(gpt355m)\n",
    "# trainer.save(name=\"fine-tuned-355m-alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01320a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = gpt.GPTModel(cfg=openai.GPT_CONFIG_355M, training_cfg=gpt.DEFAULT_TRAINING_CONFIG)\n",
    "# llm.load(\"fine-tuned-355m-alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7e223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I am conscious.\n"
     ]
    }
   ],
   "source": [
    "# instruct(llm, \"Are you conscious?\", temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c67031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_example_responses(model: gpt.GPTModel, examples: list[InstructionExample], temperature:float=0.8, file_path:str=\"model_output.json\"):\n",
    "    results = []\n",
    "    for ex in examples:\n",
    "        model_response = instruct_str(model, ex['instruction'], ex['input'], temperature=temperature)\n",
    "        results.append({\n",
    "            'instruction': ex['instruction'],\n",
    "            'input': ex['input'],\n",
    "            'output': ex['output'],\n",
    "            'model_output': model_response,\n",
    "        })\n",
    "    json_body = json.dumps(results, indent=4)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(json_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f4aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_example_responses(model=trainer, examples=test_data, file_path=\"small_training_output.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f5022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
