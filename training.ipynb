{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed1431c",
   "metadata": {},
   "source": [
    "# Training a smaller GPT-2\n",
    "\n",
    "Having created the GPT model in [gpt.ipynb](./gpt.ipynb), it's time to try training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63992f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import gpt # type: ignore\n",
    "from enum import Enum\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import math\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "import tiktoken\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.cuda as cuda\n",
    "import torch.backends\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from typing import TypedDict\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    if cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps:0\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66fa9aa",
   "metadata": {},
   "source": [
    "## The mini config\n",
    "\n",
    "While we're just setting things up and experimenting, it's better to set a lower context size. If we kept it at 1024, training would take up a lot more memory and a lot more time, slowing down development.\n",
    "\n",
    "`GPT_CONFIG_MINI` sets a smaller context size of 256, which is enough to see things start to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2416bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_MINI: gpt.GPTConfigDict = {**gpt.GPT_CONFIG_124M, \"context_length\": 256}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bf31ff",
   "metadata": {},
   "source": [
    "## Convenience Functions and Example\n",
    "\n",
    "We're adding a few functions to make it easier to interact with the model:\n",
    "- `text_to_token_ids`: represents the very first stage in running the model.\n",
    "- `token_ids_to_text`: represents the very last stage in running the model.\n",
    "\n",
    "Also, another quick example of how they work.\n",
    "\n",
    "Some things that are new:\n",
    "- `simplified_model.eval();` this disables the training functionality of the model, letting us run inference much faster. It will skip dropout and discard gradients. The semicolon just discards the return value, which seems pointless in this example.\n",
    "- `squeeze` and `unsqueeze`: These add or remove (respectively) an empty dimension to the outside of a tensor.\n",
    "  - squeeze: `[[x]]` -> `[x]` (usually to remove the batch dimension when batch_size=1)\n",
    "  - unsqueeze: `[x]` => `[[x]]` (to add a batch dimension where batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c38a43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text (untrained):\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "def text_to_token_ids(text: str, tokenizer: tiktoken.Encoding, device:torch.device=get_device()) -> torch.Tensor:\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor.to(device)\n",
    "\n",
    "def token_ids_to_text(token_ids: torch.Tensor, tokenizer: tiktoken.Encoding) -> str:\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "# example:\n",
    "def untrained_example(start_context:str=\"Every effort moves you\"):\n",
    "    torch.manual_seed(123)\n",
    "    model = gpt.GPTModel(GPT_CONFIG_MINI)\n",
    "    model.to(get_device())\n",
    "    model.eval(); # disables dropout and gradients\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    token_ids = gpt.generate_text_simple(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(start_context, tokenizer).to(get_device()),\n",
    "        max_new_tokens=10,\n",
    "        context_size=GPT_CONFIG_MINI[\"context_length\"],\n",
    "    )\n",
    "\n",
    "    print(\"Output text (untrained):\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    untrained_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf453bb",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "`Dataset` is a class that resembles an iterator, but specialized for passing to\n",
    "a Dataloader. Like an iterator, it exposes `__len__` and `__getitem__` methods.\n",
    "\n",
    "The specialization is that it returns input-target _pairs_ rather than single\n",
    "items. This is what makes it specifically useful for training.\n",
    "\n",
    "In this case, it also encapsulates the tokenization step and implements the\n",
    "length/stride logic, but that's not always the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed6fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text: str, tokenizer: tiktoken.Encoding, max_length: int, stride: int):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            start = i\n",
    "            end = start + max_length\n",
    "            input_chunk = token_ids[start:end]\n",
    "            target_chunk = token_ids[start+1:end+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f7ff51",
   "metadata": {},
   "source": [
    "## Minimal training example\n",
    "\n",
    "Initially I had written a big wrapper class that enclosed a `SimplifiedGPT` instance and encapsulated a ton of training logic.\n",
    "\n",
    "That approach had some advantages and disadvantages, but in the end I think it was a mistake. It resulted in highly complected code\n",
    "with more and more configuration and more and more conditionals, although it did save some amount of passing arguments around.\n",
    "\n",
    "Instead, I think it's preferable to mainly focus on functions that operate on the `GPTModel` class and/or its output logits.\n",
    "\n",
    "To start, I'm going to focus on the bare minimum to train a model on [The Verdict](https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt). That means I need to be able to do the following:\n",
    "\n",
    "1. download the text of The Verdict and save it locally.\n",
    "2. split text into training and validation portions, then return a `training_loader` and `validation_loader`.\n",
    "3. measure a model's loss given a batch (i.e., a set of input-output pairs).\n",
    "4. implement the minimal training flow using the above and a crucial call to `loss.backward()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc718b4",
   "metadata": {},
   "source": [
    "### Training config\n",
    "\n",
    "Training involves a lot of parameters, and they need to be referenced by a few\n",
    "different functions. At the same time, most of them can have sensible defaults.\n",
    "\n",
    "Rather than adding the parameters one by one to every function that might need them,\n",
    "I'm going to create a dataclass that we can pass around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "419eb029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig(TypedDict):\n",
    "    train_percent: float  # If the training corpus is text, the percentage to use for training. The rest is used for validation.\n",
    "    max_length: int       # the maximum length of a given training batch\n",
    "    stride: int           # the distance between starting points of training batches\n",
    "    epochs: int           # the number of times to train on one set of data\n",
    "    lr: float             # the learning rate used by the optimizer\n",
    "    weight_decay: float   # the weight decay used by the optimizer\n",
    "    temperature: float    # the temperature for token generation\n",
    "    topk: int             # the number of logits to select for top-k token generation\n",
    "\n",
    "def new_training_config(\n",
    "        train_percent:float=0.9,\n",
    "        max_length:int=1024,\n",
    "        stride:int|None=None,\n",
    "        epochs:int=1,\n",
    "        lr:float=5e-4,\n",
    "        weight_decay:float=0.1,\n",
    "        temperature:float=0.8,\n",
    "        topk:int=50,\n",
    ") -> TrainingConfig:\n",
    "    if stride is None:\n",
    "        stride = max_length // 2\n",
    "    return TrainingConfig(\n",
    "        train_percent=train_percent,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        temperature=temperature,\n",
    "        topk=topk,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97717397",
   "metadata": {},
   "source": [
    "### Training on \"The Verdict\"\n",
    "\n",
    "Training on this short story is probably the last time we'll pass a text file in\n",
    "directly for training, so some of these functions will be short-lived.\n",
    "\n",
    "The most important function here is `train_simple_text`, which illustrates in\n",
    "the most minimal possible way how the training flow works. Most of that boils\n",
    "down to this one little stanza:\n",
    "\n",
    "```python\n",
    "    for epoch in range(cfg['epochs']):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in training_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = cross_entropy_loss_for_batch(model, input_batch=input_batch, target_batch=target_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "```\n",
    "\n",
    "To break that down in excruciating detail:\n",
    "1. each epoch gets an entire run through the data loader, so we have a top-level loop here.\n",
    "2. `model.train()` puts the model into training mode, where it preserves gradients for back propagation.\n",
    "3. then we loop throuhg input-target pairs from the training loader.\n",
    "4. `optimizer.zero_grad()` clears the optimizer state so that we don't contaminate this batch with the results from a previous batch.\n",
    "5. we calculate the loss next. It's common to think of the loss as a scalar score, but in this case it encapsulates the entire gradient.\n",
    "6. hence, we can call `loss.backward()` to backpropagate and update the model's weights.\n",
    "7. finally, `optimizer.step()` updates the optimizer's parameters for one step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c8b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the text if it's not yet available, then return it as a string\n",
    "def the_verdict() -> str:\n",
    "    \"\"\"Returns the text of the short story \\\"The Verdict\\\". Uses the local filesystem for caching.\"\"\"\n",
    "    file_path = Path(\"the-verdict.txt\")\n",
    "    if not file_path.exists():\n",
    "        url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode('utf-8')\n",
    "            with open(file_path, \"w\") as f:\n",
    "                f.write(text_data)\n",
    "            return text_data\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def text_training_loaders(text: str, cfg: TrainingConfig) -> tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"Turn the given text into two Dataloaders: one for training and one for validation.\"\"\"\n",
    "    split_idx = int(len(text) * cfg['train_percent'])\n",
    "    \n",
    "    # Use partials with the Dataset and Dataloader classes to declutter and enforce consistency\n",
    "    custom_dataset = partial(GPTDatasetV1, tokenizer=tokenizer, max_length=cfg['max_length'], stride=cfg['stride'])\n",
    "    custom_dataloader = partial(DataLoader, batch_size=4, shuffle=True, drop_last=True, num_workers=0)\n",
    "\n",
    "    # raw text portions\n",
    "    train_portion = text[:split_idx]\n",
    "    validation_portion = text[split_idx:]\n",
    "\n",
    "    # tokenized datasets\n",
    "    train_dataset = custom_dataset(train_portion)\n",
    "    validation_dataset = custom_dataset(validation_portion)\n",
    "\n",
    "    # completed dataloaders\n",
    "    train_loader = custom_dataloader(train_dataset)\n",
    "    validation_loader = custom_dataloader(validation_dataset)\n",
    "    return (train_loader, validation_loader)\n",
    "\n",
    "def cross_entropy_loss_for_batch(model: gpt.GPTModel, input_batch: torch.Tensor, target_batch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Returns the model's loss for the given batch. The loss can be used to train the model.\"\"\"\n",
    "    device = model.device()\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    return nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten()) # TODO: explain why flatten\n",
    "\n",
    "def calc_loss_loader(model: gpt.GPTModel, data_loader: DataLoader, num_batches=None) -> float:\n",
    "    \"\"\"Calculates the model's total loss over a number of batches for the given\n",
    "    Dataloader. This helper is used for validation only.\"\"\"\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = cross_entropy_loss_for_batch(model, input_batch, target_batch)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def train_simple_text(model: gpt.GPTModel, text: str, cfg: TrainingConfig) -> float:\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay']\n",
    "    )\n",
    "    training_loader, validation_loader = text_training_loaders(text, cfg)\n",
    "\n",
    "    for epoch in range(cfg['epochs']):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in training_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = cross_entropy_loss_for_batch(model, input_batch=input_batch, target_batch=target_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    if len(validation_loader) == 0:\n",
    "        raise ValueError(\"Ooops, no validation data\")\n",
    "    with torch.no_grad():\n",
    "        validation_loss = calc_loss_loader(model, validation_loader)\n",
    "        return validation_loss\n",
    "\n",
    "def train_verdict(model: gpt.GPTModel) -> float:\n",
    "    torch.manual_seed(123)\n",
    "    text = the_verdict()\n",
    "    verdict_training_config = new_training_config(train_percent=0.85, max_length=256, epochs=10)\n",
    "    return train_simple_text(model=model, text=text, cfg=verdict_training_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d295cc9",
   "metadata": {},
   "source": [
    "### Test out the training loop\n",
    "\n",
    "Below you'll see the untrained output, the final training loss, and the trained output.\n",
    "\n",
    "The trained output looks _much_ better, but there is a little bit of a trick\n",
    "here. Training 10 epochs on such a small and homogenous dataset means that we're\n",
    "_massively_ overfitting. The model is basically just memorizing phrases from the text\n",
    "and spitting them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9038f09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text (untrained):\n",
      " He neverRPteness champagneceptionuffم refres Rexrunnerchapter\n",
      "\n",
      "Validation loss after training: 6.562\n",
      "\n",
      "Output text (trained):\n",
      " He never had lingered to give a lump of sugar to\n"
     ]
    }
   ],
   "source": [
    "def trained_example(model: gpt.GPTModel, start_context):\n",
    "    torch.manual_seed(123)\n",
    "    model.eval()\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    token_ids = gpt.generate_text_simple(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(start_context, tokenizer),\n",
    "        max_new_tokens=10,\n",
    "        context_size=GPT_CONFIG_MINI[\"context_length\"],\n",
    "    )\n",
    "\n",
    "    print(\"Output text (trained):\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_context = \"He never\"\n",
    "    untrained_example(start_context=start_context)\n",
    "    model = gpt.GPTModel(GPT_CONFIG_MINI)\n",
    "    model.to(get_device())\n",
    "    val_loss = train_verdict(model)\n",
    "    print(f\"\\nValidation loss after training: {val_loss:.3f}\\n\")\n",
    "    trained_example(model, start_context=start_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af6d32",
   "metadata": {},
   "source": [
    "# Better text generation\n",
    "\n",
    "So far, we've been using `gpt.generate_text_simple`, which chooses the next token by just finding the logit with the highest score.\n",
    "\n",
    "That's good enough for illustration early in the process, but it honestly sucks. It results in super boring and repetitive output.\n",
    "\n",
    "Let's implement a more typical Top-K multinomial algorithm:\n",
    "1. Limit our choices to the top k logits, where k is a constant of our choosing. This prevents extremely unlikely tokens from _ever_ being generated.\n",
    "2. Divide the logit probabilities by `temperature`. A low temperature emphasizes logits with higher probabilities, a higher temperature creates chaos by emphasizing lower probabilities.\n",
    "3. Randomly choose from the remaining logits, weighted by their respective probabilities. Logits with higher probabilities will be chosen more often, but even unlikely ones will be chosen sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ddea66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "END_OF_TEXT = 50256\n",
    "\n",
    "def choose_from_topk(logits: torch.Tensor, topk: int, temperature: float) -> torch.Tensor:\n",
    "    top_logits, top_pos = torch.topk(logits, topk)\n",
    "    filtered = torch.full_like(\n",
    "        logits, -torch.inf\n",
    "    )\n",
    "    filtered.scatter_(dim=1, index=top_pos, src=top_logits) #huh?\n",
    "    scaled = filtered / temperature\n",
    "    probabilities = torch.softmax(scaled, dim=-1) # note: might have trouble with device\n",
    "    if torch.any(torch.isnan(probabilities)) or torch.any(probabilities < 0):\n",
    "        print(\"Bad probabilities:\", probabilities)\n",
    "        print(\"Logits:\", logits)\n",
    "        raise ValueError(\"NaNs or invalid values in probabilities\")\n",
    "    return torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "def generate_text_topk(model: gpt.GPTModel, token_ids: torch.Tensor, max_new_tokens: int, context_size: int, topk: int, temperature: float):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = token_ids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        idx_next = choose_from_topk(logits, topk, temperature)\n",
    "        if idx_next.item() == END_OF_TEXT:\n",
    "            break\n",
    "        token_ids = torch.cat((token_ids, idx_next), dim=1)\n",
    "    return token_ids\n",
    "\n",
    "def text_completion_topk(model, initial_context: str, max_new_tokens:int=10, context_size:int=256, topk:int=50, temperature:float=1.5):\n",
    "    device = model.device()\n",
    "    encoded = tokenizer.encode(initial_context)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    out = generate_text_topk(\n",
    "        model,\n",
    "        encoded_tensor,\n",
    "        context_size=context_size,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        topk=topk,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e33b0332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He never thought him, so often, basking under similar\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_context = \"He never\"\n",
    "    model = gpt.GPTModel(GPT_CONFIG_MINI)\n",
    "    model.to(get_device())\n",
    "    train_verdict(model)\n",
    "    print(text_completion_topk(model, start_context, temperature=0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aa3594",
   "metadata": {},
   "source": [
    "# Saving and loading model state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model: gpt.GPTModel, name: str, overwrite:bool=False, optimizer: optim.Optimizer|None=None):\n",
    "    if len(name) == 0:\n",
    "        raise ValueError(\"name can't be empty\")\n",
    "    path = Path(f\"{name}.pth\")\n",
    "    if path.exists() and not overwrite:\n",
    "        raise FileExistsError(f\"{path} already exists and overwrite is set to False\")\n",
    "\n",
    "    optimizer_state = optimizer.state_dict() if optimizer is not None else None\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer_state,\n",
    "    }, path)\n",
    "\n",
    "def load(model: gpt.GPTModel, name: str, optimizer: optim.Optimizer|None=None):\n",
    "    if len(name) == 0:\n",
    "        raise ValueError(\"name can't be empty\")\n",
    "    path = Path(f\"{name}.pth\")\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"{path} does not exist\")\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer_state_dict = checkpoint[\"optimizer_state_dict\"]\n",
    "    if optimizer_state_dict is not None and optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cce48b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"He never\"\n",
    "model = gpt.GPTModel(GPT_CONFIG_MINI)\n",
    "model.to(get_device())\n",
    "train_verdict(model) # No access to optimizer at this point\n",
    "save(model, \"test-verdict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f09a6c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He never thought him, so often, basking under similar\n"
     ]
    }
   ],
   "source": [
    "model = gpt.GPTModel(GPT_CONFIG_MINI)\n",
    "load(model, name=\"test-verdict\")\n",
    "print(text_completion_topk(model, start_context, temperature=0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7019ee2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GPTConfigDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     19\u001b[39m DEFAULT_TRAINING_CONFIG: GPTTrainingConfig = {\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mn_epochs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m,\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minitial_lr\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.0001\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     29\u001b[39m }\n\u001b[32m     31\u001b[39m END_OF_TEXT = \u001b[32m50256\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mGPTModel\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mGPTConfigDict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_cfg\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mGPTTrainingConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_cpu\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mGPTModel\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mGPTModel\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg: \u001b[43mGPTConfigDict\u001b[49m, training_cfg: GPTTrainingConfig, force_cpu:\u001b[38;5;28mbool\u001b[39m=\u001b[38;5;28;01mFalse\u001b[39;00m, model=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     35\u001b[39m         \u001b[38;5;28mself\u001b[39m.cfg = cfg\n\u001b[32m     36\u001b[39m         \u001b[38;5;28mself\u001b[39m.training_cfg = training_cfg\n",
      "\u001b[31mNameError\u001b[39m: name 'GPTConfigDict' is not defined"
     ]
    }
   ],
   "source": [
    "class DecodingStrategy(Enum):\n",
    "    Greedy = 1\n",
    "    Multinomial = 2\n",
    "    TopK = 3\n",
    "\n",
    "class GPTTrainingConfig(TypedDict):\n",
    "    n_epochs: int            # the number of times to iterate through the entire dataloader\n",
    "    initial_lr: float        # the starting (minimum) learning rate\n",
    "    peak_lr: float           # the maximum learning rate\n",
    "    warmup_steps: int        # the number of steps it should take to ramp up from initial_lr to peak_lr.\n",
    "                             #   defaults to len(loader) / n_epochs if 0.\n",
    "    gradient_clipping: bool  # whether to apply norm-based gradient clipping with a max_norm of 1.0\n",
    "    eval_frequency: int      # the model will be evaluated and saved every this number of batches\n",
    "    name: str                # the base filename used for saving results. if empty, model will not be saved.\n",
    "    weight_decay: float      # helps to keep the model from overfitting by penalizing large weights\n",
    "    train_ratio: float       # in text mode, the portion of the text that is used\n",
    "                             #   for training. the rest is reserved for validation.\n",
    "\n",
    "DEFAULT_TRAINING_CONFIG: GPTTrainingConfig = {\n",
    "    \"n_epochs\": 1,\n",
    "    \"initial_lr\": 0.0001,\n",
    "    \"peak_lr\": 0.01,\n",
    "    \"warmup_steps\": 0,\n",
    "    \"gradient_clipping\": False,\n",
    "    \"eval_frequency\": 5,\n",
    "    \"train_ratio\": 0.9,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"name\": \"\",\n",
    "}\n",
    "\n",
    "END_OF_TEXT = 50256\n",
    "\n",
    "class GPTModel:\n",
    "    def __init__(self, cfg: GPTConfigDict, training_cfg: GPTTrainingConfig, force_cpu:bool=False, model=None):\n",
    "        self.cfg = cfg\n",
    "        self.training_cfg = training_cfg\n",
    "        self.name = training_cfg['name']\n",
    "        self.train_ratio = training_cfg['train_ratio']\n",
    "        self.eval_frequency = training_cfg['eval_frequency']\n",
    "        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.force_cpu = force_cpu\n",
    "        self.device = self.get_device()\n",
    "        self.gradient_clipping = training_cfg['gradient_clipping']\n",
    "        if model == None:\n",
    "            self.model = SimplifiedGPT(self.cfg).to(self.device)\n",
    "        else:\n",
    "            self.model = model.to(self.device)\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            weight_decay=training_cfg['weight_decay'],\n",
    "        )\n",
    "        self.tokens_seen, self.global_step = 0, -1\n",
    "\n",
    "    def warmup_steps(self, loader):\n",
    "        cfg_val = self.training_cfg['warmup_steps']\n",
    "        if cfg_val == 0:\n",
    "            return len(loader) / self.training_cfg['n_epochs']\n",
    "        return cfg_val\n",
    "\n",
    "    def learning_rate(self, step, loader):\n",
    "        warmup_steps = self.warmup_steps(loader)\n",
    "        if step < warmup_steps:\n",
    "            return self.training_cfg['initial_lr'] + step * self.lr_increment(loader)\n",
    "        \n",
    "        total_training_steps = len(loader) * self.training_cfg['n_epochs']\n",
    "        min_lr = 0.1 * self.training_cfg['initial_lr']\n",
    "        peak_lr = self.training_cfg['peak_lr']\n",
    "        # The following amounts to a cosine curve starting at the peak_lr and tapering off to the min_lr\n",
    "        # slowly by the time we get to 100% progress.\n",
    "        if total_training_steps <= warmup_steps:\n",
    "            # this code is nonsense when epochs == 1,\n",
    "            # TODO come up with a better calculation in that case.\n",
    "            total_training_steps = warmup_steps + 1\n",
    "        progress = ((step - warmup_steps) / \n",
    "                    (total_training_steps - warmup_steps))\n",
    "        return min_lr + (peak_lr - min_lr) * 0.5 * (\n",
    "            1 + math.cos(math.pi * progress)\n",
    "        )\n",
    "\n",
    "    def lr_increment(self, loader):\n",
    "        return (self.training_cfg['peak_lr'] - self.training_cfg['initial_lr']) / self.warmup_steps(loader)\n",
    "\n",
    "    def save(self, name: str):\n",
    "        if name == \"\":\n",
    "            return\n",
    "        torch.save({\n",
    "            \"model_state_dict\": self.model.state_dict(),\n",
    "            \"optimizer_state_dict\": self.optimizer.state_dict()\n",
    "        },\n",
    "        f\"{name}.pth\")\n",
    "\n",
    "    def load(self, name: str):\n",
    "        checkpoint = torch.load(f\"{name}.pth\")\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "    def get_device(self) -> torch.device:\n",
    "        if self.force_cpu:\n",
    "            return torch.device(\"cpu\")\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.device(\"cuda:0\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        else:\n",
    "            return torch.device(\"cpu\")\n",
    "\n",
    "    def create_dataloader(self, text: str, batch_size:int=4, max_length:int=256, stride:int=128, shuffle:bool=True, drop_last:bool=True, num_workers:int=0) -> DataLoader:\n",
    "        dataset = GPTDatasetV1(text, self.tokenizer, max_length, stride)\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            drop_last=drop_last,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "\n",
    "    def loss_for_batch(self, input_batch: torch.Tensor, target_batch: torch.Tensor) -> torch.Tensor:\n",
    "        assert input_batch.max().item() < self.cfg['vocab_size']\n",
    "        assert target_batch.max().item() < self.cfg['vocab_size']\n",
    "        input_batch, target_batch = input_batch.to(self.device), target_batch.to(self.device)\n",
    "        logits = self.model(input_batch)\n",
    "        return nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten()).to(self.device)\n",
    "\n",
    "    def calc_loss_loader(self, data_loader, num_batches=None) -> float:\n",
    "        total_loss = 0\n",
    "        if len(data_loader) == 0:\n",
    "            return float(\"nan\")\n",
    "        elif num_batches is None:\n",
    "            num_batches = len(data_loader)\n",
    "        else:\n",
    "            num_batches = min(num_batches, len(data_loader))\n",
    "        \n",
    "        for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "            if i < num_batches:\n",
    "                loss = self.loss_for_batch(input_batch, target_batch)\n",
    "                total_loss += loss.item()\n",
    "            else:\n",
    "                break\n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def loaders(self, text: str, max_length:int=0, stride:int=0) -> tuple[DataLoader, DataLoader]:\n",
    "        split_idx = int(self.train_ratio * len(text))\n",
    "        train_data = text[:split_idx]\n",
    "        validation_data = text[split_idx:]\n",
    "        if stride == 0:\n",
    "            stride = self.cfg[\"context_length\"]\n",
    "        if max_length == 0:\n",
    "            max_length = self.cfg[\"context_length\"]\n",
    "        train_loader = self.create_dataloader(\n",
    "            train_data,\n",
    "            batch_size=2,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "        )\n",
    "        validation_loader = self.create_dataloader(\n",
    "            validation_data,\n",
    "            batch_size=2,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        return (train_loader, validation_loader)\n",
    "    \n",
    "    def evaluate(self, validation_loader, epoch:int=0, prompt:str=\"\"):\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            validation_loss = self.calc_loss_loader(validation_loader)\n",
    "        summary = {\n",
    "            \"validation_loss\": validation_loss,\n",
    "            \"device_type\": self.device.type,\n",
    "            \"epoch\": epoch\n",
    "        }\n",
    "        if len(prompt) > 0:\n",
    "            example_output = self.prompt(prompt)\n",
    "            summary[\"example_output\"] = example_output[len(prompt):]\n",
    "        return summary\n",
    "\n",
    "    def choose(self, strategy: DecodingStrategy, logits: torch.Tensor, temperature:float, k:int=50):\n",
    "        match strategy:\n",
    "            case DecodingStrategy.Greedy:\n",
    "                probabilities = torch.softmax(logits, dim=-1)\n",
    "                result = torch.argmax(probabilities, dim=-1, keepdim=True)\n",
    "                return result\n",
    "            case DecodingStrategy.Multinomial:\n",
    "                scaled = logits / temperature\n",
    "                probabilities = torch.softmax(scaled, dim=-1)\n",
    "                result = torch.multinomial(probabilities, num_samples=1)\n",
    "                return result\n",
    "            case DecodingStrategy.TopK:\n",
    "                # print_topk_logits(logits, self.tokenizer, k)\n",
    "                top_logits, top_pos = torch.topk(logits, k)\n",
    "                filtered = torch.full_like(\n",
    "                    logits, -torch.inf\n",
    "                )\n",
    "                filtered.scatter_(dim=1, index=top_pos, src=top_logits) #huh?\n",
    "                scaled = filtered / temperature\n",
    "                probabilities = torch.softmax(scaled, dim=-1).to(self.device)\n",
    "                if torch.any(torch.isnan(probabilities)) or torch.any(probabilities < 0):\n",
    "                    print(\"Bad probabilities:\", probabilities)\n",
    "                    print(\"Logits:\", logits)\n",
    "                    raise ValueError(\"NaNs or invalid values in probabilities\")\n",
    "                # assert probabilities.device == self.device\n",
    "                return torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "    def generate_text_simple(self, token_ids: torch.Tensor, max_new_tokens, context_size, temperature:float):\n",
    "        self.model.to(self.device)\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = token_ids[:, -context_size:]\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            idx_next = self.choose(DecodingStrategy.TopK, logits, temperature=temperature)\n",
    "            if idx_next.item() == END_OF_TEXT:\n",
    "                break\n",
    "            token_ids = torch.cat((token_ids, idx_next), dim=1)\n",
    "        return token_ids\n",
    "\n",
    "    def prompt(self, text: str, max_tokens:int=10, temperature:float=0.8) -> str:\n",
    "        encoded = self.tokenizer.encode(text)\n",
    "        encoded_tensor = torch.tensor(encoded).unsqueeze(0).to(self.device)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        out = self.generate_text_simple(\n",
    "            encoded_tensor,\n",
    "            max_tokens,\n",
    "            self.cfg[\"context_length\"],\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        decoded_text = self.tokenizer.decode(out.squeeze(0).tolist())\n",
    "        return decoded_text\n",
    "\n",
    "    def generate_and_print_sample(self, prompt: str) -> None:\n",
    "        print(self.prompt(prompt))\n",
    "\n",
    "    def fix_summary(self, summary):\n",
    "        \"\"\"Makes summaries a little nicer to look at by reordering keys and rounding numbers.\"\"\"\n",
    "        summary['training_loss'] = round(summary['training_loss'], 4)\n",
    "        summary['validation_loss'] = round(summary['validation_loss'], 4)\n",
    "        return { k: summary[k] for k in [\"training_loss\", \"validation_loss\", \"tokens_seen\", \"example_output\", \"learning_rate\"] }\n",
    "\n",
    "    def train_loader(self, training_loader: DataLoader, validation_loader: DataLoader, evaluation_prompt: str, epochs:int=1, run_name=\"\"):\n",
    "        self.model.to(self.device)\n",
    "        self.tokens_seen = 0\n",
    "        loss_summaries = []\n",
    "        lr = self.training_cfg['initial_lr']\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            if len(run_name) > 0:\n",
    "                mlflow.set_tag(\"mlflow.runName\", run_name)\n",
    "            mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "            mlflow.log_param(\"lr\", lr)\n",
    "            mlflow.log_param(\"epochs\", epochs)\n",
    "            mlflow.log_metric(\"tokens_seen\", self.tokens_seen, step=self.global_step)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                self.model.train()\n",
    "                for input_batch, target_batch in training_loader:\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.global_step += 1\n",
    "                    lr = self.learning_rate(self.global_step, training_loader)\n",
    "                    for param_group in self.optimizer.param_groups:\n",
    "                        param_group[\"lr\"] = lr\n",
    "                    loss = self.loss_for_batch(\n",
    "                        input_batch, target_batch\n",
    "                    )\n",
    "                    loss_val = loss.item()\n",
    "                    loss.backward()\n",
    "    \n",
    "                    # Gradient clipping: ensure the norms of the model parameters\n",
    "                    # are not greater than 1 (only after the warmup period).\n",
    "                    if self.gradient_clipping and self.global_step < self.warmup_steps(training_loader):\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            self.model.parameters(), max_norm=1.0,\n",
    "                        )\n",
    "                    \n",
    "                    self.optimizer.step()\n",
    "                    self.tokens_seen += input_batch.numel()\n",
    "    \n",
    "                    mlflow.log_metric(\"lr\", lr, step=self.global_step)\n",
    "                    mlflow.log_metric(\"loss\", loss_val, step=self.global_step)\n",
    "                    mlflow.log_metric(\"tokens_seen\", self.tokens_seen, step=self.global_step)\n",
    "                    mlflow.log_metric(\"epoch\", epoch, step=self.global_step)\n",
    "    \n",
    "                    if self.eval_frequency > 0 and self.global_step % self.eval_frequency == 0:\n",
    "                        summary = self.evaluate(validation_loader, epoch, evaluation_prompt)\n",
    "                        summary[\"training_loss\"] = loss_val\n",
    "                        summary[\"tokens_seen\"] = self.tokens_seen\n",
    "                        summary[\"learning_rate\"] = lr\n",
    "                        summary = self.fix_summary(summary)\n",
    "                        print(summary)\n",
    "                        loss_summaries.append(summary)\n",
    "                        mlflow.log_metric(\"validation_loss\", summary[\"validation_loss\"], step=self.global_step)\n",
    "                        mlflow.log_text(summary['example_output'], artifact_file=f\"example_outputs/example_{int(time.time())}.txt\")\n",
    "                        self.save(self.name)\n",
    "                        # Defensive: try to prevent OOM\n",
    "                        torch.cuda.empty_cache()\n",
    "        print(f\"Training complete. Steps: {self.global_step}, tokens: {self.tokens_seen}\")\n",
    "\n",
    "    def train_text(self, text: str, max_length:int=0, stride:int=0, prompt:str=\"Hello, I am \"):\n",
    "        training_loader, validation_loader = self.loaders(text, max_length, stride)\n",
    "        epochs = self.training_cfg['n_epochs']\n",
    "        return self.train_loader(training_loader, validation_loader, prompt, epochs)\n",
    "\n",
    "\n",
    "def print_topk_logits(logits: torch.Tensor, tokenizer, k=10):\n",
    "    \"\"\"\n",
    "    logits: torch.Tensor of shape [vocab_size] or [1, vocab_size]\n",
    "    tokenizer: tiktoken tokenizer\n",
    "    k: number of top tokens to print\n",
    "\n",
    "    Used for debugging the `choose` method above.\n",
    "    \"\"\"\n",
    "    if logits.dim() == 2:\n",
    "        logits = logits.squeeze(0)  # shape [vocab_size]\n",
    "    \n",
    "    topk_vals, topk_indices = torch.topk(logits, k)\n",
    "    \n",
    "    print(f\"{'Rank':<4} {'Token':<12} {'Index':<6} {'Logit':<10}\")\n",
    "    print(\"-\" * 40)\n",
    "    for i in range(k):\n",
    "        token_str = tokenizer.decode([topk_indices[i].item()])\n",
    "        print(f\"{i+1:<4} {repr(token_str):<12} {topk_indices[i].item():<6} {topk_vals[i].item():<10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb011898",
   "metadata": {},
   "source": [
    "# Training on Project Gutenberg\n",
    "\n",
    "The following few sections include the code necessary to:\n",
    "1. Preprocess the [deepmind/pg19](https://huggingface.co/datasets/deepmind/pg19) dataset and wrap it in a Dataset class that we can use for training.\n",
    "2. Sample some batches from the dataset to see what typical text looks like.\n",
    "3. Train a GPTModel with a context size of 512 on this corpus.\n",
    "\n",
    "If you want to do this, better get a big cup of coffee and about 60 gigs of\n",
    "space ready. Downloading the data from HuggingFace takes a while, then you have\n",
    "to expand it and then the LazyTokenDatasetPG19 class will create a cache containing\n",
    "tokenized versions of every text.\n",
    "\n",
    "After that, training takes about 6-8 hours to reach a plateau on my machine with an NVidia 3080."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad38c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "import textwrap\n",
    "\n",
    "class LazyTokenDatasetPG19(Dataset):\n",
    "    \"\"\"Preprocesses the dataset (assumed to be deepmind/pg19!) by creating a\n",
    "    directory './tokens' containing pre-tokenized versions of all books in the\n",
    "    dataset. This takes a while the first time you run it (maybe 20 minutes),\n",
    "    but after that it's just a few seconds.\n",
    "    \n",
    "    The initialized object is suitable for passing to Dataloader.\"\"\"\n",
    "    GUTENBERG_END_RE = re.compile(r\"(?i)end of (the )?project gutenberg.*\", re.DOTALL)\n",
    "    TOO_MANY_NEWLINES_RE = re.compile(r\"\\n{3,}\")\n",
    "    LEADING_NEWLINES_RE = re.compile(r\"^\\n+\")\n",
    "    \n",
    "    def __init__(self, context_len:int=256):\n",
    "        super().__init__()\n",
    "        self.context_len = context_len\n",
    "        self.preprocess()\n",
    "        self.file_paths = glob.glob(\"tokens/*.pt\")\n",
    "        self.samples: list[tuple[int, int]] = []\n",
    "        print(\"Loading data from tokens directory\")\n",
    "        for i, path in enumerate(self.file_paths):\n",
    "            length = torch.load(path, map_location=\"cpu\").shape[0]\n",
    "            for j in range(0, length - context_len, context_len):\n",
    "                self.samples.append((i, j))\n",
    "            if i % 5_000 == 0:\n",
    "                print(f\"Loaded up to book {i}...\")\n",
    "        print(\"Loading complete\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        book_idx, start = self.samples[idx]\n",
    "        tokens = torch.load(self.file_paths[book_idx], map_location=\"cpu\")\n",
    "        input_ids = tokens[start : start + self.context_len]\n",
    "        target_ids = tokens[start + 1 : start + self.context_len + 1]\n",
    "        return input_ids, target_ids\n",
    "\n",
    "    def preprocess(self):\n",
    "        ds = load_dataset(\"deepmind/pg19\", split=\"train\")\n",
    "        os.makedirs(\"tokens\", exist_ok=True)\n",
    "        existing_filepaths = glob.glob(\"tokens/book_*.pt\")\n",
    "        if len(existing_filepaths) >= (ds.num_rows - 5_000): # type: ignore[attr-defined]\n",
    "            print(\"Preprocessing not needed.\")\n",
    "            return\n",
    "        print(\"Preprocessing data to tokens directory.\")\n",
    "        tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        for i, book in enumerate(ds):\n",
    "            path = Path(f\"tokens/book_{i}.pt\")\n",
    "            if path.exists():\n",
    "                continue\n",
    "            text = self.clean_text(book['text'])\n",
    "            if len(text) < self.context_len + 1:\n",
    "                continue\n",
    "            tokens = tokenizer.encode(text)\n",
    "            torch.save(torch.tensor(tokens, dtype=torch.long), path)\n",
    "            if i % 1_000 == 0:\n",
    "                print(f\"Completed preprocessing book {i}\")\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        # Remove Gutenberg end matter\n",
    "        text = self.GUTENBERG_END_RE.split(text)[0]\n",
    "        # Remove leading newlines/whitespace\n",
    "        text = self.LEADING_NEWLINES_RE.sub(\"\", text)\n",
    "        # Collapse 3+ newlines into exactly 2 (paragraph break)\n",
    "        text = self.TOO_MANY_NEWLINES_RE.sub(\"\\n\\n\", text)\n",
    "        # Eliminate chapter:verse markings\n",
    "        text = re.sub(r'\\b\\d+:\\d+\\b', '', text)\n",
    "        # Unwrap lines in each paragraph, but preserve paragraphs\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        unwrapped_paragraphs = [re.sub(r\"\\n\", \" \", p) for p in paragraphs]\n",
    "        text = '\\n\\n'.join(unwrapped_paragraphs)\n",
    "        # don't allow multiple spaces in a row\n",
    "        text = re.sub(r\"[^\\S\\n]+\", \" \", text)\n",
    "        return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8493b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_loader(dataloader, n):\n",
    "    \"\"\"Given a DataLoader and a number of samples, prints batches from the DataLoader.\"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    i = n\n",
    "    for input_batch, target_batch in dataloader:\n",
    "        if i == 0:\n",
    "            break\n",
    "        i -= 1\n",
    "        text = tokenizer.decode(input_batch.tolist()[:64])\n",
    "        print(text)\n",
    "        print(\"----------------------------------------\")\n",
    "\n",
    "# Uncomment below to see what's in the sanitized pg19 dataset.\n",
    "# sample_loader(DataLoader(ltds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5112be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_MEDIUM: GPTConfigDict = {**GPT_CONFIG_124M, \"context_length\": 512} # 1024 is just too big to train locally\n",
    "ltds = None\n",
    "training_cfg: GPTTrainingConfig = {**DEFAULT_TRAINING_CONFIG, \"n_epochs\": 20, \"eval_frequency\": 500, \"name\": \"pg19_medium\"}\n",
    "model = GPTModel(GPT_CONFIG_MEDIUM, training_cfg)\n",
    "\n",
    "def train_pg19(name: str, force_refresh:bool=False):\n",
    "    walden = load_dataset(\"deepmind/pg19\", split='validation')[0]['text'[63:]] # type: ignore\n",
    "    if ltds == None or force_refresh: # type: ignore\n",
    "        ltds = LazyTokenDatasetPG19(context_len=512)\n",
    "    if force_refresh:\n",
    "        model.load(name)\n",
    "    training_loader = DataLoader(\n",
    "        ltds, # type: ignore\n",
    "        shuffle=True,\n",
    "        batch_size=4,\n",
    "    )\n",
    "    model.train_loader(training_loader=training_loader, evaluation_text=walden, evaluation_prompt=\"When I\", epochs=1) # type: ignore\n",
    "\n",
    "# Uncomment below to actually train the model. You won't get good results until you do.\n",
    "# model.load(\"pg19_medium\")\n",
    "# train_pg19(\"new_training_run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802d6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(model: GPTModel, txt:str, max_tokens=128, temperature=0.8):\n",
    "    result = model.prompt(txt, max_tokens=max_tokens, temperature=temperature)\n",
    "    print(textwrap.fill(result, width=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the model is trained, you should see some... interesting results from this.\n",
    "# Otherwise it'll just be gibberish.\n",
    "# prompt(model, \"Ere thrice the sun done salutation to the dawn,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ef755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
