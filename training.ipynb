{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed1431c",
   "metadata": {},
   "source": [
    "# Training a smaller GPT-2\n",
    "\n",
    "Having created the GPT model in [gpt.ipynb](./gpt.ipynb), it's time to try training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63992f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import gpt # type: ignore\n",
    "from enum import Enum\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import math\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "import tiktoken\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.cuda as cuda\n",
    "import torch.backends\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from typing import TypedDict, Optional\n",
    "from collections.abc import Callable\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    if cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps:0\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66fa9aa",
   "metadata": {},
   "source": [
    "## The mini config\n",
    "\n",
    "While we're just setting things up and experimenting, it's better to set a lower context size. If we kept it at 1024, training would take up a lot more memory and a lot more time, slowing down development.\n",
    "\n",
    "`GPT_CONFIG_MINI` sets a smaller context size of 256, which is enough to see things start to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2416bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_MINI: gpt.GPTConfigDict = {**gpt.GPT_CONFIG_124M, \"context_length\": 256}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bf31ff",
   "metadata": {},
   "source": [
    "## Convenience Functions and Example\n",
    "\n",
    "We're adding a few functions to make it easier to interact with the model:\n",
    "- `text_to_token_ids`: represents the very first stage in running the model.\n",
    "- `token_ids_to_text`: represents the very last stage in running the model.\n",
    "\n",
    "Also, another quick example of how they work.\n",
    "\n",
    "Some things that are new:\n",
    "- `simplified_model.eval();` this disables the training functionality of the model, letting us run inference much faster. It will skip dropout and discard gradients. The semicolon just discards the return value, which seems pointless in this example.\n",
    "- `squeeze` and `unsqueeze`: These add or remove (respectively) an empty dimension to the outside of a tensor.\n",
    "  - squeeze: `[[x]]` -> `[x]` (usually to remove the batch dimension when batch_size=1)\n",
    "  - unsqueeze: `[x]` => `[[x]]` (to add a batch dimension where batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c38a43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text (untrained):\n",
      " Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "def text_to_token_ids(text: str, tokenizer: tiktoken.Encoding, device:torch.device=get_device()) -> torch.Tensor:\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor.to(device)\n",
    "\n",
    "def token_ids_to_text(token_ids: torch.Tensor, tokenizer: tiktoken.Encoding) -> str:\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "# example:\n",
    "def untrained_example(start_context:str=\"Every effort moves you\"):\n",
    "    torch.manual_seed(123)\n",
    "    model = gpt.GPTModel(GPT_CONFIG_MINI)\n",
    "    model.to(get_device())\n",
    "    model.eval(); # disables dropout and gradients\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    token_ids = gpt.generate_text_simple(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(start_context, tokenizer).to(get_device()),\n",
    "        max_new_tokens=10,\n",
    "        context_size=GPT_CONFIG_MINI[\"context_length\"],\n",
    "    )\n",
    "\n",
    "    print(\"Output text (untrained):\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    untrained_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf453bb",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "`Dataset` is a class that resembles an iterator, but specialized for passing to\n",
    "a Dataloader. Like an iterator, it exposes `__len__` and `__getitem__` methods.\n",
    "\n",
    "The specialization is that it returns input-target _pairs_ rather than single\n",
    "items. This is what makes it specifically useful for training.\n",
    "\n",
    "In this case, it also encapsulates the tokenization step and implements the\n",
    "length/stride logic, but that's not always the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed6fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text: str, tokenizer: tiktoken.Encoding, max_length: int, stride: int):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            start = i\n",
    "            end = start + max_length\n",
    "            input_chunk = token_ids[start:end]\n",
    "            target_chunk = token_ids[start+1:end+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f7ff51",
   "metadata": {},
   "source": [
    "## Minimal training example\n",
    "\n",
    "Initially I had written a big wrapper class that enclosed a `SimplifiedGPT` instance and encapsulated a ton of training logic.\n",
    "\n",
    "That approach had some advantages and disadvantages, but in the end I think it was a mistake. It resulted in highly complected code\n",
    "with more and more configuration and more and more conditionals, although it did save some amount of passing arguments around.\n",
    "\n",
    "Instead, I think it's preferable to mainly focus on functions that operate on the `GPTModel` class and/or its output logits.\n",
    "\n",
    "To start, I'm going to focus on the bare minimum to train a model on [The Verdict](https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt). That means I need to be able to do the following:\n",
    "\n",
    "1. download the text of The Verdict and save it locally.\n",
    "2. split text into training and validation portions, then return a `training_loader` and `validation_loader`.\n",
    "3. measure a model's loss given a batch (i.e., a set of input-output pairs).\n",
    "4. implement the minimal training flow using the above and a crucial call to `loss.backward()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc718b4",
   "metadata": {},
   "source": [
    "### Training config\n",
    "\n",
    "Training involves a lot of parameters, and they need to be referenced by a few\n",
    "different functions. At the same time, most of them can have sensible defaults.\n",
    "\n",
    "Rather than adding the parameters one by one to every function that might need them,\n",
    "I'm going to create a dataclass that we can pass around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "419eb029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig(TypedDict):\n",
    "    train_percent: float    # If the training corpus is text, the percentage to use for training. The rest is used for validation.\n",
    "    max_length: int         # the maximum length of a given training batch\n",
    "    stride: int             # the distance between starting points of training batches\n",
    "    epochs: int             # the number of times to train on one set of data\n",
    "    initial_lr: float       # the initial learning rate used by the optimizer\n",
    "    peak_lr: float          # the highest learning rate to be used by the optimizer\n",
    "    weight_decay: float     # the weight decay used by the optimizer\n",
    "    warmup_steps: int|None  # the number of steps it should take to ramp up from initial_lr to peak_lr\n",
    "    temperature: float      # the temperature for token generation\n",
    "    topk: int               # the number of logits to select for top-k token generation\n",
    "    eval_freq: int          # evaluate the model every [this number] of steps\n",
    "\n",
    "def new_training_config(\n",
    "        train_percent:float=0.9,\n",
    "        max_length:int=1024,\n",
    "        stride:int|None=None,\n",
    "        epochs:int=1,\n",
    "        initial_lr:float=0.0001,\n",
    "        peak_lr:float=0.01,\n",
    "        weight_decay:float=0.1,\n",
    "        warmup_steps:int|None=None,\n",
    "        temperature:float=0.8,\n",
    "        topk:int=50,\n",
    "        eval_freq:int=0,\n",
    ") -> TrainingConfig:\n",
    "    if stride is None:\n",
    "        stride = max_length // 2\n",
    "    return TrainingConfig(\n",
    "        train_percent=train_percent,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        epochs=epochs,\n",
    "        initial_lr=initial_lr,\n",
    "        peak_lr=peak_lr,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_steps=warmup_steps,\n",
    "        temperature=temperature,\n",
    "        topk=topk,\n",
    "        eval_freq=eval_freq,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97717397",
   "metadata": {},
   "source": [
    "### Training on \"The Verdict\"\n",
    "\n",
    "Training on this short story is probably the last time we'll pass a text file in\n",
    "directly for training, so some of these functions will be short-lived.\n",
    "\n",
    "The most important function here is `train_simple_text`, which illustrates in\n",
    "the most minimal possible way how the training flow works. Most of that boils\n",
    "down to this one little stanza:\n",
    "\n",
    "```python\n",
    "    for epoch in range(cfg['epochs']):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in training_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = cross_entropy_loss_for_batch(model, input_batch=input_batch, target_batch=target_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "```\n",
    "\n",
    "To break that down in excruciating detail:\n",
    "1. each epoch gets an entire run through the data loader, so we have a top-level loop here.\n",
    "2. `model.train()` puts the model into training mode, where it preserves gradients for back propagation.\n",
    "3. then we loop throuhg input-target pairs from the training loader.\n",
    "4. `optimizer.zero_grad()` clears the optimizer state so that we don't contaminate this batch with the results from a previous batch.\n",
    "5. we calculate the loss next. It's common to think of the loss as a scalar score, but in this case it encapsulates the entire gradient.\n",
    "6. hence, we can call `loss.backward()` to backpropagate and update the model's weights.\n",
    "7. finally, `optimizer.step()` updates the optimizer's parameters for one step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b0c8b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the text if it's not yet available, then return it as a string\n",
    "def the_verdict() -> str:\n",
    "    \"\"\"Returns the text of the short story \\\"The Verdict\\\". Uses the local filesystem for caching.\"\"\"\n",
    "    file_path = Path(\"the-verdict.txt\")\n",
    "    if not file_path.exists():\n",
    "        url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode('utf-8')\n",
    "            with open(file_path, \"w\") as f:\n",
    "                f.write(text_data)\n",
    "            return text_data\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def text_training_loaders(text: str, cfg: TrainingConfig) -> tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"Turn the given text into two Dataloaders: one for training and one for validation.\"\"\"\n",
    "    split_idx = int(len(text) * cfg['train_percent'])\n",
    "    \n",
    "    # Use partials with the Dataset and Dataloader classes to declutter and enforce consistency\n",
    "    custom_dataset = partial(GPTDatasetV1, tokenizer=tokenizer, max_length=cfg['max_length'], stride=cfg['stride'])\n",
    "    custom_dataloader = partial(DataLoader, batch_size=4, shuffle=True, drop_last=True, num_workers=0)\n",
    "\n",
    "    # raw text portions\n",
    "    train_portion = text[:split_idx]\n",
    "    validation_portion = text[split_idx:]\n",
    "\n",
    "    # tokenized datasets\n",
    "    train_dataset = custom_dataset(train_portion)\n",
    "    validation_dataset = custom_dataset(validation_portion)\n",
    "\n",
    "    # completed dataloaders\n",
    "    train_loader = custom_dataloader(train_dataset)\n",
    "    validation_loader = custom_dataloader(validation_dataset)\n",
    "    return (train_loader, validation_loader)\n",
    "\n",
    "def cross_entropy_loss_for_batch(model: gpt.GPTModel, input_batch: torch.Tensor, target_batch: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Returns the model's loss for the given batch. The loss can be used to train the model.\"\"\"\n",
    "    device = model.device()\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    return nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten()) # TODO: explain why flatten\n",
    "\n",
    "def calc_loss_loader(model: gpt.GPTModel, data_loader: DataLoader, num_batches=None) -> float:\n",
    "    \"\"\"Calculates the model's total loss over a number of batches for the given\n",
    "    Dataloader. This helper is used for validation only.\"\"\"\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = cross_entropy_loss_for_batch(model, input_batch, target_batch)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def train_simple_text(model: gpt.GPTModel, text: str, cfg: TrainingConfig) -> float:\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), lr=cfg['peak_lr'], weight_decay=cfg['weight_decay']\n",
    "    )\n",
    "    training_loader, validation_loader = text_training_loaders(text, cfg)\n",
    "\n",
    "    for epoch in range(cfg['epochs']):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in training_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = cross_entropy_loss_for_batch(model, input_batch=input_batch, target_batch=target_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    if len(validation_loader) == 0:\n",
    "        raise ValueError(\"Ooops, no validation data\")\n",
    "    with torch.no_grad():\n",
    "        validation_loss = calc_loss_loader(model, validation_loader)\n",
    "        return validation_loss\n",
    "\n",
    "def train_verdict(model: gpt.GPTModel) -> float:\n",
    "    torch.manual_seed(123)\n",
    "    text = the_verdict()\n",
    "    verdict_training_config = new_training_config(train_percent=0.85, peak_lr=5e-4, max_length=256, epochs=10)\n",
    "    return train_simple_text(model=model, text=text, cfg=verdict_training_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d295cc9",
   "metadata": {},
   "source": [
    "### Test out the training loop\n",
    "\n",
    "Below you'll see the untrained output, the final training loss, and the trained output.\n",
    "\n",
    "The trained output looks _much_ better, but there is a little bit of a trick\n",
    "here. Training 10 epochs on such a small and homogenous dataset means that we're\n",
    "_massively_ overfitting. The model is basically just memorizing phrases from the text\n",
    "and spitting them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9038f09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text (untrained):\n",
      " He neverRPteness champagneceptionuffÙ… refres Rexrunnerchapter\n",
      "\n",
      "Validation loss after training: 6.562\n",
      "\n",
      "Output text (trained):\n",
      " He never had lingered to give a lump of sugar to\n"
     ]
    }
   ],
   "source": [
    "def trained_example(model: gpt.GPTModel, start_context):\n",
    "    torch.manual_seed(123)\n",
    "    model.eval()\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    token_ids = gpt.generate_text_simple(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(start_context, tokenizer),\n",
    "        max_new_tokens=10,\n",
    "        context_size=GPT_CONFIG_MINI[\"context_length\"],\n",
    "    )\n",
    "\n",
    "    print(\"Output text (trained):\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_context = \"He never\"\n",
    "    untrained_example(start_context=start_context)\n",
    "    model = gpt.GPTModel(GPT_CONFIG_MINI)\n",
    "    model.to(get_device())\n",
    "    val_loss = train_verdict(model)\n",
    "    print(f\"\\nValidation loss after training: {val_loss:.3f}\\n\")\n",
    "    trained_example(model, start_context=start_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af6d32",
   "metadata": {},
   "source": [
    "# Better text generation\n",
    "\n",
    "So far, we've been using `gpt.generate_text_simple`, which chooses the next token by just finding the logit with the highest score.\n",
    "\n",
    "That's good enough for illustration early in the process, but it honestly sucks. It results in super boring and repetitive output.\n",
    "\n",
    "Let's implement a more typical Top-K multinomial algorithm:\n",
    "1. Limit our choices to the top k logits, where k is a constant of our choosing. This prevents extremely unlikely tokens from _ever_ being generated.\n",
    "2. Divide the logit probabilities by `temperature`. A low temperature emphasizes logits with higher probabilities, a higher temperature creates chaos by emphasizing lower probabilities.\n",
    "3. Randomly choose from the remaining logits, weighted by their respective probabilities. Logits with higher probabilities will be chosen more often, but even unlikely ones will be chosen sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ddea66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "END_OF_TEXT = 50256\n",
    "\n",
    "def choose_from_topk(logits: torch.Tensor, topk: int, temperature: float) -> torch.Tensor:\n",
    "    top_logits, top_pos = torch.topk(logits, topk)\n",
    "    filtered = torch.full_like(\n",
    "        logits, -torch.inf\n",
    "    )\n",
    "    filtered.scatter_(dim=1, index=top_pos, src=top_logits) #huh?\n",
    "    scaled = filtered / temperature\n",
    "    probabilities = torch.softmax(scaled, dim=-1) # note: might have trouble with device\n",
    "    if torch.any(torch.isnan(probabilities)) or torch.any(probabilities < 0):\n",
    "        print(\"Bad probabilities:\", probabilities)\n",
    "        print(\"Logits:\", logits)\n",
    "        raise ValueError(\"NaNs or invalid values in probabilities\")\n",
    "    return torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "def generate_text_topk(model: gpt.GPTModel, token_ids: torch.Tensor, max_new_tokens: int, context_size: int, topk: int, temperature: float):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = token_ids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        idx_next = choose_from_topk(logits, topk, temperature)\n",
    "        if idx_next.item() == END_OF_TEXT:\n",
    "            break\n",
    "        token_ids = torch.cat((token_ids, idx_next), dim=1)\n",
    "    return token_ids\n",
    "\n",
    "def text_completion_topk(model, initial_context: str, max_new_tokens:int=10, context_size:int=256, topk:int=50, temperature:float=1.5):\n",
    "    device = model.device()\n",
    "    encoded = tokenizer.encode(initial_context)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    out = generate_text_topk(\n",
    "        model,\n",
    "        encoded_tensor,\n",
    "        context_size=context_size,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        topk=topk,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e33b0332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He never his lean sunburnt cheeks furrowed by\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_context = \"He never\"\n",
    "    model = gpt.GPTModel(GPT_CONFIG_MINI)\n",
    "    model.to(get_device())\n",
    "    train_verdict(model)\n",
    "    print(text_completion_topk(model, start_context, temperature=0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aa3594",
   "metadata": {},
   "source": [
    "# Saving and loading model state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e43a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model: gpt.GPTModel, name: str, overwrite:bool=False, optimizer: optim.Optimizer|None=None):\n",
    "    if len(name) == 0:\n",
    "        raise ValueError(\"name can't be empty\")\n",
    "    path = Path(f\"{name}.pth\")\n",
    "    if path.exists() and not overwrite:\n",
    "        raise FileExistsError(f\"{path} already exists and overwrite is set to False\")\n",
    "\n",
    "    optimizer_state = optimizer.state_dict() if optimizer is not None else None\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer_state,\n",
    "    }, path)\n",
    "\n",
    "def load(model: gpt.GPTModel, name: str, optimizer: optim.Optimizer|None=None):\n",
    "    if len(name) == 0:\n",
    "        raise ValueError(\"name can't be empty\")\n",
    "    path = Path(f\"{name}.pth\")\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"{path} does not exist\")\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer_state_dict = checkpoint[\"optimizer_state_dict\"]\n",
    "    if optimizer_state_dict is not None and optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cce48b01",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "test-verdict.pth already exists and overwrite is set to False",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileExistsError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m model.to(get_device())\n\u001b[32m      4\u001b[39m train_verdict(model) \u001b[38;5;66;03m# No access to optimizer at this point\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest-verdict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(model, name, overwrite, optimizer)\u001b[39m\n\u001b[32m      4\u001b[39m path = Path(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path.exists() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m overwrite:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m already exists and overwrite is set to False\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m optimizer_state = optimizer.state_dict() \u001b[38;5;28;01mif\u001b[39;00m optimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      9\u001b[39m torch.save({\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m\"\u001b[39m: model.state_dict(),\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moptimizer_state_dict\u001b[39m\u001b[33m\"\u001b[39m: optimizer_state,\n\u001b[32m     12\u001b[39m }, path)\n",
      "\u001b[31mFileExistsError\u001b[39m: test-verdict.pth already exists and overwrite is set to False"
     ]
    }
   ],
   "source": [
    "start_context = \"He never\"\n",
    "model = gpt.GPTModel(GPT_CONFIG_MINI)\n",
    "model.to(get_device())\n",
    "train_verdict(model) # No access to optimizer at this point\n",
    "save(model, \"test-verdict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09a6c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gpt.GPTModel(GPT_CONFIG_MINI)\n",
    "load(model, name=\"test-verdict\")\n",
    "print(text_completion_topk(model, start_context, temperature=0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad6bedd",
   "metadata": {},
   "source": [
    "## A more mature training loop\n",
    "\n",
    "A complete training function has to represent a set of somewhat orthogonal concerns:\n",
    "\n",
    "1. Training itself: calculating training loss and backpropagating.\n",
    "2. Setting the learning rate based on the current step, etc.\n",
    "3. Evaluating the current validation loss for the model.\n",
    "4. Reporting metrics (e.g., to MLflow or stdout), which may include generating an example completion.\n",
    "\n",
    "The `train_simple_text` function attempts to focus as much as possible on 1, barely touching the other points.\n",
    "Now that we're moving on, that's going to have to change. The problem is that we need some amount of extensibility\n",
    "and I don't think a gigantic configuration dict and a mess of conditionals is the way to go.\n",
    "\n",
    "So the question is this: how do we build an extensible and maintainable training loop?\n",
    "\n",
    "### 1. The core training loop\n",
    "\n",
    "For now, I don't see much reason to do this differently than `train_simple_text`. I think any significant differences\n",
    "will come from implementing the other requirements below.\n",
    "\n",
    "### 2. Setting the learning rate\n",
    "\n",
    "This is an area where I do need to build on what I did before, and it definitely needs to be configurable. However,\n",
    "I don't think I need to modularize this part. I'm pretty sure that a learning rate warmup with cosine decay will be\n",
    "good enough for all my planned use cases, so I'll implement that as a one-size-fits-all solution.\n",
    "\n",
    "I do wonder how this will fit in, though. Will it all be part of the same function? Controlled by a separate helper\n",
    "function? Encapsulated in a class? Unclear at this point, but hopefully not that last option.\n",
    "\n",
    "### 3. Evaluating the current validation loss for the model\n",
    "\n",
    "I think there are two real options here:\n",
    "- **a**. use some kind of modular/callback system to invert control around model evaluation (think event loop)\n",
    "- **b**. rely on a `validation_loader` parameter and configuration options\n",
    "\n",
    "Option **a** is probably needlessly complicated on its own, but there's an argument that we get it for \"free\" if we\n",
    "implement some kind of event-driven metrics system. I'll say more about that below.\n",
    "\n",
    "If option **b** is adequate, it's hard to argue that it's not the right way forward. I might need to review the classification\n",
    "code, but IIRC there's really no need to do anything differently for validation there; we're still just comparing the model's\n",
    "output to the target from the validation loader. If I'm wrong about that, then that weighs heavily in favor of a more modular\n",
    "system.\n",
    "\n",
    "### 4. Reporting metrics\n",
    "\n",
    "Here's where I really think a modular approach is appropriate. I like using MLflow, but it's not appropriate for every single\n",
    "training run. Sometimes I may want to log to stdout, send alerts via Pushover, capture logs in a buffer for testing purposes,\n",
    "or just disable metrics altogether.\n",
    "\n",
    "Creating a basic interface that is similar to the MLflow module probably makes the most sense. You'd be able to inject whatever\n",
    "implementation you want when you begin training and the training function itself wouldn't need to be aware of the multitude of\n",
    "different options.\n",
    "\n",
    "As I mentioned in sub-section 3, this does open up the possibility that the metrics handler could _also_ decide when and how\n",
    "to do a validation run, which would simplify the code in the training function and provide more flexibility. However, there's\n",
    "a good chance that every metrics handler implementation would end up doing basically the same thing for validation and then\n",
    "it's not a win at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9989ad1c",
   "metadata": {},
   "source": [
    "### 2. Configuring the learning rate\n",
    "\n",
    "I've decided to go with an _optionally_ configurable learning rate. The default\n",
    "is the linear warmup with cosine decay, and I expect that's going to be what's\n",
    "used probably every time. But if I ever want to experiment with a different type of scheduling, I can do\n",
    "that easily. \n",
    "\n",
    "This works by having an optional `Callable[[int], float]` parameter, `lr_schedule_fn`. This is a function of\n",
    "step number -> learning rate multiplier. Note: the function does _not_ return the new learning rate! It scales\n",
    "the base learning rate.\n",
    "\n",
    "The default implementation is returned by `cosine_decay_lr`. In order to set reasonable values for `warmup_steps`, etc,\n",
    "it needs to peek at the training loader. This is basically unavoidable, since the scheduler _only_ passes the step number\n",
    "and not anything else that would be useful, like the total number of steps or anything like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9968bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_decay_lr(cfg: TrainingConfig, training_loader: DataLoader) -> Callable[[int], float]:\n",
    "    \"\"\"Returns a LambdaLR function that closes over the cfg and implements a basic linear warmup with cosine decay.\"\"\"\n",
    "    total_steps = len(training_loader) * cfg['epochs']\n",
    "    warmup_steps = cfg['warmup_steps'] or int(0.2 * total_steps)\n",
    "    warmup_steps = max(warmup_steps, 1)\n",
    "    decay_steps = total_steps - warmup_steps\n",
    "    decay_steps = max(decay_steps, 1)\n",
    "    def lambda_lr(step: int) -> float:\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        else:\n",
    "            progress = (step - warmup_steps) / decay_steps\n",
    "            progress = min(progress, 1.0)\n",
    "            return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "    return lambda_lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4162764",
   "metadata": {},
   "source": [
    "### 4. Reporting metrics\n",
    "\n",
    "I really like using MLflow for visualizing and tracking metrics, but it's not\n",
    "something that I want to require for every training run. It would be great\n",
    "to have other metrics implementations that can be swapped in at will.\n",
    "\n",
    "So I'm going to create a `Metrics` abstract base class that will have the following implementations:\n",
    "1. MLflow, as a basic passthrough.\n",
    "2. stdout, the default since it's convenient and doesn't require a separate service.\n",
    "3. pushover (at some point in the future), for when I have longer running jobs in the cloud.\n",
    "\n",
    "The class below is modeled after the MLflow methods that I used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e9adb341",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        pass\n",
    "\n",
    "    def start_run(self):\n",
    "        return self\n",
    "\n",
    "    @abstractmethod\n",
    "    def log_param(self, name: str, val):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def log_metric(self, name: str, val, step: int):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def log_example(self, name: str, contents: str, step: int):\n",
    "        pass\n",
    "\n",
    "class StdoutMetrics(Metrics):\n",
    "    def __init__(self, print_interval: int=1):\n",
    "        self.print_interval = print_interval\n",
    "    \n",
    "    def log_param(self, name: str, val):\n",
    "        print(f\"Parameter: \\\"{name}\\\"={val}\")\n",
    "    \n",
    "    def log_metric(self, name: str, val, step: int):\n",
    "        if step % self.print_interval == 0:\n",
    "            print(f\"[{step}] Metric: \\\"{name}\\\"={val}\")\n",
    "    \n",
    "    def log_example(self, name: str, contents: str, step: int):\n",
    "        # I'm going to assume we always want to log these, regardless of the step\n",
    "        print(f\"[{step}] Example ({name}): \\\"{contents}\\\"\")\n",
    "\n",
    "class MLflowMetrics(Metrics):\n",
    "    def __init__(self, tracking_uri: str = \"http://localhost:5000\", artifact_dir: str = \"examples\"):\n",
    "        mlflow.set_tracking_uri(tracking_uri)\n",
    "        os.makedirs(artifact_dir, exist_ok=True)\n",
    "        self.artifact_dir = artifact_dir\n",
    "    \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.run.__exit__(exc_type, exc_val, exc_tb)\n",
    "    \n",
    "    def start_run(self):\n",
    "        if hasattr(self, \"run\"):\n",
    "            raise RuntimeError(\"MLflow run has already been started\")\n",
    "        self.run = mlflow.start_run()\n",
    "        return self\n",
    "    \n",
    "    def log_param(self, name: str, val):\n",
    "        mlflow.log_param(name, val)\n",
    "\n",
    "    def log_metric(self, name: str, val, step: int):\n",
    "        mlflow.log_metric(name, val, step = step)\n",
    "    \n",
    "    def log_example(self, name: str, contents: str, step: int):\n",
    "        mlflow.log_text(contents, artifact_file=f\"{self.artifact_dir}/{name}_{step}.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5294c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: gpt.GPTModel,\n",
    "        optimizer: optim.Optimizer,\n",
    "        training_loader: DataLoader,\n",
    "        validation_loader: DataLoader,\n",
    "        cfg:TrainingConfig,\n",
    "        lr_schedule_fn: Optional[Callable[[int], float]] = None,\n",
    "        metrics: Metrics = StdoutMetrics(print_interval=20)\n",
    "        ):\n",
    "    if lr_schedule_fn is None:\n",
    "        lr_schedule_fn = cosine_decay_lr(cfg, training_loader)\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_schedule_fn)\n",
    "\n",
    "    tokens_seen = 0\n",
    "    global_step = 0\n",
    "\n",
    "    with metrics.start_run():\n",
    "        metrics.log_param(\"training size\", len(training_loader))\n",
    "        metrics.log_param(\"epochs\", cfg['epochs'])\n",
    "\n",
    "        for epoch in range(cfg['epochs']):\n",
    "            model.train()\n",
    "            for input_batch, target_batch in training_loader:\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                loss = cross_entropy_loss_for_batch(model, input_batch=input_batch, target_batch=target_batch)\n",
    "                loss_val = loss.item()\n",
    "                loss.backward()\n",
    "                tokens_seen += input_batch.numel()\n",
    "\n",
    "                # TODO gradient clipping\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                metrics.log_metric(\"training loss\", loss_val, step=global_step)\n",
    "                metrics.log_metric(\"lr\", scheduler.get_last_lr()[0], step=global_step)\n",
    "                metrics.log_metric(\"tokens seen\", tokens_seen, step=global_step)\n",
    "                metrics.log_metric(\"epoch\", epoch, step=global_step)\n",
    "                # TODO validation loss/sample\n",
    "        # TODO end metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6eeb644f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: \"training size\"=8\n",
      "Parameter: \"epochs\"=10\n",
      "[0] Metric: \"lr\"=0.0\n",
      "[0] Metric: \"tokens seen\"=0\n",
      "[0] Metric: \"epoch\"=0\n",
      "[20] Metric: \"training loss\"=4.989264488220215\n",
      "[20] Metric: \"lr\"=0.00043196970259237353\n",
      "[20] Metric: \"tokens seen\"=20480\n",
      "[20] Metric: \"epoch\"=2\n",
      "[40] Metric: \"training loss\"=4.061412811279297\n",
      "[40] Metric: \"lr\"=0.0002549705469162675\n",
      "[40] Metric: \"tokens seen\"=40960\n",
      "[40] Metric: \"epoch\"=5\n",
      "[60] Metric: \"training loss\"=2.4781665802001953\n",
      "[60] Metric: \"lr\"=7.49894813576437e-05\n",
      "[60] Metric: \"tokens seen\"=61440\n",
      "[60] Metric: \"epoch\"=7\n",
      "[80] Metric: \"training loss\"=2.508859872817993\n"
     ]
    }
   ],
   "source": [
    "model = gpt.GPTModel(GPT_CONFIG_MINI)\n",
    "model.to(get_device())\n",
    "torch.manual_seed(123)\n",
    "text = the_verdict()\n",
    "verdict_training_config = new_training_config(train_percent=0.85, initial_lr=5e-4, peak_lr=5e-4, warmup_steps=1, weight_decay=0, max_length=256, epochs=10)\n",
    "training_loader, validation_loader = text_training_loaders(text, verdict_training_config)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), lr=verdict_training_config['peak_lr'], weight_decay=verdict_training_config['weight_decay']\n",
    ")\n",
    "\n",
    "train(model, optimizer, training_loader, validation_loader, verdict_training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60a9fd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He never he had been through, and in the picture for\n"
     ]
    }
   ],
   "source": [
    "print(text_completion_topk(model, \"He never\", temperature=0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb011898",
   "metadata": {},
   "source": [
    "# Training on Project Gutenberg\n",
    "\n",
    "The following few sections include the code necessary to:\n",
    "1. Preprocess the [deepmind/pg19](https://huggingface.co/datasets/deepmind/pg19) dataset and wrap it in a Dataset class that we can use for training.\n",
    "2. Sample some batches from the dataset to see what typical text looks like.\n",
    "3. Train a GPTModel with a context size of 512 on this corpus.\n",
    "\n",
    "If you want to do this, better get a big cup of coffee and about 60 gigs of\n",
    "space ready. Downloading the data from HuggingFace takes a while, then you have\n",
    "to expand it and then the LazyTokenDatasetPG19 class will create a cache containing\n",
    "tokenized versions of every text.\n",
    "\n",
    "After that, training takes about 6-8 hours to reach a plateau on my machine with an NVidia 3080."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad38c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "import textwrap\n",
    "\n",
    "class LazyTokenDatasetPG19(Dataset):\n",
    "    \"\"\"Preprocesses the dataset (assumed to be deepmind/pg19!) by creating a\n",
    "    directory './tokens' containing pre-tokenized versions of all books in the\n",
    "    dataset. This takes a while the first time you run it (maybe 20 minutes),\n",
    "    but after that it's just a few seconds.\n",
    "    \n",
    "    The initialized object is suitable for passing to Dataloader.\"\"\"\n",
    "    GUTENBERG_END_RE = re.compile(r\"(?i)end of (the )?project gutenberg.*\", re.DOTALL)\n",
    "    TOO_MANY_NEWLINES_RE = re.compile(r\"\\n{3,}\")\n",
    "    LEADING_NEWLINES_RE = re.compile(r\"^\\n+\")\n",
    "    \n",
    "    def __init__(self, context_len:int=256):\n",
    "        super().__init__()\n",
    "        self.context_len = context_len\n",
    "        self.preprocess()\n",
    "        self.file_paths = glob.glob(\"tokens/*.pt\")\n",
    "        self.samples: list[tuple[int, int]] = []\n",
    "        print(\"Loading data from tokens directory\")\n",
    "        for i, path in enumerate(self.file_paths):\n",
    "            length = torch.load(path, map_location=\"cpu\").shape[0]\n",
    "            for j in range(0, length - context_len, context_len):\n",
    "                self.samples.append((i, j))\n",
    "            if i % 5_000 == 0:\n",
    "                print(f\"Loaded up to book {i}...\")\n",
    "        print(\"Loading complete\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        book_idx, start = self.samples[idx]\n",
    "        tokens = torch.load(self.file_paths[book_idx], map_location=\"cpu\")\n",
    "        input_ids = tokens[start : start + self.context_len]\n",
    "        target_ids = tokens[start + 1 : start + self.context_len + 1]\n",
    "        return input_ids, target_ids\n",
    "\n",
    "    def preprocess(self):\n",
    "        ds = load_dataset(\"deepmind/pg19\", split=\"train\")\n",
    "        os.makedirs(\"tokens\", exist_ok=True)\n",
    "        existing_filepaths = glob.glob(\"tokens/book_*.pt\")\n",
    "        if len(existing_filepaths) >= (ds.num_rows - 5_000): # type: ignore[attr-defined]\n",
    "            print(\"Preprocessing not needed.\")\n",
    "            return\n",
    "        print(\"Preprocessing data to tokens directory.\")\n",
    "        tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        for i, book in enumerate(ds):\n",
    "            path = Path(f\"tokens/book_{i}.pt\")\n",
    "            if path.exists():\n",
    "                continue\n",
    "            text = self.clean_text(book['text'])\n",
    "            if len(text) < self.context_len + 1:\n",
    "                continue\n",
    "            tokens = tokenizer.encode(text)\n",
    "            torch.save(torch.tensor(tokens, dtype=torch.long), path)\n",
    "            if i % 1_000 == 0:\n",
    "                print(f\"Completed preprocessing book {i}\")\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        # Remove Gutenberg end matter\n",
    "        text = self.GUTENBERG_END_RE.split(text)[0]\n",
    "        # Remove leading newlines/whitespace\n",
    "        text = self.LEADING_NEWLINES_RE.sub(\"\", text)\n",
    "        # Collapse 3+ newlines into exactly 2 (paragraph break)\n",
    "        text = self.TOO_MANY_NEWLINES_RE.sub(\"\\n\\n\", text)\n",
    "        # Eliminate chapter:verse markings\n",
    "        text = re.sub(r'\\b\\d+:\\d+\\b', '', text)\n",
    "        # Unwrap lines in each paragraph, but preserve paragraphs\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        unwrapped_paragraphs = [re.sub(r\"\\n\", \" \", p) for p in paragraphs]\n",
    "        text = '\\n\\n'.join(unwrapped_paragraphs)\n",
    "        # don't allow multiple spaces in a row\n",
    "        text = re.sub(r\"[^\\S\\n]+\", \" \", text)\n",
    "        return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8493b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_loader(dataloader, n):\n",
    "    \"\"\"Given a DataLoader and a number of samples, prints batches from the DataLoader.\"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    i = n\n",
    "    for input_batch, target_batch in dataloader:\n",
    "        if i == 0:\n",
    "            break\n",
    "        i -= 1\n",
    "        text = tokenizer.decode(input_batch.tolist()[:64])\n",
    "        print(text)\n",
    "        print(\"----------------------------------------\")\n",
    "\n",
    "# Uncomment below to see what's in the sanitized pg19 dataset.\n",
    "# sample_loader(DataLoader(ltds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5112be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_MEDIUM: GPTConfigDict = {**GPT_CONFIG_124M, \"context_length\": 512} # 1024 is just too big to train locally\n",
    "ltds = None\n",
    "training_cfg: GPTTrainingConfig = {**DEFAULT_TRAINING_CONFIG, \"n_epochs\": 20, \"eval_frequency\": 500, \"name\": \"pg19_medium\"}\n",
    "model = GPTModel(GPT_CONFIG_MEDIUM, training_cfg)\n",
    "\n",
    "def train_pg19(name: str, force_refresh:bool=False):\n",
    "    walden = load_dataset(\"deepmind/pg19\", split='validation')[0]['text'[63:]] # type: ignore\n",
    "    if ltds == None or force_refresh: # type: ignore\n",
    "        ltds = LazyTokenDatasetPG19(context_len=512)\n",
    "    if force_refresh:\n",
    "        model.load(name)\n",
    "    training_loader = DataLoader(\n",
    "        ltds, # type: ignore\n",
    "        shuffle=True,\n",
    "        batch_size=4,\n",
    "    )\n",
    "    model.train_loader(training_loader=training_loader, evaluation_text=walden, evaluation_prompt=\"When I\", epochs=1) # type: ignore\n",
    "\n",
    "# Uncomment below to actually train the model. You won't get good results until you do.\n",
    "# model.load(\"pg19_medium\")\n",
    "# train_pg19(\"new_training_run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802d6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(model: GPTModel, txt:str, max_tokens=128, temperature=0.8):\n",
    "    result = model.prompt(txt, max_tokens=max_tokens, temperature=temperature)\n",
    "    print(textwrap.fill(result, width=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the model is trained, you should see some... interesting results from this.\n",
    "# Otherwise it'll just be gibberish.\n",
    "# prompt(model, \"Ere thrice the sun done salutation to the dawn,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ef755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
