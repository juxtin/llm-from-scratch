{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed1431c",
   "metadata": {},
   "source": [
    "# Training a smaller GPT-2\n",
    "\n",
    "Having created the GPT model in [gpt.ipynb](./gpt.ipynb), it's time to try training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63992f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import gpt # type: ignore\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import mlflow\n",
    "import os\n",
    "import urllib.request\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.cuda as cuda\n",
    "import torch.backends\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from typing import TypedDict, Optional\n",
    "from collections.abc import Callable\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    if cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps:0\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "\n",
    "def clear_cache():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66fa9aa",
   "metadata": {},
   "source": [
    "## The mini config\n",
    "\n",
    "While we're just setting things up and experimenting, it's better to set a lower context size. If we kept it at 1024, training would take up a lot more memory and a lot more time, slowing down development.\n",
    "\n",
    "`GPT_CONFIG_MINI` sets a smaller context size of 256, which is enough to see things start to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2416bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_MINI: gpt.GPTConfigDict = {**gpt.GPT_CONFIG_124M, \"context_length\": 256}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bf31ff",
   "metadata": {},
   "source": [
    "## Convenience Functions and Example\n",
    "\n",
    "We're adding a few functions to make it easier to interact with the model:\n",
    "- `text_to_token_ids`: represents the very first stage in running the model.\n",
    "- `token_ids_to_text`: represents the very last stage in running the model.\n",
    "\n",
    "Also, another quick example of how they work.\n",
    "\n",
    "Some things that are new:\n",
    "- `simplified_model.eval();` this disables the training functionality of the model, letting us run inference much faster. It will skip dropout and discard gradients. The semicolon just discards the return value, which seems pointless in this example.\n",
    "- `squeeze` and `unsqueeze`: These add or remove (respectively) an empty dimension to the outside of a tensor.\n",
    "  - squeeze: `[[x]]` -> `[x]` (usually to remove the batch dimension when batch_size=1)\n",
    "  - unsqueeze: `[x]` => `[[x]]` (to add a batch dimension where batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c38a43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text (untrained):\n",
      " Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "def text_to_token_ids(text: str, tokenizer: tiktoken.Encoding, device:torch.device=get_device()) -> torch.Tensor:\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor.to(device)\n",
    "\n",
    "def token_ids_to_text(token_ids: torch.Tensor, tokenizer: tiktoken.Encoding) -> str:\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "# example:\n",
    "def untrained_example(start_context:str=\"Every effort moves you\"):\n",
    "    torch.manual_seed(123)\n",
    "    model = gpt.GPTModel(GPT_CONFIG_MINI)\n",
    "    model.to(get_device())\n",
    "    model.eval(); # disables dropout and gradients\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    token_ids = gpt.generate_text_simple(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(start_context, tokenizer).to(get_device()),\n",
    "        max_new_tokens=10,\n",
    "        context_size=GPT_CONFIG_MINI[\"context_length\"],\n",
    "    )\n",
    "\n",
    "    print(\"Output text (untrained):\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    untrained_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf453bb",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "`Dataset` is a class that resembles an iterator, but specialized for passing to\n",
    "a Dataloader. Like an iterator, it exposes `__len__` and `__getitem__` methods.\n",
    "\n",
    "The specialization is that it returns input-target _pairs_ rather than single\n",
    "items. This is what makes it specifically useful for training.\n",
    "\n",
    "In this case, it also encapsulates the tokenization step and implements the\n",
    "length/stride logic, but that's not always the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed6fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text: str, tokenizer: tiktoken.Encoding, max_length: int, stride: int):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            start = i\n",
    "            end = start + max_length\n",
    "            input_chunk = token_ids[start:end]\n",
    "            target_chunk = token_ids[start+1:end+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f7ff51",
   "metadata": {},
   "source": [
    "## Minimal training example\n",
    "\n",
    "Initially I had written a big wrapper class that enclosed a `SimplifiedGPT` instance and encapsulated a ton of training logic.\n",
    "\n",
    "That approach had some advantages and disadvantages, but in the end I think it was a mistake. It resulted in highly complected code\n",
    "with more and more configuration and more and more conditionals, although it did save some amount of passing arguments around.\n",
    "\n",
    "Instead, I think it's preferable to mainly focus on functions that operate on the `GPTModel` class and/or its output logits.\n",
    "\n",
    "To start, I'm going to focus on the bare minimum to train a model on [The Verdict](https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt). That means I need to be able to do the following:\n",
    "\n",
    "1. download the text of The Verdict and save it locally.\n",
    "2. split text into training and validation portions, then return a `training_loader` and `validation_loader`.\n",
    "3. measure a model's loss given a batch (i.e., a set of input-output pairs).\n",
    "4. implement the minimal training flow using the above and a crucial call to `loss.backward()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc718b4",
   "metadata": {},
   "source": [
    "### Training config\n",
    "\n",
    "Training involves a lot of parameters, and they need to be referenced by a few\n",
    "different functions. At the same time, most of them can have sensible defaults.\n",
    "\n",
    "Rather than adding the parameters one by one to every function that might need them,\n",
    "I'm going to create a dataclass that we can pass around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419eb029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig(TypedDict):\n",
    "    train_percent: float                   # If the training corpus is text, the percentage to use for training. The rest is used for validation.\n",
    "    max_length: int                        # the maximum length of a given training batch\n",
    "    stride: int                            # the distance between starting points of training batches\n",
    "    epochs: int                            # the number of times to train on one set of data\n",
    "    initial_lr: float                      # the initial learning rate used by the optimizer\n",
    "    peak_lr: float                         # the highest learning rate to be used by the optimizer\n",
    "    weight_decay: float                    # the weight decay used by the optimizer\n",
    "    gradient_clipping: bool                # whether to force gradient norms down to after every step 1.0\n",
    "    temperature: float                     # the temperature for token generation\n",
    "    topk: int                              # the number of logits to select for top-k token generation\n",
    "    eval_freq: int                         # evaluate the model every [this number] of steps\n",
    "    max_validation_batches: Optional[int]  # the maximum number of batches to use for validation\n",
    "    classification: bool                   # allows the model to run in classification mode. If false, assumes completion mode.\n",
    "\n",
    "def new_training_config(\n",
    "        train_percent:float=0.9,\n",
    "        max_length:int=1024,\n",
    "        stride:Optional[int]=None,\n",
    "        epochs:int=1,\n",
    "        initial_lr:float=0.0001,\n",
    "        peak_lr:float=0.01,\n",
    "        weight_decay:float=0.1,\n",
    "        gradient_clipping:bool=False,\n",
    "        temperature:float=0.8,\n",
    "        topk:int=50,\n",
    "        eval_freq:int=0,\n",
    "        max_validation_batches:Optional[int]=None,\n",
    "        classification:bool=False,\n",
    ") -> TrainingConfig:\n",
    "    if stride is None:\n",
    "        stride = max_length // 2\n",
    "    return TrainingConfig(\n",
    "        train_percent=train_percent,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        epochs=epochs,\n",
    "        initial_lr=initial_lr,\n",
    "        peak_lr=peak_lr,\n",
    "        weight_decay=weight_decay,\n",
    "        gradient_clipping=gradient_clipping,\n",
    "        temperature=temperature,\n",
    "        topk=topk,\n",
    "        eval_freq=eval_freq,\n",
    "        max_validation_batches=max_validation_batches,\n",
    "        classification=classification,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97717397",
   "metadata": {},
   "source": [
    "### Training on \"The Verdict\"\n",
    "\n",
    "Training on this short story is probably the last time we'll pass a text file in\n",
    "directly for training, so some of these functions will be short-lived.\n",
    "\n",
    "The most important function here is `train_simple_text`, which illustrates in\n",
    "the most minimal possible way how the training flow works. Most of that boils\n",
    "down to this one little stanza:\n",
    "\n",
    "```python\n",
    "    for epoch in range(cfg['epochs']):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in training_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = cross_entropy_loss_for_batch(model, input_batch=input_batch, target_batch=target_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "```\n",
    "\n",
    "To break that down in excruciating detail:\n",
    "1. each epoch gets an entire run through the data loader, so we have a top-level loop here.\n",
    "2. `model.train()` puts the model into training mode, where it preserves gradients for back propagation.\n",
    "3. then we loop throuhg input-target pairs from the training loader.\n",
    "4. `optimizer.zero_grad()` clears the optimizer state so that we don't contaminate this batch with the results from a previous batch.\n",
    "5. we calculate the loss next. It's common to think of the loss as a scalar score, but in this case it encapsulates the entire gradient.\n",
    "6. hence, we can call `loss.backward()` to backpropagate and update the model's weights.\n",
    "7. finally, `optimizer.step()` updates the optimizer's parameters for one step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c8b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the text if it's not yet available, then return it as a string\n",
    "def the_verdict() -> str:\n",
    "    \"\"\"Returns the text of the short story \\\"The Verdict\\\". Uses the local filesystem for caching.\"\"\"\n",
    "    file_path = Path(\"the-verdict.txt\")\n",
    "    if not file_path.exists():\n",
    "        url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode('utf-8')\n",
    "            with open(file_path, \"w\") as f:\n",
    "                f.write(text_data)\n",
    "            return text_data\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def text_training_loaders(text: str, cfg: TrainingConfig) -> tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"Turn the given text into two Dataloaders: one for training and one for validation.\"\"\"\n",
    "    split_idx = int(len(text) * cfg['train_percent'])\n",
    "    \n",
    "    # Use partials with the Dataset and Dataloader classes to declutter and enforce consistency\n",
    "    custom_dataset = partial(GPTDatasetV1, tokenizer=tokenizer, max_length=cfg['max_length'], stride=cfg['stride'])\n",
    "    custom_dataloader = partial(DataLoader, batch_size=4, shuffle=True, drop_last=True, num_workers=0)\n",
    "\n",
    "    # raw text portions\n",
    "    train_portion = text[:split_idx]\n",
    "    validation_portion = text[split_idx:]\n",
    "\n",
    "    # tokenized datasets\n",
    "    train_dataset = custom_dataset(train_portion)\n",
    "    validation_dataset = custom_dataset(validation_portion)\n",
    "\n",
    "    # completed dataloaders\n",
    "    train_loader = custom_dataloader(train_dataset)\n",
    "    validation_loader = custom_dataloader(validation_dataset)\n",
    "    return (train_loader, validation_loader)\n",
    "\n",
    "def cross_entropy_loss_for_batch(model: gpt.GPTModel, input_batch: torch.Tensor, target_batch: torch.Tensor, classification: bool = False) -> torch.Tensor:\n",
    "    \"\"\"Returns the model's loss for the given batch. The loss can be used to train the model.\n",
    "    Supports classification and completion modes. Usually, you want completion (classification=False).\"\"\"\n",
    "    device = model.device()\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    if classification:\n",
    "        logits = logits[:, -1, :]\n",
    "        return nn.functional.cross_entropy(logits, target_batch)\n",
    "    else:\n",
    "        return nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten()) # TODO: explain why flatten\n",
    "\n",
    "def calc_loss_loader(model: gpt.GPTModel, data_loader: DataLoader, num_batches=None, classification: bool = False) -> float:\n",
    "    \"\"\"Calculates the model's total loss over a number of batches for the given\n",
    "    Dataloader. This helper is used for validation only.\"\"\"\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = cross_entropy_loss_for_batch(model, input_batch, target_batch, classification=classification)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def train_simple_text(model: gpt.GPTModel, text: str, cfg: TrainingConfig) -> float:\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), lr=cfg['peak_lr'], weight_decay=cfg['weight_decay']\n",
    "    )\n",
    "    training_loader, validation_loader = text_training_loaders(text, cfg)\n",
    "\n",
    "    for epoch in range(cfg['epochs']):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in training_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = cross_entropy_loss_for_batch(model, input_batch=input_batch, target_batch=target_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    if len(validation_loader) == 0:\n",
    "        raise ValueError(\"Ooops, no validation data\")\n",
    "    with torch.no_grad():\n",
    "        validation_loss = calc_loss_loader(model, validation_loader)\n",
    "        return validation_loss\n",
    "\n",
    "def train_verdict(model: gpt.GPTModel) -> float:\n",
    "    torch.manual_seed(123)\n",
    "    text = the_verdict()\n",
    "    verdict_training_config = new_training_config(train_percent=0.85, peak_lr=5e-4, max_length=256, epochs=10)\n",
    "    return train_simple_text(model=model, text=text, cfg=verdict_training_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d295cc9",
   "metadata": {},
   "source": [
    "### Test out the training loop\n",
    "\n",
    "Below you'll see the untrained output, the final training loss, and the trained output.\n",
    "\n",
    "The trained output looks _much_ better, but there is a little bit of a trick\n",
    "here. Training 10 epochs on such a small and homogenous dataset means that we're\n",
    "_massively_ overfitting. The model is basically just memorizing phrases from the text\n",
    "and spitting them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9038f09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text (untrained):\n",
      " He neverRPteness champagneceptionuffÙ… refres Rexrunnerchapter\n",
      "\n",
      "Validation loss after training: 6.562\n",
      "\n",
      "Output text (trained):\n",
      " He never had lingered to give a lump of sugar to\n"
     ]
    }
   ],
   "source": [
    "def trained_example(model: gpt.GPTModel, start_context):\n",
    "    torch.manual_seed(123)\n",
    "    model.eval()\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    token_ids = gpt.generate_text_simple(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(start_context, tokenizer),\n",
    "        max_new_tokens=10,\n",
    "        context_size=GPT_CONFIG_MINI[\"context_length\"],\n",
    "    )\n",
    "\n",
    "    print(\"Output text (trained):\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_context = \"He never\"\n",
    "    untrained_example(start_context=start_context)\n",
    "    model = gpt.GPTModel(GPT_CONFIG_MINI)\n",
    "    model.to(get_device())\n",
    "    val_loss = train_verdict(model)\n",
    "    print(f\"\\nValidation loss after training: {val_loss:.3f}\\n\")\n",
    "    trained_example(model, start_context=start_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af6d32",
   "metadata": {},
   "source": [
    "# Better text generation\n",
    "\n",
    "So far, we've been using `gpt.generate_text_simple`, which chooses the next token by just finding the logit with the highest score.\n",
    "\n",
    "That's good enough for illustration early in the process, but it honestly sucks. It results in super boring and repetitive output.\n",
    "\n",
    "Let's implement a more typical Top-K multinomial algorithm:\n",
    "1. Limit our choices to the top k logits, where k is a constant of our choosing. This prevents extremely unlikely tokens from _ever_ being generated.\n",
    "2. Divide the logit probabilities by `temperature`. A low temperature emphasizes logits with higher probabilities, a higher temperature creates chaos by emphasizing lower probabilities.\n",
    "3. Randomly choose from the remaining logits, weighted by their respective probabilities. Logits with higher probabilities will be chosen more often, but even unlikely ones will be chosen sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ddea66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "END_OF_TEXT = 50256\n",
    "\n",
    "def choose_from_topk(logits: torch.Tensor, topk: int, temperature: float) -> torch.Tensor:\n",
    "    top_logits, top_pos = torch.topk(logits, topk)\n",
    "    filtered = torch.full_like(\n",
    "        logits, -torch.inf\n",
    "    )\n",
    "    filtered.scatter_(dim=1, index=top_pos, src=top_logits) #huh?\n",
    "    scaled = filtered / temperature\n",
    "    probabilities = torch.softmax(scaled, dim=-1) # note: might have trouble with device\n",
    "    if torch.any(torch.isnan(probabilities)) or torch.any(probabilities < 0):\n",
    "        print(\"Bad probabilities:\", probabilities)\n",
    "        print(\"Logits:\", logits)\n",
    "        raise ValueError(\"NaNs or invalid values in probabilities\")\n",
    "    return torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "def generate_text_topk(model: gpt.GPTModel, token_ids: torch.Tensor, max_new_tokens: int, context_size: int, topk: int, temperature: float):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = token_ids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        idx_next = choose_from_topk(logits, topk, temperature)\n",
    "        if idx_next.item() == END_OF_TEXT:\n",
    "            break\n",
    "        token_ids = torch.cat((token_ids, idx_next), dim=1)\n",
    "    return token_ids\n",
    "\n",
    "def text_completion_topk(model, initial_context: str, max_new_tokens:int=10, context_size:int=256, topk:int=50, temperature:float=1.5):\n",
    "    device = model.device()\n",
    "    encoded = tokenizer.encode(initial_context)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    out = generate_text_topk(\n",
    "        model,\n",
    "        encoded_tensor,\n",
    "        context_size=context_size,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        topk=topk,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e33b0332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He never his lean sunburnt cheeks furrowed by\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_context = \"He never\"\n",
    "    model = gpt.GPTModel(GPT_CONFIG_MINI)\n",
    "    model.to(get_device())\n",
    "    train_verdict(model)\n",
    "    print(text_completion_topk(model, start_context, temperature=0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aa3594",
   "metadata": {},
   "source": [
    "# Saving and loading model state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model: gpt.GPTModel, optimizer: Optional[optim.Optimizer], name: str, overwrite:bool=False, base_path: str = \".\"):\n",
    "    if len(name) == 0:\n",
    "        raise ValueError(\"name can't be empty\")\n",
    "    path = Path(f\"{base_path}/{name}.pth\")\n",
    "    if path.exists() and not overwrite:\n",
    "        raise FileExistsError(f\"{path} already exists and overwrite is set to False\")\n",
    "\n",
    "    optimizer_state = optimizer.state_dict() if optimizer is not None else None\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer_state,\n",
    "    }, path)\n",
    "\n",
    "def load(model: gpt.GPTModel, optimizer: Optional[optim.Optimizer], name: str, base_path: str = \".\", device: Optional[torch.device] = None):\n",
    "    if len(name) == 0:\n",
    "        raise ValueError(\"name can't be empty\")\n",
    "    path = Path(f\"{base_path}/{name}.pth\")\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"{path} does not exist\")\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer_state_dict = checkpoint[\"optimizer_state_dict\"]\n",
    "    if optimizer_state_dict is not None and optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d18822",
   "metadata": {},
   "source": [
    "## Default optimizer\n",
    "\n",
    "An unfortunate aspect of the current architecture is that callers have to manage\n",
    "their own optimizers to some extent.\n",
    "\n",
    "This helper takes on a little bit of that burden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3a5d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_optimizer(model, cfg: TrainingConfig):\n",
    "    return optim.AdamW(\n",
    "        model.parameters(), lr=cfg['peak_lr'], weight_decay=cfg['weight_decay']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad6bedd",
   "metadata": {},
   "source": [
    "## A more mature training loop\n",
    "\n",
    "A complete training function has to represent a set of somewhat orthogonal concerns:\n",
    "\n",
    "1. Configuring the learning rate\n",
    "2. Reporting metrics\n",
    "3. Generating samples\n",
    "4. The training loop itself\n",
    "\n",
    "The `train_simple_text` function attempts to focus as much as possible on 4, barely touching the other points.\n",
    "Now that we're moving on, that's going to have to change. The problem is that we need some amount of extensibility\n",
    "and I don't think a gigantic configuration dict and a mess of conditionals is the way to go.\n",
    "\n",
    "What follows are my attempts to make 1-3 configurable while keeping the training function as light as reasonably possible\n",
    "and providing good defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9989ad1c",
   "metadata": {},
   "source": [
    "### 1. Configuring the learning rate\n",
    "\n",
    "I've decided to go with an _optionally_ configurable learning rate. The default\n",
    "is the linear warmup with cosine decay, and I expect that's going to be what's\n",
    "used probably every time. But if I ever want to experiment with a different type of scheduling, I can do\n",
    "that easily. \n",
    "\n",
    "The extension is responsible for returning two things in a tuple:\n",
    "1. A `LearningRateFunction`, which takes the step number and returns a learning\n",
    "   rate multiplier. Note: the function does _not_ return the new learning rate! It\n",
    "   scales the base learning rate.\n",
    "2. The number of warmup steps. This used to be individually configurable, but now it's\n",
    "   owned entirely by the learning rate scheduling abstraction. This number is used in the\n",
    "   training loop to make sure we don't do gradient clipping during warmup.\n",
    "\n",
    "The default implementation is returned by `cosine_decay_lr`. In order to set reasonable values for `warmup_steps`, etc,\n",
    "it needs to peek at the training loader. This is basically unavoidable, since the scheduler _only_ passes the step number\n",
    "and not anything else that would be useful, like the total number of steps or anything like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9968bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LearningRateFunction = Callable[[int], float]\n",
    "\n",
    "def cosine_decay_lr(cfg: TrainingConfig, training_loader: DataLoader) -> tuple[LearningRateFunction, int]:\n",
    "    \"\"\"Returns a LambdaLR function that closes over the cfg and implements a basic linear warmup with cosine decay.\n",
    "    The number of warmup steps is also returned.\"\"\"\n",
    "    total_steps = len(training_loader) * cfg['epochs']\n",
    "    warmup_steps = int(0.1 * total_steps)\n",
    "    warmup_steps = max(warmup_steps, 1)\n",
    "    decay_steps = total_steps - warmup_steps\n",
    "    decay_steps = max(decay_steps, 1)\n",
    "    def lambda_lr(step: int) -> float:\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        else:\n",
    "            progress = (step - warmup_steps) / decay_steps\n",
    "            progress = min(progress, 1.0)\n",
    "            return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "    return (lambda_lr, warmup_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4162764",
   "metadata": {},
   "source": [
    "### 2. Reporting metrics\n",
    "\n",
    "I really like using MLflow for visualizing and tracking metrics, but it's not\n",
    "something that I want to require for every training run. It would be great\n",
    "to have other metrics implementations that can be swapped in at will.\n",
    "\n",
    "So I'm going to create a `Metrics` abstract base class that will have the following implementations:\n",
    "1. MLflow, as a basic passthrough.\n",
    "2. stdout, the default since it's convenient and doesn't require a separate service.\n",
    "3. pushover (at some point in the future), for when I have longer running jobs in the cloud.\n",
    "\n",
    "The class below is modeled after the MLflow methods that I used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9adb341",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        pass\n",
    "\n",
    "    def start_run(self):\n",
    "        return self\n",
    "    \n",
    "    def error(self, msg: str):\n",
    "        pass\n",
    "\n",
    "    def finish(self, msg: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def log_param(self, name: str, val):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def log_metric(self, name: str, val, step: int):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def log_example(self, name: str, contents: str, step: int):\n",
    "        pass\n",
    "\n",
    "class StdoutMetrics(Metrics):\n",
    "    def __init__(self, print_interval: int=1):\n",
    "        self.print_interval = print_interval\n",
    "    \n",
    "    def error(self, msg: str):\n",
    "        print(f\"Training finished with error: {msg}\")\n",
    "    \n",
    "    def finish(self, msg: str):\n",
    "        print(f\"Training finished successfully: {msg}\")\n",
    "    \n",
    "    def log_param(self, name: str, val):\n",
    "        print(f\"Parameter: \\\"{name}\\\"={val}\")\n",
    "    \n",
    "    def log_metric(self, name: str, val, step: int):\n",
    "        if step % self.print_interval == 0:\n",
    "            print(f\"[{step}] Metric: \\\"{name}\\\"={val}\")\n",
    "    \n",
    "    def log_example(self, name: str, contents: str, step: int):\n",
    "        # I'm going to assume we always want to log these, regardless of the step\n",
    "        print(f\"[{step}] Example ({name}): \\\"{contents}\\\"\")\n",
    "\n",
    "class MLflowMetrics(Metrics):\n",
    "    def __init__(self, tracking_uri: str = \"http://localhost:5000\", artifact_dir: str = \"examples\"):\n",
    "        mlflow.set_tracking_uri(tracking_uri)\n",
    "        os.makedirs(artifact_dir, exist_ok=True)\n",
    "        self.artifact_dir = artifact_dir\n",
    "    \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.run.__exit__(exc_type, exc_val, exc_tb)\n",
    "    \n",
    "    def start_run(self):\n",
    "        if hasattr(self, \"run\"):\n",
    "            raise RuntimeError(\"MLflow run has already been started\")\n",
    "        self.run = mlflow.start_run()\n",
    "        return self\n",
    "    \n",
    "    def log_param(self, name: str, val):\n",
    "        mlflow.log_param(name, val)\n",
    "\n",
    "    def log_metric(self, name: str, val, step: int):\n",
    "        mlflow.log_metric(name, val, step = step)\n",
    "    \n",
    "    def log_example(self, name: str, contents: str, step: int):\n",
    "        mlflow.log_text(contents, artifact_file=f\"{self.artifact_dir}/{name}_{step}.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805147d1",
   "metadata": {},
   "source": [
    "### 3. Generating samples\n",
    "\n",
    "My earlier training method took a `sample_prompt` parameter and used that to\n",
    "generate a completion to add to the logs. That works as long as your focused on\n",
    "text completion, but it's not really adequate in general. Just in this project,\n",
    "I've done three types of generation:\n",
    "\n",
    "1. Text completion, as I just described\n",
    "2. Classification, where the output is lower dimensional and not based on tokens\n",
    "3. Instruction responses, where it's best to strip the full instruction preamble and only show the generated output\n",
    "\n",
    "I had written three different training methods for those three scenarios, and that always bugged me. Instead, I will\n",
    "write another abstract base class that can provide a way to satisfy those three use cases (and others), hopefully\n",
    "with minimal extra code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faa58f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleGenerator(ABC):\n",
    "    @abstractmethod\n",
    "    def generate(self, model: gpt.GPTModel) -> str:\n",
    "        pass\n",
    "\n",
    "class SimpleCompletion(ExampleGenerator):\n",
    "    def __init__(\n",
    "            self,\n",
    "            prompt: str = \"It is good\",\n",
    "            tokenizer: tiktoken.Encoding = tiktoken.get_encoding(\"gpt2\"),\n",
    "            max_new_tokens: int = 10,\n",
    "            context_size: int = 128,\n",
    "            topk: int = 50,\n",
    "            temperature: float = 0.8,\n",
    "        ):\n",
    "        self.prompt = prompt\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.context_size = context_size\n",
    "        self.topk = topk\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def generate(self, model: gpt.GPTModel) -> str:\n",
    "        return text_completion_topk(\n",
    "            model,\n",
    "            self.prompt,\n",
    "            max_new_tokens=self.max_new_tokens,\n",
    "            context_size=self.context_size,\n",
    "            topk=self.topk,\n",
    "            temperature=self.temperature\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6fdf3f",
   "metadata": {},
   "source": [
    "### 4. The training loop itself\n",
    "\n",
    "This section ties it all together. The core training code is just a few lines,\n",
    "so most of this is actually taken up by looping, metrics, and error\n",
    "handling.\n",
    "\n",
    "For now, the periodic evaluation logic relies on some basic configuration. There's\n",
    "no option yet to do something like generating and logging an example completion,\n",
    "although I'll probably add that soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5294c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: gpt.GPTModel,\n",
    "        optimizer: optim.Optimizer,\n",
    "        training_loader: DataLoader,\n",
    "        validation_loader: Optional[DataLoader],\n",
    "        cfg: TrainingConfig,\n",
    "        lr_schedule_fn: Optional[LearningRateFunction] = None,\n",
    "        metrics: Metrics = StdoutMetrics(print_interval=20),\n",
    "        example_generator: ExampleGenerator = SimpleCompletion(),\n",
    "        ):\n",
    "    warmup_steps = 1\n",
    "    if lr_schedule_fn is None:\n",
    "        lr_schedule_fn, warmup_steps = cosine_decay_lr(cfg, training_loader)\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_schedule_fn)\n",
    "\n",
    "    tokens_seen = 0\n",
    "    global_step = 0\n",
    "    total_steps = cfg['epochs'] * len(training_loader)\n",
    "    loss_val = 0.0\n",
    "\n",
    "    try:\n",
    "        with metrics.start_run():\n",
    "            # Initial metrics (job-level params)\n",
    "            metrics.log_param(\"epoch size\", len(training_loader))\n",
    "            metrics.log_param(\"epochs\", cfg['epochs'])\n",
    "            metrics.log_param(\"total training size\", len(training_loader) * cfg['epochs'])\n",
    "            metrics.log_param(\"validation size\", len(validation_loader or []))\n",
    "            metrics.log_param(\"gradient clipping\", cfg['gradient_clipping'])\n",
    "\n",
    "            for epoch in range(cfg['epochs']):\n",
    "                clear_cache()\n",
    "                for input_batch, target_batch in training_loader:\n",
    "                    model.train()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Actual training\n",
    "                    loss = cross_entropy_loss_for_batch(model, input_batch=input_batch, target_batch=target_batch, classification=cfg[\"classification\"])\n",
    "                    loss_val = loss.item()\n",
    "                    loss.backward()\n",
    "                    tokens_seen += input_batch.numel()\n",
    "                    global_step += 1\n",
    "\n",
    "                    # Gradient clipping if enabled and not warming up\n",
    "                    if cfg['gradient_clipping'] and global_step >= warmup_steps:\n",
    "                            nn.utils.clip_grad_norm_(\n",
    "                            model.parameters(), max_norm=1.0,\n",
    "                        )\n",
    "\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                    # Per-batch metrics\n",
    "                    metrics.log_metric(\"training loss\", loss_val, step=global_step)\n",
    "                    metrics.log_metric(\"learning rate\", scheduler.get_last_lr()[0], step=global_step)\n",
    "                    metrics.log_metric(\"tokens seen\", tokens_seen, step=global_step)\n",
    "                    metrics.log_metric(\"epoch\", epoch, step=global_step)\n",
    "                    metrics.log_metric(\"progress percent\", (global_step / total_steps) * 100, step=global_step)\n",
    "                    \n",
    "                    # Periodic evaluation\n",
    "                    if cfg['eval_freq'] > 0 and global_step % cfg['eval_freq'] == 0: # validation_loader not required\n",
    "                        clear_cache()\n",
    "                        model.eval()\n",
    "                        with torch.inference_mode():\n",
    "                            example = example_generator.generate(model)\n",
    "                            metrics.log_example(\"example\", example, step=global_step)\n",
    "                            if validation_loader is not None:\n",
    "                                validation_loss = calc_loss_loader(model, validation_loader, num_batches=cfg['max_validation_batches'], classification=cfg['classification'])\n",
    "                                metrics.log_metric(\"validation loss\", validation_loss, step=global_step)\n",
    "                        clear_cache()\n",
    "            # Final metrics\n",
    "            metrics.log_metric(\"tokens seen\", tokens_seen, step=global_step)\n",
    "            if validation_loader is not None:\n",
    "                clear_cache()\n",
    "                model.eval()\n",
    "                with torch.inference_mode():\n",
    "                    total_validation_loss = calc_loss_loader(model, validation_loader, classification=cfg['classification'])\n",
    "                    metrics.finish(f\"final validation loss {total_validation_loss:.3f}\")\n",
    "            else:\n",
    "                metrics.finish(f\"final training loss {loss_val:.3f}\")\n",
    "    # Outer catchall block\n",
    "    except Exception as error:\n",
    "        metrics.error(f\"{error}\")\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6eeb644f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: \"epoch size\"=8\n",
      "Parameter: \"epochs\"=10\n",
      "Parameter: \"total training size\"=80\n",
      "[20] Metric: \"training loss\"=5.888547420501709\n",
      "[20] Metric: \"lr\"=0.0009903926402016153\n",
      "[20] Metric: \"tokens seen\"=20480\n",
      "[20] Metric: \"epoch\"=2\n",
      "[20] Metric: \"validation loss\"=7.122949123382568\n",
      "[20] Example (example): \"It is good St haveI the, to The\n",
      ".\n",
      "\"\n",
      "[40] Metric: \"training loss\"=5.506793022155762\n",
      "[40] Metric: \"lr\"=0.000691341716182545\n",
      "[40] Metric: \"tokens seen\"=40960\n",
      "[40] Metric: \"epoch\"=4\n",
      "[40] Metric: \"validation loss\"=7.0354461669921875\n",
      "[40] Example (example): \"It is good to the room one.\"\"\" had, in\"\n",
      "[60] Metric: \"training loss\"=4.0062456130981445\n",
      "[60] Metric: \"lr\"=0.00022221488349019903\n",
      "[60] Metric: \"tokens seen\"=61440\n",
      "[60] Metric: \"epoch\"=7\n",
      "[60] Metric: \"validation loss\"=6.461377143859863\n",
      "[60] Example (example): \"It is good it for sun-hum that it was nothis\"\n",
      "[80] Metric: \"training loss\"=3.269244432449341\n",
      "[80] Metric: \"lr\"=0.0\n",
      "[80] Metric: \"tokens seen\"=81920\n",
      "[80] Metric: \"epoch\"=9\n",
      "[80] Metric: \"validation loss\"=6.57221794128418\n",
      "[80] Example (example): \"It is good? I said, so that on a little of\"\n",
      "[80] Metric: \"tokens seen\"=81920\n",
      "Training finished successfully: final validation loss 6.356\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = gpt.GPTModel(GPT_CONFIG_MINI)\n",
    "    model.to(get_device())\n",
    "    torch.manual_seed(123)\n",
    "    text = the_verdict()\n",
    "    verdict_training_config = new_training_config(train_percent=0.85, initial_lr=0.0001, peak_lr=0.001, weight_decay=0, max_length=256, epochs=10, eval_freq=20)\n",
    "    training_loader, validation_loader = text_training_loaders(text, verdict_training_config)\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), lr=verdict_training_config['peak_lr'], weight_decay=verdict_training_config['weight_decay']\n",
    "    )\n",
    "\n",
    "    train(model, optimizer, training_loader, validation_loader, verdict_training_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
