{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "624e282c",
   "metadata": {},
   "source": [
    "# DeepSeek\n",
    "\n",
    "In this notebook, I'll be experimenting with the architectural enhancements that characterize DeepSeek."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ddd69",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "The attention mechanism used by DeepSeek is Multi-Head Latent Attention (MLA), so I'll be working towards that.\n",
    "But rather than immediately jumping to MLA, I'll build up to it in steps:\n",
    "\n",
    "1. MHA with a KV cache\n",
    "2. Multi-Query Attention\n",
    "3. Grouped-Query Attention\n",
    "4. Multi-Head Latent Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4329529f",
   "metadata": {},
   "source": [
    "### KV Cache\n",
    "\n",
    "The idea behind the KV cache is that the autoregressive nature of LLM text\n",
    "generation results in a ton of redundant calculations.\n",
    "Since tokens are predicted one at a time, and only the final context vector in a\n",
    "sequence influences the prediction, we should only have to calculate one context\n",
    "vector at a time.\n",
    "\n",
    "In order to make that possible, we cache previously-calculated context vectors\n",
    "and recall them when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ae19b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "class KVCache(torch.Tensor):\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.w_key = None\n",
    "        self.w_val = None\n",
    "\n",
    "    def save_key(self, x: torch.Tensor, val: nn.Linear):\n",
    "        pass\n",
    "\n",
    "    def save_val(self, x: torch.Tensor, val: nn.Linear):\n",
    "        pass\n",
    "\n",
    "    def get_key(self, x: torch.Tensor) -> Optional[nn.Linear]:\n",
    "        return None\n",
    "\n",
    "    def get_val(self, x: torch.Tensor) -> Optional[nn.Linear]:\n",
    "        return None\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,  # embedding dimension\n",
    "        d_out: int, # embedding dimension\n",
    "        context_length: int,\n",
    "        dropout: float,\n",
    "        num_heads: int,\n",
    "        qkv_bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if d_out % num_heads != 0:\n",
    "            raise ValueError(\"The number of heads must evenly divide d_out.\")\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_width = d_out // num_heads\n",
    "        self.qkv_bias = qkv_bias\n",
    "\n",
    "        # construct the weights for Q, K, and V.\n",
    "        # these will be registered as trainable parameters automatically.\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # create and register a KV cache\n",
    "        self.kv_cache = KVCache()\n",
    "        self.register_buffer(\"kv_cache\", self.kv_cache)\n",
    "\n",
    "        # and the output projection, also trainable.\n",
    "        self.w_out = nn.Linear(d_out, d_out)\n",
    "\n",
    "        # and the dropout layer. not trainable, just drops random values\n",
    "        # to zero with a probability determined by the dropout parameter\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # and the mask, which prevents each token from \"seeing\" later ones\n",
    "        mask = torch.triu(  # an upper triangular matrix\n",
    "            torch.ones(context_length, context_length),  # consisting of ones\n",
    "            diagonal=1,  # starting one row above the diagonal, leaving the diagonal itself as zeroes.\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"mask\", mask\n",
    "        )  # register this tensor as non-trainable, but keep it on the same device\n",
    "        self.mask: torch.Tensor  # to make the type-checker happy\n",
    "\n",
    "    def forward_cached(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        prev_x = x[:, :-1, :]\n",
    "\n",
    "        keys = self.kv_cache.get_key(prev_x)\n",
    "        assert(keys is not None)\n",
    "        values = self.kv_cache.get_val(prev_x)\n",
    "        assert(values is not None)\n",
    "\n",
    "        queries = self.w_query(x)\n",
    "\n",
    "        pass\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch, num_tokens, d_in = x.shape\n",
    "        queries = self.w_query(x)\n",
    "        keys = self.w_key(x)\n",
    "        values = self.w_value(x)\n",
    "\n",
    "        # Split the last dimension of the tensors into multiple heads\n",
    "        q_heads = queries.view(batch, num_tokens, self.num_heads, self.head_width)\n",
    "        k_heads = keys.view(batch, num_tokens, self.num_heads, self.head_width)\n",
    "        v_heads = values.view(batch, num_tokens, self.num_heads, self.head_width)\n",
    "\n",
    "        #                                  [  0  ,     1     ,    2     ,      3    ]\n",
    "        # {q,k,v}_heads now have the shape [batch, num_tokens, num_heads, head_width],\n",
    "        # but we want them to be:          [batch, num_heads, num_tokens, head_width]\n",
    "        q_heads = q_heads.transpose(1, 2)\n",
    "        k_heads = k_heads.transpose(1, 2)\n",
    "        v_heads = v_heads.transpose(1, 2)\n",
    "\n",
    "        # now we need to calculate the raw dot-product attention scores between Q and K^T,\n",
    "        # where K^T has the shape [batch, num_heads, head_width, num_tokens].\n",
    "        # that gives attention_scores the shape [batch, num_heads, num_tokens, num_tokens]\n",
    "        attention_scores = q_heads @ k_heads.transpose(2, 3)\n",
    "        # and apply the causal mask\n",
    "        mask = self.mask[:num_tokens, :num_tokens]\n",
    "        attention_scores = attention_scores.masked_fill(mask == 1, float(\"-inf\"))\n",
    "\n",
    "        # and we construct the weights using softmax on the scaled final dimension\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / self.head_width**0.5, dim=-1\n",
    "        )\n",
    "        # and apply dropout\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        #                                 [  0  ,     1    ,     2     ,     3     ]\n",
    "        # attention_weights has the shape [batch, num_heads, num_tokens, num_tokens]\n",
    "        # v_heads has the shape:          [batch, num_heads, num_tokens, head_width]\n",
    "        # if we multiply them, we get:    [batch, num_heads, num_tokens, head_width]\n",
    "        # but in the end, we want:        [batch, num_tokens, d_out]\n",
    "        context = (\n",
    "            attention_weights @ v_heads\n",
    "        )  # [batch, num_heads, num_tokens, head_width]\n",
    "\n",
    "        # so we need to first transpose and get [batch, num_tokens, num_heads, head_width]\n",
    "        context = context.transpose(1, 2)\n",
    "        # and then concatenate the last two dimensions together to get d_out\n",
    "        context = context.contiguous().view(batch, num_tokens, self.d_out)\n",
    "        # and multiply by the output projection\n",
    "        return self.w_out(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34604ae9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6ea607",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_q = nn.Linear(768, 768, False)\n",
    "w_k = nn.Linear(768, 768, False)\n",
    "w_v = nn.Linear(768, 768, False)\n",
    "\n",
    "x0 = torch.randn(1, 68, 768).type(torch.float32) # the \"previous\" sequence\n",
    "new_tok = torch.randn(1, 768).unsqueeze(0) # the new row/token\n",
    "x1 = torch.cat([x0, new_tok], dim=1) # the new sequence\n",
    "\n",
    "q0 = w_q(x0)\n",
    "k0 = w_k(x0)\n",
    "v0 = w_v(x0)\n",
    "qkt0 = q0 @ k0.transpose(1,2)\n",
    "\n",
    "q1 = w_q(x1)\n",
    "k1 = w_k(x1)\n",
    "v1 = w_v(x1)\n",
    "qkt1 = q1 @ k1.transpose(1,2)\n",
    "\n",
    "cache = qkt0.clone()\n",
    "qkt1c = cache\n",
    "\n",
    "q_new = (q1[:, -1:, :] @ k1.transpose(1, 2))[:, :, :-1]\n",
    "print(f\"q_new shape: {q_new.shape}\")\n",
    "qkt1c = torch.cat([qkt1c, q_new], dim=1)\n",
    "print(f\"qkt1c shape: {qkt1c.shape}\")\n",
    "k_new = (q1 @ k1[:, -1:, :].transpose(1,2))\n",
    "print(f\"k_new shape: {k_new.shape}\")\n",
    "qkt1c = torch.cat([qkt1c, k_new], dim=2)\n",
    "print(f\"qkt1c shape: {qkt1c.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a9a29821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[235.0335,  -2.6566, -12.2506,  ..., -24.8320,  -2.8388,  15.2576],\n",
       "         [ -2.6566, 221.3925, -12.1649,  ...,   5.8514,  -6.2172,   1.1360],\n",
       "         [-12.2506, -12.1649, 301.8590,  ...,  18.5389,   5.1033,  -1.4170],\n",
       "         ...,\n",
       "         [-24.8320,   5.8514,  18.5389,  ..., 222.6481, -20.0580, -36.5833],\n",
       "         [ -2.8388,  -6.2172,   5.1033,  ..., -20.0580, 237.3867,   2.4074],\n",
       "         [ 15.2576,   1.1360,  -1.4170,  ..., -36.5833,   2.4074, 268.4286]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aaeedd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.7412e+00,  1.5995e+00, -3.0473e+00,  ..., -8.5874e+00,\n",
       "          -4.3166e+00,  1.5258e+01],\n",
       "         [ 1.0625e+01,  2.1516e+01, -8.1298e+00,  ..., -1.4035e+01,\n",
       "           7.6252e-01,  1.1360e+00],\n",
       "         [-8.6974e-01,  1.4532e+01,  1.0480e+00,  ..., -6.3004e-01,\n",
       "           2.4673e+01, -1.4170e+00],\n",
       "         ...,\n",
       "         [-7.2940e-02,  5.4469e+00, -3.5401e+00,  ...,  1.5306e+00,\n",
       "           3.0205e+00, -3.6583e+01],\n",
       "         [ 2.4017e+01,  1.1086e+01, -4.3359e+00,  ..., -1.0554e+01,\n",
       "          -1.7331e+00,  2.4074e+00],\n",
       "         [ 1.5258e+01,  1.1360e+00, -1.4170e+00,  ..., -3.6583e+01,\n",
       "           2.4074e+00,  2.6843e+02]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkt1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfffd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-from-scratch (3.11.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
