{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "624e282c",
   "metadata": {},
   "source": [
    "# DeepSeek\n",
    "\n",
    "In this notebook, I'll be experimenting with the architectural enhancements that characterize DeepSeek."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ddd69",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "The attention mechanism used by DeepSeek is Multi-Head Latent Attention (MLA), so I'll be working towards that.\n",
    "But rather than immediately jumping to MLA, I'll build up to it in steps:\n",
    "\n",
    "1. MHA with a KV cache\n",
    "2. Multi-Query Attention\n",
    "3. Grouped-Query Attention\n",
    "4. Multi-Head Latent Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4329529f",
   "metadata": {},
   "source": [
    "### KV Cache\n",
    "\n",
    "The idea behind the KV cache is that the autoregressive nature of LLM text\n",
    "generation results in a ton of redundant calculations.\n",
    "Since tokens are predicted one at a time, and only the final context vector in a\n",
    "sequence influences the prediction, we should only have to calculate one context\n",
    "vector at a time.\n",
    "\n",
    "In order to make that possible, we cache previously-calculated context vectors\n",
    "and recall them when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ae19b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "class KVCache():\n",
    "    def __init__(self):\n",
    "        self.x: Optional[torch.Tensor] = None\n",
    "        self.keys: Optional[torch.Tensor] = None\n",
    "        self.vals: Optional[torch.Tensor] = None\n",
    "\n",
    "    def cache_hit(self, x: torch.Tensor) -> bool:\n",
    "        \"\"\"As an extremely rough cut, just check that the x (param) looks kinda\n",
    "        like the previous x plus a new row\"\"\"\n",
    "        if self.x is None:\n",
    "            return False\n",
    "        _, cached_tokens, _ = self.x.shape\n",
    "        _, incoming_tokens, _ = x.shape\n",
    "        if cached_tokens != (incoming_tokens - 1):\n",
    "            # Rather than require the caller to manually reset the cache, I'll just reset it\n",
    "            # whenever the incoming data looks like it comes from a new sequence.\n",
    "            self.reset()\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def save_keys(self, x: torch.Tensor, val: torch.Tensor):\n",
    "        self.x = x\n",
    "        self.keys = val\n",
    "\n",
    "    def save_vals(self, x: torch.Tensor, val: torch.Tensor):\n",
    "        self.x = x\n",
    "        self.vals = val\n",
    "\n",
    "    def get_keys(self, x: torch.Tensor) -> nn.Linear:\n",
    "        assert(self.keys is not None)\n",
    "        return self.keys\n",
    "\n",
    "    def get_vals(self, x: torch.Tensor) -> nn.Linear:\n",
    "        assert(self.vals is not None)\n",
    "        return self.vals\n",
    "    \n",
    "    def reset(self):\n",
    "        self.__init__()\n",
    "\n",
    "class MultiHeadAttentionWithCache(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,  # embedding dimension\n",
    "        d_out: int, # embedding dimension\n",
    "        context_length: int,\n",
    "        dropout: float,\n",
    "        num_heads: int,\n",
    "        qkv_bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if d_out % num_heads != 0:\n",
    "            raise ValueError(\"The number of heads must evenly divide d_out.\")\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_width = d_out // num_heads\n",
    "        self.qkv_bias = qkv_bias\n",
    "\n",
    "        # construct the weights for Q, K, and V.\n",
    "        # these will be registered as trainable parameters automatically.\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # create a KV cache\n",
    "        self.kv_cache = KVCache()\n",
    "\n",
    "        # and the output projection, also trainable.\n",
    "        self.w_out = nn.Linear(d_out, d_out)\n",
    "\n",
    "        # and the dropout layer. not trainable, just drops random values\n",
    "        # to zero with a probability determined by the dropout parameter\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # and the mask, which prevents each token from \"seeing\" later ones\n",
    "        mask = torch.triu(  # an upper triangular matrix\n",
    "            torch.ones(context_length, context_length),  # consisting of ones\n",
    "            diagonal=1,  # starting one row above the diagonal, leaving the diagonal itself as zeroes.\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"mask\", mask\n",
    "        )  # register this tensor as non-trainable, but keep it on the same device\n",
    "        self.mask: torch.Tensor  # to make the type-checker happy\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch, num_tokens, d_in = x.shape\n",
    "        queries = self.w_query(x)\n",
    "\n",
    "        if self.kv_cache.cache_hit(x):\n",
    "            new_token: torch.Tensor = x[:, -1:, :]\n",
    "\n",
    "            keys = self.kv_cache.get_keys(x)\n",
    "            new_key_row: torch.Tensor = self.w_key(new_token)\n",
    "            keys = torch.cat([keys, new_key_row], dim=1)\n",
    "\n",
    "            values = self.kv_cache.get_vals(x)\n",
    "            new_val_row: torch.Tensor = self.w_value(new_token)\n",
    "            values = torch.cat([values, new_val_row], dim=1)\n",
    "        else:\n",
    "            keys = self.w_key(x)\n",
    "            values = self.w_value(x)\n",
    "        \n",
    "        self.kv_cache.save_keys(x, keys)\n",
    "        self.kv_cache.save_vals(x, values)\n",
    "\n",
    "        # Split the last dimension of the tensors into multiple heads\n",
    "        q_heads = queries.view(batch, num_tokens, self.num_heads, self.head_width)\n",
    "        k_heads = keys.view(batch, num_tokens, self.num_heads, self.head_width)\n",
    "        v_heads = values.view(batch, num_tokens, self.num_heads, self.head_width)\n",
    "\n",
    "        #                                  [  0  ,     1     ,    2     ,      3    ]\n",
    "        # {q,k,v}_heads now have the shape [batch, num_tokens, num_heads, head_width],\n",
    "        # but we want them to be:          [batch, num_heads, num_tokens, head_width]\n",
    "        q_heads = q_heads.transpose(1, 2)\n",
    "        k_heads = k_heads.transpose(1, 2)\n",
    "        v_heads = v_heads.transpose(1, 2)\n",
    "\n",
    "        # now we need to calculate the raw dot-product attention scores between Q and K^T,\n",
    "        # where K^T has the shape [batch, num_heads, head_width, num_tokens].\n",
    "        # that gives attention_scores the shape [batch, num_heads, num_tokens, num_tokens]\n",
    "        attention_scores = q_heads @ k_heads.transpose(2, 3)\n",
    "        # and apply the causal mask\n",
    "        mask = self.mask[:num_tokens, :num_tokens]\n",
    "        attention_scores = attention_scores.masked_fill(mask == 1, float(\"-inf\"))\n",
    "\n",
    "        # and we construct the weights using softmax on the scaled final dimension\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / self.head_width**0.5, dim=-1\n",
    "        )\n",
    "        # and apply dropout\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        #                                 [  0  ,     1    ,     2     ,     3     ]\n",
    "        # attention_weights has the shape [batch, num_heads, num_tokens, num_tokens]\n",
    "        # v_heads has the shape:          [batch, num_heads, num_tokens, head_width]\n",
    "        # if we multiply them, we get:    [batch, num_heads, num_tokens, head_width]\n",
    "        # but in the end, we want:        [batch, num_tokens, d_out]\n",
    "        context = (\n",
    "            attention_weights @ v_heads\n",
    "        )  # [batch, num_heads, num_tokens, head_width]\n",
    "\n",
    "        # so we need to first transpose and get [batch, num_tokens, num_heads, head_width]\n",
    "        context = context.transpose(1, 2)\n",
    "        # and then concatenate the last two dimensions together to get d_out\n",
    "        context = context.contiguous().view(batch, num_tokens, self.d_out)\n",
    "        # and multiply by the output projection\n",
    "        return self.w_out(context)\n",
    "\n",
    "    def reset(self):\n",
    "        self.kv_cache.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34604ae9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-from-scratch (3.11.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
