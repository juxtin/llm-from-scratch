{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f0be09",
   "metadata": {},
   "source": [
    "# Running on Apple's Neural Engines\n",
    "\n",
    "This notebook is an experiment in converting my Torch models into something that\n",
    "can run on the integrated Neural Engines on my Mac Mini.\n",
    "\n",
    "This process is pretty well documented, e.g., [here in the coremltools docs](https://coremltools.readme.io/v6.3/docs/pytorch-conversion), but maybe I can add a little context.\n",
    "\n",
    "You need to install coremltools for this:\n",
    "\n",
    "```\n",
    "uv pip install --group apple\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b5e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scikit-learn version 1.7.0 is not supported. Minimum required version: 0.17. Maximum required version: 1.5.1. Disabling scikit-learn conversion API.\n",
      "TensorFlow version 2.19.0 has not been tested with coremltools. You may run into unexpected errors. TensorFlow 2.12.0 is the most recent version that has been tested.\n",
      "Torch version 2.7.1 has not been tested with coremltools. You may run into unexpected errors. Torch 2.5.0 is the most recent version that has been tested.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import import_ipynb\n",
    "import training  # type: ignore\n",
    "import openai  # type: ignore\n",
    "import gpt  # type: ignore\n",
    "import chat  # type: ignore\n",
    "import coremltools as ct\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634d4f44",
   "metadata": {},
   "source": [
    "## Construct and load the model\n",
    "\n",
    "You should make sure `model` is an instance of the actual model you want to\n",
    "export. Make sure you know the context length, you'll need it in the next step.\n",
    "\n",
    "For now, I'll construct the model, load the weights, and do a quick test to make sure\n",
    "it works.\n",
    "\n",
    "(Note: I'm not distributing the `.pth` file for this one, so don't expect this to run on a fresh clone. You'll need\n",
    "your own model, which you can create in [chat.ipynb](./chat.ipynb).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f799086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was successful!\n"
     ]
    }
   ],
   "source": [
    "model = gpt.GPTModel(openai.GPT_CONFIG_355M)\n",
    "training.load(\n",
    "    model, optimizer=None, name=\"355M-alpaca\", base_path=\"../models\", device=\"cpu\"\n",
    ")\n",
    "model.to(gpt.get_device())\n",
    "chat.instruct(model, \"Tell the user that it was successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4980f3ed",
   "metadata": {},
   "source": [
    "## Construct a dummy input vector\n",
    "\n",
    "Here we need an input vector that we'll run through the model using `torch.jit.trace`. The vector should be the maximum dimensions that you want the model to support, so in this case $1\\times\\text{context\\_length}$, or `[1, 1024]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb16cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = gpt.get_device()\n",
    "dummy = torch.ones(1, 1024).long().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337d784",
   "metadata": {},
   "source": [
    "## Create a traced model\n",
    "\n",
    "> Using torch.jit.trace and torch.jit.trace_module, you can turn an existing module or Python function into a TorchScript ScriptFunction or ScriptModule. You must provide example inputs, and we run the function, recording the operations performed on all the tensors.\n",
    "\n",
    "—[torch.jit.trace docs](https://docs.pytorch.org/docs/stable/generated/torch.jit.trace.html#torch-jit-trace)\n",
    "\n",
    "This step transforms the model into a form that can be easily exported. There are tons of caveats here, and not every model would work. As far as I can tell, this one does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3e9e69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:7: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "traced_model = torch.jit.trace(model, dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeb6665",
   "metadata": {},
   "source": [
    "## Convert the traced model to a CoreML package\n",
    "\n",
    "The final step uses `coremltools` to convert the traced model into an\n",
    "`mlpackage`, which is a bundle of weights, JSON, etc., that you can import into\n",
    "Xcode to use in a Swift app.\n",
    "\n",
    "Note that it the output file should start with a capital letter. It sounds weird,\n",
    "but Xcode will automatically generate a class based on the name and classes should\n",
    "start with capitals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "496781bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 1781/1782 [00:00<00:00, 6760.11 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 40.17 passes/s]\n",
      "Running MIL default pipeline:   9%|▉         | 8/89 [00:00<00:02, 35.37 passes/s]/Users/justin/src/llm-scratch/.venv/lib/python3.11/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '2489', of the source model, has been renamed to 'var_2489' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline:  62%|██████▏   | 55/89 [00:02<00:01, 31.76 passes/s]/Users/justin/src/llm-scratch/.venv/lib/python3.11/site-packages/coremltools/converters/mil/mil/ops/defs/iOS15/elementwise_unary.py:889: RuntimeWarning: overflow encountered in cast\n",
      "  return input_var.val.astype(dtype=string_to_nptype(dtype_val))\n",
      "Running MIL default pipeline: 100%|██████████| 89/89 [00:12<00:00,  7.00 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 61.15 passes/s]\n"
     ]
    }
   ],
   "source": [
    "cml_model = ct.convert(\n",
    "    traced_model,\n",
    "    convert_to=\"mlprogram\",\n",
    "    inputs=[\n",
    "        ct.TensorType(shape=[1, ct.RangeDim(1, 1024)], dtype=np.float32, name=\"in_idx\")\n",
    "    ],\n",
    ")\n",
    "cml_model.save(\"Chatmodel.mlpackage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df27643",
   "metadata": {},
   "source": [
    "## Use it in your app\n",
    "\n",
    "If you want to see an example of this, check out [CrapGPT](https://github.com/juxtin/crapgpt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55240c6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-from-scratch (3.11.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
