{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad33f71",
   "metadata": {},
   "source": [
    "# Importing OpenAI Weights\n",
    "\n",
    "In this notebook, I'll be attempting to import official trained model weights from OpenAI into my own GPT model.\n",
    "\n",
    "I'll be importing code from [gpt.ipynb](./gpt.ipynb), so refer to that when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "655e3b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "# Import the notebook gpt.ipynb\n",
    "import gpt # type: ignore\n",
    "import torch\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e15bd",
   "metadata": {},
   "source": [
    "## Downloading the gpt_download.py script\n",
    "\n",
    "This script was provided as part of the book Build a Large Language Model (From Scratch), which I'm following here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dabd3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_script():\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/rasbt/\"\n",
    "        \"LLMs-from-scratch/main/ch05/\"\n",
    "        \"01_main-chapter-code/gpt_download.py\"\n",
    "    )\n",
    "    filename = url.split('/')[-1]\n",
    "    if Path(filename).exists():\n",
    "        # nothing to do\n",
    "        return\n",
    "    print(f\"Downloading {filename}\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "ensure_script()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9838d7",
   "metadata": {},
   "source": [
    "## Running gpt_download.py\n",
    "\n",
    "This script will download the following files:\n",
    "- checkpoint\n",
    "- encoder.json\n",
    "- hparams.json\n",
    "- model.ckpt.data-00000-of-00001\n",
    "- model.ckpt.index\n",
    "- model.ckpt.meta\n",
    "- vocab.bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89592b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", models_dir=\"gpt2\"\n",
    ")\n",
    "assert settings['n_embd'] == 768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e2922",
   "metadata": {},
   "source": [
    "# Define the OpenAI model config\n",
    "\n",
    "These are the basic hyperparameters that distinguish the various OpenAI GPT-2 models.\n",
    "We'll be focusing on the 124M version, at least initially, so we'll create `NEW_CONFIG`\n",
    "with the right settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07aea2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "NEW_CONFIG: gpt.GPTConfigDict = gpt.GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[\"gpt2-small (124M)\"])\n",
    "# We had set the context_length to 256 before, but we need it back at 1024.\n",
    "NEW_CONFIG.update({\"context_length\": 1024})\n",
    "\n",
    "# QKV Bias is not so popular anymore, but GPT-2 used it, so we will too.\n",
    "NEW_CONFIG.update({\"qkv_bias\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c769d7",
   "metadata": {},
   "source": [
    "# Create a new model based on GPT-2 and transfer weights\n",
    "\n",
    "This could get long. We're using a helper to \"safely\" overwrite the weights in\n",
    "our model. There are a lot of layers to do this with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1b36409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape},\"\n",
    "                         \"Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "def load_weights_into_gpt(model: gpt.SimplifiedGPT, params):\n",
    "    # Restore the token embeddings and positional embeddings\n",
    "    model.positional_embedding.weight = assign(model.positional_embedding.weight, params['wpe'])\n",
    "    model.token_embedding.weight = assign(model.token_embedding.weight, params['wte'])\n",
    "\n",
    "    # For each transformer block...\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        # ...restore the attention QKV weights\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        model.transformer_blocks[b].attention.w_query.weight = assign(\n",
    "            model.transformer_blocks[b].attention.w_query.weight, q_w.T)\n",
    "        model.transformer_blocks[b].attention.w_key.weight = assign(\n",
    "            model.transformer_blocks[b].attention.w_key.weight, k_w.T)\n",
    "        model.transformer_blocks[b].attention.w_value.weight = assign(\n",
    "            model.transformer_blocks[b].attention.w_value.weight, v_w.T)\n",
    "\n",
    "        # and the QKV biases\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        model.transformer_blocks[b].attention.w_query.bias = assign(\n",
    "            model.transformer_blocks[b].attention.w_query.bias, q_b)\n",
    "        model.transformer_blocks[b].attention.w_key.bias = assign(\n",
    "            model.transformer_blocks[b].attention.w_key.bias, k_b)\n",
    "        model.transformer_blocks[b].attention.w_value.bias = assign(\n",
    "            model.transformer_blocks[b].attention.w_value.bias, v_b)\n",
    "\n",
    "        # and the attention output projection\n",
    "        model.transformer_blocks[b].attention.w_out.weight = assign(\n",
    "            model.transformer_blocks[b].attention.w_out.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        model.transformer_blocks[b].attention.w_out.bias = assign(\n",
    "            model.transformer_blocks[b].attention.w_out.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # and the FeedForward layer weights\n",
    "        model.transformer_blocks[b].feedforward.layers[0].weight = assign(\n",
    "            model.transformer_blocks[b].feedforward.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        model.transformer_blocks[b].feedforward.layers[0].bias = assign(\n",
    "            model.transformer_blocks[b].feedforward.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        model.transformer_blocks[b].feedforward.layers[2].weight = assign(\n",
    "            model.transformer_blocks[b].feedforward.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        model.transformer_blocks[b].feedforward.layers[2].bias = assign(\n",
    "            model.transformer_blocks[b].feedforward.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # and the LayerNorm scale and shift weights\n",
    "        model.transformer_blocks[b].layer_norm_1.scale = assign(\n",
    "            model.transformer_blocks[b].layer_norm_1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        model.transformer_blocks[b].layer_norm_1.shift = assign(\n",
    "            model.transformer_blocks[b].layer_norm_1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        model.transformer_blocks[b].layer_norm_2.scale = assign(\n",
    "            model.transformer_blocks[b].layer_norm_2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        model.transformer_blocks[b].layer_norm_2.shift = assign(\n",
    "            model.transformer_blocks[b].layer_norm_2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    # and finally, restore the final norm scale and shift layers\n",
    "    model.layer_norm.scale = assign(model.layer_norm.scale, params[\"g\"])\n",
    "    model.layer_norm.shift = assign(model.layer_norm.shift, params[\"b\"])\n",
    "\n",
    "    # and the output head is also different in this version.\n",
    "    model.output.weight = assign(model.output.weight, params[\"wte\"])\n",
    "\n",
    "\n",
    "model = gpt.SimplifiedGPT(NEW_CONFIG)\n",
    "model.eval()\n",
    "load_weights_into_gpt(model.float(), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c4c9064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you as far as the hand can go until the end of your turn unless something interrupts your control\n",
      "flow. As you may observe I\n"
     ]
    }
   ],
   "source": [
    "gpt.manual_seed(123)\n",
    "trainable_model = gpt.GPTModel(NEW_CONFIG, model=model, force_cpu=False)\n",
    "gpt.prompt(trainable_model, \"Every effort moves you\", temperature=1.5, max_tokens=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb183ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
