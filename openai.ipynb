{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad33f71",
   "metadata": {},
   "source": [
    "# Importing OpenAI Weights\n",
    "\n",
    "In this notebook, I'll be attempting to import official trained model weights from OpenAI into my own GPT model.\n",
    "\n",
    "I'll be importing code from [gpt.ipynb](./gpt.ipynb), so refer to that when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "655e3b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n",
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "# Import the notebook gpt.ipynb\n",
    "import gpt # type: ignore\n",
    "import torch\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps:0\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e15bd",
   "metadata": {},
   "source": [
    "## Downloading the gpt_download.py script\n",
    "\n",
    "This script was provided as part of the book Build a Large Language Model (From Scratch), which I'm following here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dabd3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_script():\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/rasbt/\"\n",
    "        \"LLMs-from-scratch/main/ch05/\"\n",
    "        \"01_main-chapter-code/gpt_download.py\"\n",
    "    )\n",
    "    filename = url.split('/')[-1]\n",
    "    if Path(filename).exists():\n",
    "        # nothing to do\n",
    "        return\n",
    "    print(f\"Downloading {filename}\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "ensure_script()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9838d7",
   "metadata": {},
   "source": [
    "## Running gpt_download.py\n",
    "\n",
    "This script will download the following files:\n",
    "- checkpoint\n",
    "- encoder.json\n",
    "- hparams.json\n",
    "- model.ckpt.data-00000-of-00001\n",
    "- model.ckpt.index\n",
    "- model.ckpt.meta\n",
    "- vocab.bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89592b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 15:55:05.596477: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-21 15:55:05.858154: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750546505.992315    4662 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750546506.033466    4662 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750546506.282526    4662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750546506.282578    4662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750546506.282586    4662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750546506.282590    4662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-21 15:55:06.297736: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e2922",
   "metadata": {},
   "source": [
    "# Define the OpenAI model config\n",
    "\n",
    "These are the basic hyperparameters that distinguish the various OpenAI GPT-2 models.\n",
    "We'll be focusing on the 124M version, at least initially, so we'll create `NEW_CONFIG`\n",
    "with the right settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07aea2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "GPT_SMALL: gpt.GPTConfigDict = gpt.GPT_CONFIG_124M.copy()\n",
    "GPT_SMALL.update(model_configs[\"gpt2-small (124M)\"])\n",
    "# We had set the context_length to 256 before, but we need it back at 1024.\n",
    "GPT_SMALL.update({\"context_length\": 1024})\n",
    "\n",
    "# QKV Bias is not so popular anymore, but GPT-2 used it, so we will too.\n",
    "GPT_SMALL.update({\"qkv_bias\": True})\n",
    "\n",
    "GPT_CONFIG_355M: gpt.GPTConfigDict = gpt.GPT_CONFIG_124M.copy()\n",
    "GPT_CONFIG_355M.update(model_configs[\"gpt2-medium (355M)\"])\n",
    "GPT_CONFIG_355M.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "GPT_CONFIG_774M: gpt.GPTConfigDict = gpt.GPT_CONFIG_124M.copy()\n",
    "GPT_CONFIG_774M.update(model_configs[\"gpt2-large (774M)\"])\n",
    "GPT_CONFIG_774M.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "GPT_CONFIG_1558M: gpt.GPTConfigDict = gpt.GPT_CONFIG_124M.copy()\n",
    "GPT_CONFIG_1558M.update(model_configs[\"gpt2-xl (1558M)\"])\n",
    "GPT_CONFIG_1558M.update({\"context_length\": 1024, \"qkv_bias\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c769d7",
   "metadata": {},
   "source": [
    "# Create a new model based on GPT-2 and transfer weights\n",
    "\n",
    "This could get long. We're using a helper to \"safely\" overwrite the weights in\n",
    "our model. There are a lot of layers to do this with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b36409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n",
      "355M model loaded.\n"
     ]
    }
   ],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape},\"\n",
    "                         \"Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "def load_weights_into_gpt(model: gpt.SimplifiedGPT, params):\n",
    "    # Restore the token embeddings and positional embeddings\n",
    "    model.positional_embedding.weight = assign(model.positional_embedding.weight, params['wpe'])\n",
    "    model.token_embedding.weight = assign(model.token_embedding.weight, params['wte'])\n",
    "\n",
    "    # For each transformer block...\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        # ...restore the attention QKV weights\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        model.transformer_blocks[b].attention.w_query.weight = assign(\n",
    "            model.transformer_blocks[b].attention.w_query.weight, q_w.T)\n",
    "        model.transformer_blocks[b].attention.w_key.weight = assign(\n",
    "            model.transformer_blocks[b].attention.w_key.weight, k_w.T)\n",
    "        model.transformer_blocks[b].attention.w_value.weight = assign(\n",
    "            model.transformer_blocks[b].attention.w_value.weight, v_w.T)\n",
    "\n",
    "        # and the QKV biases\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        model.transformer_blocks[b].attention.w_query.bias = assign(\n",
    "            model.transformer_blocks[b].attention.w_query.bias, q_b)\n",
    "        model.transformer_blocks[b].attention.w_key.bias = assign(\n",
    "            model.transformer_blocks[b].attention.w_key.bias, k_b)\n",
    "        model.transformer_blocks[b].attention.w_value.bias = assign(\n",
    "            model.transformer_blocks[b].attention.w_value.bias, v_b)\n",
    "\n",
    "        # and the attention output projection\n",
    "        model.transformer_blocks[b].attention.w_out.weight = assign(\n",
    "            model.transformer_blocks[b].attention.w_out.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        model.transformer_blocks[b].attention.w_out.bias = assign(\n",
    "            model.transformer_blocks[b].attention.w_out.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # and the FeedForward layer weights\n",
    "        model.transformer_blocks[b].feedforward.layers[0].weight = assign(\n",
    "            model.transformer_blocks[b].feedforward.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        model.transformer_blocks[b].feedforward.layers[0].bias = assign(\n",
    "            model.transformer_blocks[b].feedforward.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        model.transformer_blocks[b].feedforward.layers[2].weight = assign(\n",
    "            model.transformer_blocks[b].feedforward.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        model.transformer_blocks[b].feedforward.layers[2].bias = assign(\n",
    "            model.transformer_blocks[b].feedforward.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # and the LayerNorm scale and shift weights\n",
    "        model.transformer_blocks[b].layer_norm_1.scale = assign(\n",
    "            model.transformer_blocks[b].layer_norm_1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        model.transformer_blocks[b].layer_norm_1.shift = assign(\n",
    "            model.transformer_blocks[b].layer_norm_1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        model.transformer_blocks[b].layer_norm_2.scale = assign(\n",
    "            model.transformer_blocks[b].layer_norm_2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        model.transformer_blocks[b].layer_norm_2.shift = assign(\n",
    "            model.transformer_blocks[b].layer_norm_2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    # and finally, restore the final norm scale and shift layers\n",
    "    model.layer_norm.scale = assign(model.layer_norm.scale, params[\"g\"])\n",
    "    model.layer_norm.shift = assign(model.layer_norm.shift, params[\"b\"])\n",
    "\n",
    "    # and the output head is also different in this version.\n",
    "    model.output.weight = assign(model.output.weight, params[\"wte\"])\n",
    "\n",
    "\n",
    "def load_openai_model(config: gpt.GPTConfigDict, size: str) -> gpt.GPTModel:\n",
    "    settings, params = download_and_load_gpt2(\n",
    "        model_size=size, models_dir=\"gpt2\"\n",
    "    )\n",
    "    model = gpt.SimplifiedGPT(config)\n",
    "    model.eval()\n",
    "    load_weights_into_gpt(model, params)\n",
    "    trainable_model = gpt.GPTModel(config, gpt.DEFAULT_TRAINING_CONFIG, model=model, force_cpu=True)\n",
    "    print(f\"{size} model loaded.\")\n",
    "    return trainable_model\n",
    "\n",
    "# Uncomment one of the following to load that model\n",
    "# model = load_openai_model(GPT_SMALL, \"124M\")\n",
    "# model = load_openai_model(GPT_CONFIG_355M, \"355M\")\n",
    "model = load_openai_model(GPT_CONFIG_1558M, \"1558M\") # needs force_cpu=True on my system or it crashes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c4c9064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub is the place for organize our parts so please consider coming. If you want have a look around it. As you may\n",
      "observe I did not write a whole new article about it, just took a little while to prepare things, as it has been quite\n",
      "the last few months. Please use my work you got from this article. And feel free to ask me anything because they're all\n",
      "useful. If you have questions please refer back to article where GitBook is discussed. Also you can download my book\n",
      "\"Kudos\" [download it here, and donÂ´t miss it. It is recommended for this article]. Just enjoy working in GitBook. Enjoy!\n",
      "Posted by Greg Smith at 13:55\n"
     ]
    }
   ],
   "source": [
    "gpt.manual_seed(123)\n",
    "gpt.prompt(model, \"GitHub is\", temperature=1.5, max_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def chat_gpt(model, prompt, temperature=1.5, max_tokens=128):\n",
    "    base = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n\"\n",
    "    tail = \"\\n\\n### Response:\\n\"\n",
    "    model_input = base + prompt + tail\n",
    "    completion = model.prompt(model_input, temperature=temperature, max_tokens=max_tokens)\n",
    "    response = completion[len(model_input):].strip()\n",
    "    print(textwrap.fill(response, width=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c321893e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As mentioned above a response may not even be submitted by application. One reason could be because of memory capacity\n",
      "limitation or time-out caused by browser rendering the link request while waiting in browser response.  If users of\n",
      "browsers are expecting to retrieve this data with HTML, it is recommended that to provide valid HTML code for that part\n",
      "of request.  An effective approach of creating code (HTML or image: nth c# #if !(?=!HTML)(?:\\  ){function* response-\n",
      "tally-html-t1 (i:Integer){if(t1){$((this == \"Response: \" ?  Response : \"<script type= 'text/javascript'> %7Escript\n",
      "<video id= \\\" '+ t1+' \"' />' ');}}}}\",t1: 0}else response-tally-html-t2 (i:Integer): $(\"iframe<(i<=?2||>=?2)+\\\">\n",
      "\"){if(-j1){(i : t2 ? : t1).= jQuery.(i)</t2>(;) }try if(t2){$(\"p.post(\"*p://[a-z]*\n"
     ]
    }
   ],
   "source": [
    "chat_gpt(model, \"Answer this question: what is 2+2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee32c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
