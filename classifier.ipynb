{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2a07c6d",
   "metadata": {},
   "source": [
    "# Fine tuning for spam classification\n",
    "\n",
    "This follows chapter 6 of the book [Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed2f9849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import openai # type:ignore\n",
    "import gpt # type:ignore\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import ssl\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "import time\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available(): # type: ignore[attr-defined]\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available(): # type: ignore[attr-defined]\n",
    "        return torch.device(\"mps:0\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a298b",
   "metadata": {},
   "source": [
    "## Download and preprocess the UCI spam data\n",
    "\n",
    "The fine folks at the University of California at Irvine have provided a nice little data set for SMS spam.\n",
    "Let's download that and save it in a convenient CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f5452b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "# NOTE: as of 6/21/25, the UCI archive server is unreachable. I downloaded this file\n",
    "# manually from a mirror.\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "     \n",
    "    ssl_context = ssl._create_unverified_context()\n",
    "\n",
    "    with urllib.request.urlopen(url, context=ssl_context) as response:\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)    \n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b96f86",
   "metadata": {},
   "source": [
    "The data set contains 4825 ham messages and only 747 spam messages. Since we want an equal number of both, we'll have to take 747 ham messages at random and discard the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9394e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91b9c5",
   "metadata": {},
   "source": [
    "Now we want to create the following splits:\n",
    "- 70% for training\n",
    "- 10% for validation\n",
    "- 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3632c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "102c2b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv():\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "    df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "    balanced_df = create_balanced_dataset(df)\n",
    "    balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "    train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "    train_df.to_csv(\"train.csv\", index=None) # type:ignore\n",
    "    validation_df.to_csv(\"validation.csv\", index=None) # type:ignore\n",
    "    test_df.to_csv(\"test.csv\", index=None) # type:ignore\n",
    "\n",
    "# Uncomment if you haven't saved this yet\n",
    "# save_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f8b45d",
   "metadata": {},
   "source": [
    "## SpamDataset\n",
    "\n",
    "This class:\n",
    "1. Pre-tokenizes the texts from the dataset\n",
    "2. Truncates any sequences that are longer than the maximum length (or the longest text, if no maximum is set).\n",
    "3. Pads any sequences shorter than the max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9440ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file: Path, tokenizer: tiktoken.Encoding, max_length:int|None=None, pad_token_id:int=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "        \n",
    "            # truncate sequences that are longer than max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        # pad the sequences\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded = self.encoded_texts[idx]\n",
    "        label = self.data.iloc[idx][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        return max([len(txt) for txt in self.encoded_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4d84faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = SpamDataset(\n",
    "    csv_file=Path(\"train.csv\"),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "max_length = train_dataset.max_length\n",
    "val_dataset = SpamDataset(\n",
    "    csv_file=Path(\"validation.csv\"),\n",
    "    max_length=max_length,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=Path(\"test.csv\"),\n",
    "    max_length=max_length,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "31f364f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "gpt.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "# Note: I think drop_last is true because the last batch won't have enough messages in it,\n",
    "# not because the last message might be short."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deacb9a",
   "metadata": {},
   "source": [
    "## Creating the ClassifierGPT class\n",
    "\n",
    "The ClassifierGPT class wraps and modifies a normal SimplifiedGPT model.\n",
    "Maybe it would be better to use a function that explicitly modifies its argument?\n",
    "\n",
    "### What's the difference between this and SimplifiedGPT?\n",
    "\n",
    "Truthfully, not much: \n",
    "- The final output layer has dimensions $\\text{context\\_length}\\times\\text{classifications}$, rather than $\\text{context\\_length}\\times\\text{vocabulary}$.\n",
    "- We discard the gradients for the inner layers, since those are adequately trained already.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "59604766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierGPT(nn.Module):\n",
    "    \"\"\"Wraps a SimplifiedGPT model and bases a classification model on it.\n",
    "    Note that the model argument WILL BE MODIFIED.\"\"\"\n",
    "    def __init__(self, model: gpt.SimplifiedGPT, classifications:int):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        cfg = model.cfg\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.model.output = nn.Linear(cfg[\"emb_dim\"], classifications)\n",
    "        for param in self.model.transformer_blocks[-1].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.model.layer_norm.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, in_idx: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(in_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "88ea4e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n",
      "124M model loaded.\n"
     ]
    }
   ],
   "source": [
    "gpt124m = openai.load_openai_model(openai.GPT_CONFIG_124M, \"124M\").model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "25485829",
   "metadata": {},
   "outputs": [],
   "source": [
    "clas = ClassifierGPT(gpt124m, 2).to(get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3eaea9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs tensor([[ 4261,    38,  3525,  3228,  3406,   604,     9, 18133,  4216,  4294,\n",
      "         22770,   393,  4248, 27641, 25507,  4947,    13,  4889,  7769,  2713,\n",
      "           405, 12865,  2598,  2735,   284, 44819,    13,   311, 14242,    11,\n",
      "         17283,   264,    11,   350,  9864,  1140, 31380,    11, 10500,   634,\n",
      "            11, 14277,  2548,    87,    71,    11,  6446, 14988,    16,    13,\n",
      "          1120,    14,  4426,    11,  5436,   940, 42951]], device='cuda:0')\n",
      "Input dimensions: torch.Size([1, 57])\n",
      "Outputs:\n",
      "tensor([[[-0.1383, -0.8963],\n",
      "         [-5.2545, -1.9323],\n",
      "         [-6.2596, -3.4245],\n",
      "         [-5.2016, -3.4344],\n",
      "         [-6.5946, -3.3080],\n",
      "         [-5.2458, -2.9205],\n",
      "         [-6.4475, -2.9775],\n",
      "         [-6.7713, -3.1688],\n",
      "         [-3.3267, -2.2104],\n",
      "         [-7.6786, -4.0148],\n",
      "         [-7.3234, -3.7454],\n",
      "         [-6.9835, -3.6252],\n",
      "         [-6.4855, -5.9448],\n",
      "         [-7.2155, -3.5419],\n",
      "         [-6.6906, -3.4634],\n",
      "         [-7.4141, -4.8514],\n",
      "         [-7.3734, -5.0421],\n",
      "         [-5.8442, -4.2101],\n",
      "         [-4.7937, -4.4368],\n",
      "         [-4.8720, -3.8535],\n",
      "         [-5.8827, -4.6282],\n",
      "         [-6.3721, -5.2402],\n",
      "         [-6.5575, -5.2808],\n",
      "         [-8.3653, -4.5875],\n",
      "         [-9.3706, -4.8765],\n",
      "         [-7.4274, -4.1729],\n",
      "         [-6.0070, -2.8428],\n",
      "         [-6.3275, -3.4766],\n",
      "         [-8.0705, -3.4614],\n",
      "         [-7.7090, -3.8098],\n",
      "         [-5.4524, -2.9824],\n",
      "         [-5.3937, -3.1252],\n",
      "         [-7.3893, -3.8293],\n",
      "         [-6.1187, -3.3933],\n",
      "         [-5.3406, -2.5093],\n",
      "         [-5.0762, -2.6803],\n",
      "         [-5.2786, -3.1832],\n",
      "         [-6.6820, -3.4006],\n",
      "         [-5.3508, -2.9180],\n",
      "         [-5.6469, -2.7596],\n",
      "         [-7.0217, -3.3063],\n",
      "         [-5.9398, -3.0285],\n",
      "         [-5.6458, -3.7656],\n",
      "         [-6.5495, -4.3317],\n",
      "         [-7.0831, -3.7712],\n",
      "         [-7.1410, -3.4896],\n",
      "         [-5.9817, -2.4606],\n",
      "         [-6.9419, -5.2788],\n",
      "         [-5.6686, -3.2425],\n",
      "         [-6.4793, -5.9481],\n",
      "         [-6.6022, -4.3595],\n",
      "         [-6.5039, -4.1736],\n",
      "         [-6.6191, -3.8100],\n",
      "         [-8.0252, -4.2252],\n",
      "         [-6.6187, -3.2280],\n",
      "         [-6.3461, -3.7693],\n",
      "         [-6.8923, -3.7247]]], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Output dimensions: torch.Size([1, 57, 2])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"URGENT!! Your 4* Costa Del Sol Holiday or £5000 await collection. Call 09050090044 Now toClaim. SAE, TC s, POBox334, Stockport, SK38xh, Cost£1.50/pm, Max10mins\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0).to(get_device())\n",
    "print(f\"Inputs {inputs}\")\n",
    "print(f\"Input dimensions: {inputs.shape}\")\n",
    "\n",
    "output = clas(inputs)\n",
    "print(f\"Outputs:\\n{output}\")\n",
    "print(f\"Output dimensions: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d2415e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1321, 0.8679]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(output[:, -1, :], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "18052f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(dataloader: DataLoader, model: ClassifierGPT, device: torch.device, num_batches:int|None=None) -> float:\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(dataloader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(dataloader))\n",
    "    for i, (input_batch, target_batch) in enumerate(dataloader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "92cb4e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch: torch.Tensor, target_batch: torch.Tensor, model: ClassifierGPT, device: torch.device) -> torch.Tensor:\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :]\n",
    "    loss = nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8a384ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(dataloader: DataLoader, model: ClassifierGPT, device: torch.device, num_batches:int|None=None) -> float:\n",
    "    total_loss = 0.\n",
    "    if len(dataloader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(dataloader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(dataloader))\n",
    "    for i, (input_batch, target_batch) in enumerate(dataloader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b4081de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.088\n",
      "Validation loss: 0.073\n",
      "Test loss: 0.326\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # disable gradient tracking for efficiency right now\n",
    "    train_loss = calc_loss_loader(train_loader, clas, get_device(), num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, clas, get_device(), num_batches=5)\n",
    "    test_loss = calc_loss_loader(test_loader, clas, get_device(), num_batches=5)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "32b553f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: ClassifierGPT, train_loader: DataLoader, val_loader: DataLoader, device: torch.device, eval_iter:int):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fd815c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_simple(model: ClassifierGPT, train_loader: DataLoader, val_loader: DataLoader, optimizer, device: torch.device, num_epochs:int, eval_freq:int, eval_iter:int):\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            examples_seen += input_batch.shape[0]\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end =\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1b99f7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 0.112, Val loss 0.069\n",
      "Ep 1 (Step 000050): Train loss 0.094, Val loss 0.076\n",
      "Ep 1 (Step 000100): Train loss 0.052, Val loss 0.170\n",
      "Training accuracy: 92.50% | Validation accuracy: 100.00%\n",
      "Ep 2 (Step 000150): Train loss 0.054, Val loss 0.100\n",
      "Ep 2 (Step 000200): Train loss 0.104, Val loss 0.153\n",
      "Ep 2 (Step 000250): Train loss 0.153, Val loss 0.095\n",
      "Training accuracy: 95.00% | Validation accuracy: 100.00%\n",
      "Ep 3 (Step 000300): Train loss 0.033, Val loss 0.084\n",
      "Ep 3 (Step 000350): Train loss 0.193, Val loss 0.063\n",
      "Training accuracy: 97.50% | Validation accuracy: 97.50%\n",
      "Ep 4 (Step 000400): Train loss 0.210, Val loss 0.023\n",
      "Ep 4 (Step 000450): Train loss 0.072, Val loss 0.138\n",
      "Ep 4 (Step 000500): Train loss 0.036, Val loss 0.138\n",
      "Training accuracy: 97.50% | Validation accuracy: 100.00%\n",
      "Ep 5 (Step 000550): Train loss 0.090, Val loss 0.042\n",
      "Ep 5 (Step 000600): Train loss 0.061, Val loss 0.060\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Training completed in 0.53 minutes.\n"
     ]
    }
   ],
   "source": [
    "def training_run():\n",
    "    start_time = time.time()\n",
    "    gpt.manual_seed(123)\n",
    "    optimizer = torch.optim.AdamW(clas.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "    num_epochs = 5\n",
    "    train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "        clas, train_loader, val_loader, optimizer, get_device(), num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    execution_time_minutes = (end_time - start_time) / 60\n",
    "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "\n",
    "training_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "528aaf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.17%\n",
      "Validation accuracy: 97.92%\n",
      "Test accuracy: 95.27%\n"
     ]
    }
   ],
   "source": [
    "def model_accuracy(model: ClassifierGPT, device):\n",
    "    train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "    val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "    test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "    print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "    print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "    print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "model_accuracy(clas, get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5232f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text: str, model: ClassifierGPT, tokenizer: tiktoken.Encoding, device: torch.device, max_length:int=0, pad_token_id:int=50256) -> str:\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = model.model.cfg['context_length']\n",
    "    if max_length == 0:\n",
    "        max_length = supported_context_length\n",
    "\n",
    "    # truncate if too long\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
    "    # pad if too short\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    input_tensor = torch.tensor(input_ids, device=device, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :]\n",
    "    print(f\"Logits: {logits}\")\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f26c4051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 2.7385, -3.7317]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'not spam'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = train_dataset.max_length\n",
    "sample = \"hey dude, what up wit it\"\n",
    "classify(sample, clas, tokenizer, get_device(), max_length=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc820982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
