{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2a07c6d",
   "metadata": {},
   "source": [
    "# Fine tuning for spam classification\n",
    "\n",
    "This is a bit of a digression from the book, based on the [Vizuara course](https://www.youtube.com/watch?v=yZpy_hsC1bE).\n",
    "I think the idea is to get a more gradual introduction into fine tuning by starting with something simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed2f9849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import openai # type:ignore\n",
    "import gpt # type:ignore\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import ssl\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a298b",
   "metadata": {},
   "source": [
    "## Download and preprocess the UCI spam data\n",
    "\n",
    "The fine folks at the University of California at Irvine have provided a nice little data set for SMS spam.\n",
    "Let's download that and save it in a convenient CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5452b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection/SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "# NOTE: on 6/21/25, the UCI archive server is unreachable. I downloaded this file\n",
    "# manually from a mirror.\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "     \n",
    "    ssl_context = ssl._create_unverified_context()\n",
    "\n",
    "    with urllib.request.urlopen(url, context=ssl_context) as response:\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)    \n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b96f86",
   "metadata": {},
   "source": [
    "The data set contains 4825 ham messages and only 747 spam messages. Since we want an equal number of both, we'll have to take 747 ham messages at random and discard the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9394e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91b9c5",
   "metadata": {},
   "source": [
    "Now we want to create the following splits:\n",
    "- 70% for training\n",
    "- 10% for validation\n",
    "- 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3632c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102c2b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv():\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "    df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "    balanced_df = create_balanced_dataset(df)\n",
    "    balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "    train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "    train_df.to_csv(\"train.csv\", index=None) # type:ignore\n",
    "    validation_df.to_csv(\"validation.csv\", index=None) # type:ignore\n",
    "    test_df.to_csv(\"test.csv\", index=None) # type:ignore\n",
    "\n",
    "# Uncomment if you haven't saved this yet\n",
    "# save_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9440ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
