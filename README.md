# LLM (From Scratch)

In this repo, I'm building a large language model comparable to GPT-2, using the following resources:
1. The book [Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch), by [@rasbt](https://github.com/rasbt).
2. The YouTube playlist [Building LLMs from scratch](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu) by [Vizuara](https://www.youtube.com/@vizuara).
3. [Attention Is All You Need](https://arxiv.org/abs/1706.03762), by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.

The code and my notes/explanations are in [gpt.ipynb](./gpt.ipynb).

# Current Status

I have a working, but untrained GPT-2 style LLM now. Because it's still untrained, it only outputs garbage.
However, it outputs the exact correct garbage, so I know this implementation is equivalent to the book's.